{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After talking with Riley, he suggested that I shift my focus from time-series-analysis, and more towards topic modeling ##\n",
    "\n",
    "So, for my sources I'll do the following:\n",
    "- Reddit: try to collect the actual submissions or at LEAST the title of the submissions. Maybe comments (secondary)\n",
    "- arXiv: Abstract/ summary of the paper (not the whole paper)\n",
    "- Job posts: The limited crappy findings\n",
    "- News API: The headline and whatever text News API supports\n",
    "- Google: Nothin doing\n",
    "- Wiki: just do a TM of the page. Or better yet do TM over time of the page. We'll see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, arXiv, because I think it'll be the easiest data to get ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "import time\n",
    "import feedparser\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arx_and_recreation(search_term, start_idx, scope='ti'):\n",
    "    '''uses urllib, time, and feedparser'''\n",
    "    #first escape search terms\n",
    "    search_term= search_term.replace('\"',\"%22\").replace(\" \", \"+\");\n",
    "    # set wait time and iteration step\n",
    "    iterstep= 200;\n",
    "    wait_time= 2 #make this 3 to 'play nice' with the api. I am nice I promise! but impatient atm\n",
    "    base_url = 'http://export.arxiv.org/api/query?'\n",
    "    start= start_idx\n",
    "    date_dict={\n",
    "        \"date\":[],\n",
    "        \"article_id\": [],\n",
    "        \"summary\":[],\n",
    "        \"source\": \"arXiv\"\n",
    "    }\n",
    "    while True:\n",
    "        response= urllib.request.urlopen(base_url+f\"search_query={scope}:{search_term}&sortBy=submittedDate&sortOrder=ascending&start={start}&max_results={iterstep}\")\n",
    "        feed= feedparser.parse(response)\n",
    "        if not feed.entries:\n",
    "            print('query complete')\n",
    "            print(f\"There should be {feed.feed.opensearch_totalresults} results?\")\n",
    "            break\n",
    "        date_dict['date'].extend([entry.published for entry in feed.entries])\n",
    "        date_dict['article_id'].extend([entry.id.split('/abs/')[-1] for entry in feed.entries])\n",
    "        date_dict['summary'].extend([entry.summary.replace(\"\\n\", \" \") for entry in feed.entries])\n",
    "        print(f\"gathering results {start} to {start + iterstep-1} \")\n",
    "        start = start + iterstep\n",
    "        time.sleep(wait_time)\n",
    "        \n",
    "    return pd.DataFrame(date_dict)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering results 0 to 199 \n",
      "gathering results 200 to 399 \n",
      "gathering results 400 to 599 \n",
      "gathering results 600 to 799 \n",
      "gathering results 800 to 999 \n",
      "gathering results 1000 to 1199 \n",
      "gathering results 1200 to 1399 \n",
      "gathering results 1400 to 1599 \n",
      "gathering results 1600 to 1799 \n",
      "gathering results 1800 to 1999 \n",
      "gathering results 2000 to 2199 \n",
      "gathering results 2200 to 2399 \n",
      "gathering results 2400 to 2599 \n",
      "gathering results 2600 to 2799 \n",
      "gathering results 2800 to 2999 \n",
      "gathering results 3000 to 3199 \n",
      "gathering results 3200 to 3399 \n",
      "gathering results 3400 to 3599 \n",
      "gathering results 3600 to 3799 \n",
      "gathering results 3800 to 3999 \n",
      "gathering results 4000 to 4199 \n",
      "gathering results 4200 to 4399 \n",
      "gathering results 4400 to 4599 \n",
      "gathering results 4600 to 4799 \n",
      "gathering results 4800 to 4999 \n",
      "gathering results 5000 to 5199 \n",
      "gathering results 5200 to 5399 \n",
      "gathering results 5400 to 5599 \n",
      "gathering results 5600 to 5799 \n",
      "gathering results 5800 to 5999 \n",
      "gathering results 6000 to 6199 \n",
      "gathering results 6200 to 6399 \n",
      "gathering results 6400 to 6599 \n",
      "gathering results 6600 to 6799 \n",
      "gathering results 6800 to 6999 \n",
      "gathering results 7000 to 7199 \n",
      "gathering results 7200 to 7399 \n",
      "gathering results 7400 to 7599 \n",
      "gathering results 7600 to 7799 \n",
      "gathering results 7800 to 7999 \n",
      "gathering results 8000 to 8199 \n",
      "gathering results 8200 to 8399 \n",
      "gathering results 8400 to 8599 \n",
      "gathering results 8600 to 8799 \n",
      "gathering results 8800 to 8999 \n",
      "gathering results 9000 to 9199 \n",
      "gathering results 9200 to 9399 \n",
      "gathering results 9400 to 9599 \n",
      "gathering results 9600 to 9799 \n",
      "gathering results 9800 to 9999 \n",
      "query complete\n",
      "There should be 16008 results?\n"
     ]
    }
   ],
   "source": [
    "#collect all cat:cs.AI from arXiv\n",
    "# ai1 = arx_and_recreation(\"cs.AI\", 0, scope='cat')\n",
    "\n",
    "# ai2= arx_and_recreation(\"cs.AI\", 10000, scope='cat')\n",
    "\n",
    "# ai3 = arx_and_recreation(\"cs.AI\", 12200, scope='cat')\n",
    "\n",
    "# ai4 = arx_and_recreation(\"cs.AI\", 14400, scope='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ai = pd.concat([ai1, ai2, ai3, ai4], ignore_index=True)\n",
    "\n",
    "# all_ai.head()\n",
    "\n",
    "# all_ai['date']= pd.DatetimeIndex(all_ai['date']).normalize()\n",
    "\n",
    "# all_ai.to_csv('./new_hope_data/arxiv_csAI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It took re-running my function 4 times to get all the articles.###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai = pd.read_csv('./new_hope_data/arxiv_csAI.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ai['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 1.331s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.289s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=16008 and n_features=1000...\n",
      "done in 2.311s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: image features method classification based task images dataset language methods text approach visual datasets using feature art proposed object results\n",
      "Topic #1: logic programs semantics programming answer reasoning program set asp fuzzy rules logics language propositional theory order stable sets argumentation logical\n",
      "Topic #2: algorithm problem search problems algorithms optimization optimal time solution solutions planning constraint constraints number solving results local based heuristic new\n",
      "Topic #3: learning reinforcement policy deep rl tasks learn policies machine reward environment task function action methods exploration algorithms state agent value\n",
      "Topic #4: network neural networks deep training layer architecture convolutional recurrent memory trained architectures layers bayesian input nodes accuracy weights parameters train\n",
      "Topic #5: inference belief probability probabilistic bayesian variables causal theory conditional decision probabilities uncertainty distribution networks distributions evidence independence models functions graphical\n",
      "Topic #6: data mining sets analysis machine real clustering time patterns learning fuzzy using techniques web large discovery information applications used approach\n",
      "Topic #7: agents agent intelligence ai systems human artificial game decision games environment research information social intelligent uncertainty theory planning actions multi\n",
      "Topic #8: model models sequence generative prediction modeling latent based parameters predictive process predictions dynamics propose free graphical state attention simulation time\n",
      "Topic #9: knowledge ontology semantic web ontologies domain information reasoning base representation concepts based bases rules language systems paper query queries description\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=16008 and n_features=1000...\n",
      "done in 12.496s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: using paper propose results based used proposed performance work approach present method novel time use data new different real methods\n",
      "Topic #1: paper set logic based reasoning theory present study used problem new semantics rules programming order given results way properties general\n",
      "Topic #2: algorithm problem problems algorithms search optimal time optimization solution number results space solving value solutions solve function new performance state\n",
      "Topic #3: agent reinforcement agents learning environment world based human systems work game policy actions use paper environments planning framework control behavior\n",
      "Topic #4: neural networks network learning model deep training tasks models state trained learn task input learned introduce representations demonstrate simple recurrent\n",
      "Topic #5: model probability paper uncertainty decision probabilistic bayesian information inference networks used belief theory variables models making network structure probabilities use\n",
      "Topic #6: data learning methods real method world algorithm sets provide variables distribution set structure statistical algorithms present prior theoretical mining sample\n",
      "Topic #7: systems paper intelligence artificial research machine learning recent new computer theory problems years applications field ai networks algorithms various used\n",
      "Topic #8: model models work problem techniques language programming recent problems question research modeling machine program variety specific solving human programs search\n",
      "Topic #9: knowledge semantic language representation web domain information natural graph text present ontology representations relations task word reasoning user specific queries\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=16008 and n_features=1000...\n",
      "done in 24.660s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: information user users social web based online systems software service content paper preferences interactive recommendation items security group provide topic\n",
      "Topic #1: probability belief function functions distribution bayesian value uncertainty approximation problem random distributions optimal decision probabilities number space bound theory markov\n",
      "Topic #2: data models method results model methods learning based using performance classification proposed datasets algorithm dataset propose accuracy features approach networks\n",
      "Topic #3: inference data time fuzzy based process rules temporal rule model probabilistic paper patterns using queries event techniques events mining proposed\n",
      "Topic #4: logic set reasoning programs program models paper causal programming complexity semantics model variables theory sets given problem properties order class\n",
      "Topic #5: knowledge language semantic representation based text model objects question natural ontology paper domain information representations data object word task models\n",
      "Topic #6: agent agents game planning actions games action environment multi environments state plan goal strategies based plans goals task player approach\n",
      "Topic #7: problem problems algorithm search algorithms optimization solutions solution based constraints solving paper approach optimal time constraint new results solve local\n",
      "Topic #8: learning neural deep model network networks training policy reinforcement tasks state image based learn performance using method propose images task\n",
      "Topic #9: human systems decision intelligence ai artificial research model machine paper robot making learning different cognitive models based computer work theory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Going to modify this code to use my corpus instead of the 20newsgroups\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = len(corpus)\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "# print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = corpus[:n_samples]\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gathering results 0 to 199 \n",
      "gathering results 200 to 399 \n",
      "gathering results 400 to 599 \n",
      "gathering results 600 to 799 \n",
      "query complete\n",
      "There should be 18961 results?\n"
     ]
    }
   ],
   "source": [
    "# # Gonna try the same thing with arXiv's stat.ML category\n",
    "# ml1 = arx_and_recreation(\"stat.ML\", 0, scope='cat')\n",
    "\n",
    "# ml2 = arx_and_recreation(\"stat.ML\", 800, scope='cat')\n",
    "\n",
    "# ml3 = arx_and_recreation(\"stat.ML\", 1400, scope='cat')\n",
    "\n",
    "# ml4 = arx_and_recreation(\"stat.ML\", 10400, scope='cat')\n",
    "\n",
    "# ml5 = arx_and_recreation(\"stat.ML\", 17000, scope='cat')\n",
    "\n",
    "# ml6 = arx_and_recreation(\"stat.ML\", 18400, scope='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ml = pd.concat([ml1, ml2, ml3, ml4, ml5, ml6], ignore_index=True)\n",
    "\n",
    "# all_ml['date']= pd.DatetimeIndex(all_ml['date']).normalize()\n",
    "\n",
    "# all_ml.to_csv('./new_hope_data/arxiv_csML.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ml = pd.read_csv('./new_hope_data/arxiv_csML.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_corpus = list(all_ml['summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 1.652s.\n",
      "Extracting tf features for LDA...\n",
      "done in 1.536s.\n",
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=18961 and n_features=1000...\n",
      "done in 1.776s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: data method features classification time based analysis approach proposed methods feature using high real information prediction model datasets dimensional set\n",
      "Topic #1: learning machine tasks reinforcement deep task policy algorithms agent learn supervised framework rl state performance systems multi new transfer agents\n",
      "Topic #2: algorithm optimization gradient convex stochastic convergence algorithms problem function problems rate optimal functions descent non bounds linear regret method bound\n",
      "Topic #3: model models inference bayesian latent variational variables gaussian posterior distribution process likelihood probabilistic distributions markov parameters mixture graphical sampling generative\n",
      "Topic #4: neural network networks deep training convolutional layer layers recurrent architectures architecture trained cnn input image using weights performance memory accuracy\n",
      "Topic #5: graph graphs nodes node structure edges network spectral random edge embedding graphical community networks problem structures structured based social information\n",
      "Topic #6: matrix rank low sparse matrices completion tensor norm factorization recovery covariance problem entries decomposition algorithm dimensional noise signal subspace pca\n",
      "Topic #7: clustering clusters cluster spectral means algorithm data mixture points hierarchical subspace number algorithms based density similarity distance method unsupervised em\n",
      "Topic #8: kernel kernels regression space gaussian function methods approximation density functions spaces mean nonlinear based learning support vector positive nonparametric distributions\n",
      "Topic #9: adversarial training generative attacks examples gan gans images samples image robustness distribution classifier trained generate networks target domain generation robust\n",
      "\n",
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=18961 and n_features=1000...\n",
      "done in 17.555s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: results data using paper based proposed propose set used method performance problem methods approach use number new model study provide\n",
      "Topic #1: neural networks network deep training learning using model trained work state models performance tasks task art used use based classification\n",
      "Topic #2: optimization algorithm gradient function problem convergence problems stochastic convex algorithms optimal rate results theoretical functions descent loss learning bounds non\n",
      "Topic #3: models model inference bayesian variational gaussian parameters distribution variables process posterior latent sampling likelihood markov probabilistic distributions probability stochastic variable\n",
      "Topic #4: matrix sparse dimensional number high low linear structure random problem spectral analysis estimation method matrices recovery noise theoretical signal numerical\n",
      "Topic #5: time learning problem graph reinforcement world algorithm propose algorithms policy state real based structure graphs present nodes paper information network\n",
      "Topic #6: model sparse problem real world systems propose low user paper users matrix models rank time state work applications latent sparsity\n",
      "Topic #7: data method time real propose using proposed large methods paper algorithm world used scale clustering experiments new datasets synthetic analysis\n",
      "Topic #8: learning methods machine problems paper kernel state based method framework art space approach large recent algorithms novel scale applications approaches\n",
      "Topic #9: learning training data generative adversarial supervised samples new distribution space unsupervised semi examples work target distributions class sample metric learn\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=18961 and n_features=1000...\n",
      "done in 35.780s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: learning data classification training supervised class method classifier domain methods proposed performance labels datasets classifiers approach label set based space\n",
      "Topic #1: kernel distribution bayesian gaussian process distributions sampling processes posterior inference variational markov data probability space function model carlo monte using\n",
      "Topic #2: data based learning time model approach methods using information method different paper features machine real proposed prediction used task systems\n",
      "Topic #3: optimization gradient algorithm method methods problems stochastic algorithms problem convergence large convex descent propose proposed computational paper based non efficient\n",
      "Topic #4: learning algorithm algorithms problem policy reinforcement optimal online bound bounds regret agent time setting function based reward value complexity rl\n",
      "Topic #5: model models data latent inference variables structure approach learning probabilistic variable causal methods bayesian tree using generative predictive conditional propose\n",
      "Topic #6: loss regression function learning functions regularization linear feature selection risk sparse features convex non vector norm problems machine dictionary lasso\n",
      "Topic #7: graph graphs random nodes local energy node log matching results algorithms function problem bound number error algorithm convergence prove theory\n",
      "Topic #8: neural networks network deep training learning adversarial model models convolutional trained layer performance input using architecture generative results layers architectures\n",
      "Topic #9: data matrix estimation clustering dimensional high algorithm analysis rank low method sample problem noise number estimator based proposed results sparse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Going to modify this code to use my corpus instead of the 20newsgroups\n",
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = len(ml_corpus)\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "# print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = ml_corpus[:n_samples]\n",
    "# print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
