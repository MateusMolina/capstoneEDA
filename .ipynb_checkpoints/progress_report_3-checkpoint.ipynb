{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do you have data fully in hand and if not, what blockers are you facing?**\n",
    "\n",
    "I have pivoted quite significantly since my last milestone.\n",
    "I don't have all my data in hand. But that's ok because all my data is easily accessible. I currently have upwards of 18,000 academic journal abstracts. I will be doing topic modeling on them, and basing my capstone on clustering documents with similar topics, as well as charting how topics in a given field (for example, AI) differ from year to year. \n",
    "\n",
    "The bulk of my EDA and pipelining is happening on these abstracts, and then I will generalize to different categories of papers (as well as potentially downloading whole papers and doing the modeling on them) as time permits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have you done a full EDA on all of your data?**\n",
    "\n",
    "My data is pretty clean, as it comes in professional, peer-reviewed journal format. The bag of words or count-vec processes can be thought of as EDA, but I haven't yet done something like distribution of word counts\n",
    "\n",
    "I have done an heuristic EDA as far as throwing in my corpus into a Tokenization-> LDA/NMF script. Sure enough, I got coherent topics outputted. But my focus now is to decide on an efficient and effective pre-proccessing pipeline, as well as tune my parameters for the num_topics, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have you begun the modeling process? How accurate are your predictions so far?**\n",
    "\n",
    "The following is the output of my initial Topic-modeling attempts:\n",
    "\n",
    "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=18961 and n_features=1000...\n",
    "done in 3.012s.\n",
    "\n",
    "Topics in NMF model (Frobenius norm):\n",
    " - Topic #0: data method features classification time based analysis approach proposed methods feature using high real information prediction model datasets dimensional set\n",
    " - Topic #1: learning machine tasks reinforcement deep task policy algorithms agent learn supervised framework rl state performance systems multi new transfer agents\n",
    " - Topic #2: algorithm optimization gradient convex stochastic convergence algorithms problem function problems rate optimal functions descent non bounds linear regret method bound\n",
    " - Topic #3: model models inference bayesian latent variational variables gaussian posterior distribution process likelihood probabilistic distributions markov parameters mixture graphical sampling generative\n",
    " - Topic #4: neural network networks deep training convolutional layer layers recurrent architectures architecture trained cnn input image using weights performance memory accuracy\n",
    " - Topic #5: graph graphs nodes node structure edges network spectral random edge embedding graphical community networks problem structures structured based social information\n",
    " - Topic #6: matrix rank low sparse matrices completion tensor norm factorization recovery covariance problem entries decomposition algorithm dimensional noise signal subspace pca\n",
    " - Topic #7: clustering clusters cluster spectral means algorithm data mixture points hierarchical subspace number algorithms based density similarity distance method unsupervised em\n",
    " - Topic #8: kernel kernels regression space gaussian function methods approximation density functions spaces mean nonlinear based learning support vector positive nonparametric distributions\n",
    " - Topic #9: adversarial training generative attacks examples gan gans images samples image robustness distribution classifier trained generate networks target domain generation robust\n",
    "\n",
    "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=18961 and n_features=1000...\n",
    "done in 17.315s.\n",
    "\n",
    "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
    "- Topic #0: results data using paper based proposed propose set used method performance problem methods approach use number new model study provide\n",
    "- Topic #1: neural networks network deep training learning using model trained work state models performance tasks task art used use based classification\n",
    "- Topic #2: optimization algorithm gradient function problem convergence problems stochastic convex algorithms optimal rate results theoretical functions descent loss learning bounds non\n",
    "- Topic #3: models model inference bayesian variational gaussian parameters distribution variables process posterior latent sampling likelihood markov probabilistic distributions probability stochastic variable\n",
    "- Topic #4: matrix sparse dimensional number high low linear structure random problem spectral analysis estimation method matrices recovery noise theoretical signal numerical\n",
    "- Topic #5: time learning problem graph reinforcement world algorithm propose algorithms policy state real based structure graphs present nodes paper information network\n",
    "- Topic #6: model sparse problem real world systems propose low user paper users matrix models rank time state work applications latent sparsity\n",
    "- Topic #7: data method time real propose using proposed large methods paper algorithm world used scale clustering experiments new datasets synthetic analysis\n",
    "- Topic #8: learning methods machine problems paper kernel state based method framework art space approach large recent algorithms novel scale applications approaches\n",
    "- Topic #9: learning training data generative adversarial supervised samples new distribution space unsupervised semi examples work target distributions class sample metric learn\n",
    "\n",
    "Fitting LDA models with tf features, n_samples=18961 and n_features=1000...\n",
    "done in 35.347s.\n",
    "\n",
    "Topics in LDA model:\n",
    "- Topic #0: learning data classification training supervised class method classifier domain methods proposed performance labels datasets classifiers approach label set based space\n",
    "- Topic #1: kernel distribution bayesian gaussian process distributions sampling processes posterior inference variational markov data probability space function model carlo monte using\n",
    "- Topic #2: data based learning time model approach methods using information method different paper features machine real proposed prediction used task systems\n",
    "- Topic #3: optimization gradient algorithm method methods problems stochastic algorithms problem convergence large convex descent propose proposed computational paper based non efficient\n",
    "- Topic #4: learning algorithm algorithms problem policy reinforcement optimal online bound bounds regret agent time setting function based reward value complexity rl\n",
    "- Topic #5: model models data latent inference variables structure approach learning probabilistic variable causal methods bayesian tree using generative predictive conditional propose\n",
    "- Topic #6: loss regression function learning functions regularization linear feature selection risk sparse features convex non vector norm problems machine dictionary lasso\n",
    "- Topic #7: graph graphs random nodes local energy node log matching results algorithms function problem bound number error algorithm convergence prove theory\n",
    "- Topic #8: neural networks network deep training learning adversarial model models convolutional trained layer performance input using architecture generative results layers architectures\n",
    "- Topic #9: data matrix estimation clustering dimensional high algorithm analysis rank low method sample problem noise number estimator based proposed results sparse\n",
    "\n",
    "The topics are somewhat coherent. Still, I think a lot better results can be had from thinking more deeply about what exactly I'm looking for. Ideally I'd like to get topics that are analogous to different topics in Machine Learning (which is the category of these papers). To do this I need to be more intentional about Named Entities, document frequency, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What blockers are you facing, including processing power, data acquisition, modeling difficulties, data cleaning, etc.? How can we help you overcome those challenges?**\n",
    "\n",
    "My blockers at this point are ... time? I'm going to be using the library gensim so I'm going through its documentation (which is thankfully very helpful). I think I'll be all right. If I do eventually bring in entire documents, then processing power will be a bottleneck, at which point I'll have to decide how to implement that on AWS. \n",
    "\n",
    "The other main blocker is thinking about the best way of doing topic modeling x time interaction. I was initially thinking year by year, but how do I relate those n topics/year to the n topics for all the other years? This is worth discussing with the globals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Have you changed topics since your lightning talk? Since you submitted your Problem Statement and EDA? If so, do you have the necessary data in hand (and the requisite EDA completed) to continue moving forward?**\n",
    "\n",
    "yes and yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is your timeline for the next week and a half? What do you have to get done versus what would you like to get done?**\n",
    "\n",
    "What I have to get done is the pre-processing of my corpora. That is the hardest and most important part of NLP. If I can do it in such a way where I get coherent, differentiable topics, I will have an important part of my project done. \n",
    "\n",
    "After that point, my next need-to-do is w.r.t time-based analysis of topics. How to split up the corpus into year-by-year, and then create a different topic matrix for every year, and then thinking about how to relate the topic clusters.\n",
    "\n",
    "Other than that, I should do document similarities. Which document is most representative of a given topic? Can I create a metric of document similarities based on shared topics?\n",
    "\n",
    "_Want to Haves:_\n",
    "\n",
    "- A sweet UI to be able to query topics and related documents\n",
    "- Use the actual papers as my corpus, not just the abstracts\n",
    "- Do a cross- analysis of topic representation and citation-analysis (which papers are the most seminal in a given topic)\n",
    "- moar data (use the UW Libraries subscription to download huge datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What topics do you want to discuss during your 1:1?**\n",
    "\n",
    "I'd like to talk about the progress that I've made so far, and focus on my key deliverables for an effective capstone presentation (i.e. what are some realistic goals, and what are some good reach goals)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
