{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello there! My capstone project has to do with the field of Natural Language Programming (NLP) known as topic modeling (TM). ##\n",
    "\n",
    "It involves extracting the key words that summarize _what_ exactly a document is about, especially within the context of a larger corpus of documents.\n",
    "\n",
    "To this end, I am using the `gensim` library which specializes in efficient implementations of TM algorithms. Specifically, I will be using Latent Dirichlet Allocation (LDA), a popular model that generates _t_ discriminative topics based on assumptions about the number of topics and their distributions throughout the corpus.\n",
    "\n",
    "The corpus I am working with is from the `stat.ML` category of the [arXiv](https://arxiv.org/list/stat.ML/recent) database. I have about 18,000 paper abstracts covering a range of Machine Learning topics such as Optimization, Neural Network Architecture, and Applications to medical data.\n",
    "\n",
    "### Problem : ### \n",
    "**Within the context of an overarching topic, how can we extract more detailed subtopics and provide similar documents/recommendations effectively? (Secondary Problem) Can we use Dynamic Topic Modeling to show the papers that have been most influential in their fields?**\n",
    "\n",
    "### Data: ###\n",
    "**I used the same data I collected from Milestone #2 (with function arx_and_recreation()). I isolate just the abstracts (no date for now) in a Pandas Series or list**\n",
    "\n",
    "### Pre-Processing : ###\n",
    "\n",
    "I use a combination of \n",
    "    - regular expressions : strip extraneous punctuation/formatting/etc. \n",
    "    - English stop words : strip useless connecting words\n",
    "    - lemmatizers : combine variations of words (tense, pluaral) into one\n",
    "    - phrasers : common words that go together (like bigrams) are combined into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processor:\n",
    "import re\n",
    "import gensim.parsing.preprocessing as genpre\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lmtzr= WordNetLemmatizer()\n",
    "def prep_text(text):\n",
    "     #this regex removes LATEX formatting, numbers, citations, splits hyphens into two words\n",
    "    myreg=r'\\\\[\\w]+[\\{| ]|\\$[^\\$]+\\$|\\(.+\\, *\\d{2,4}\\w*\\)|\\S*\\/\\/\\S*|[\\\\.,\\/#!$%\\^&\\*;:{}=_`\\'\\\"~()><\\|]|\\[.+\\]|\\d+|\\b\\w{1,2}\\b'\n",
    "    parsed_data = text.replace('-', ' ')\n",
    "    parsed_data = re.sub(myreg, '', parsed_data)\n",
    "    parsed_data = [lmtzr.lemmatize(w) for w in parsed_data.lower().split() if w not in genpre.STOPWORDS]\n",
    "    if len(parsed_data) ==1: return parsed_data[0]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having transformed all my abstracts from things that looked like this:\n",
    "```\n",
    "In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for empirical risk minimizers. In the general setting, we prove sanity-check bounds in the spirit of \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case error of this estimate is not much worse that of training error estimate} \\textquotedblright . General loss functions and class of predictors with finite VC-dimension are considered. We closely follow the formalism introduced by \\cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$% -fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\\upsilon$-out cross-validation.   In particular, we focus on proving the consistency of the various cross-validation procedures. We point out the interest of each cross-validation procedure in terms of rate of convergence. An estimation curve with transition phases depending on the cross-validation procedure and not only on the percentage of observations in the test sample gives a simple rule on how to choose the cross-validation. An interesting consequence is that the size of the test sample is not required to grow to infinity for the consistency of the cross-validation procedure.\n",
    "```\n",
    "to this (it is not perfect but the odd strings that get thru the cracks are too rare to be of any effect):\n",
    "```\n",
    "['article', 'derive', 'concentration', 'inequality', 'cross', 'validation', 'estimate', 'generalization', 'error', 'empirical', 'risk', 'minimizers', 'general', 'setting', 'prove', 'sanity', 'check', 'bound', 'spirit', 'kr', 'textquotedblleftbounds', 'showing', 'worst', 'case', 'error', 'estimate', 'worse', 'training', 'error', 'estimate', 'general', 'loss', 'function', 'class', 'predictor', 'finite', 'dimension', 'considered', 'closely', 'follow', 'formalism', 'introduced', 'dud', 'cover', 'large', 'variety', 'cross', 'validation', 'procedure', 'including', 'leave', 'cross', 'validation', 'fold', 'cross', 'validation', 'hold', 'cross', 'validation', 'split', 'sample', 'leave', 'cross', 'validation', 'particular', 'focus', 'proving', 'consistency', 'cross', 'validation', 'procedure', 'point', 'cross', 'validation', 'procedure', 'term', 'rate', 'convergence', 'estimation', 'curve', 'transition', 'phase', 'depending', 'cross', 'validation', 'procedure', 'percentage', 'observation', 'test', 'sample', 'give', 'simple', 'rule', 'choose', 'cross', 'validation', 'interesting', 'consequence', 'size', 'test', 'sample', 'required', 'grow', 'infinity', 'consistency', 'cross', 'validation', 'procedure']\n",
    "```\n",
    "\n",
    "I am now in good position to conduct a topic analysis of my pre-processed text. I would like to do some Exploratory Data Analysis, too, to see what the distribution of my words are, etc. But first, let's examine the key gensim object with which we will be working:\n",
    "\n",
    "    - Corpus: this is the list of list of tokens/n_grams (a list such as above for every abstract)\n",
    "    - Dictionary: this assigns every unique token an id. It is used to look up id->token and token->id\n",
    "    - Bag Of Words (BoW): sums the total occurances of a token in a document (word ordering is not considered)\n",
    "    - LdaModel: uses the dictionary and BOW to generate a probablity distribution of topics across docs, and of words across topics\n",
    "    - Market Matrix: a more efficient way of storing th corpus, useful when calculating similarities between documents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the relevant objects\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, TfidfModel\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# The raw documents: \n",
    "abstracts = pd.read_csv('./new_hope_data/arxiv_csML.csv')['summary']\n",
    "\n",
    "with open('./the_data_strikes_back/bigrams', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    "\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "bow = [dictionary.doc2bow(text) for text in corpus]\n",
    "\n",
    "# The gensim phraser I'm using\n",
    "bigrams = gensim.utils.SaveLoad.load('./the_data_strikes_back/bigram_phrases')\n",
    "#define the lda model\n",
    "model = LdaModel(bow, id2word=dictionary, num_topics=5)\n",
    "\n",
    "# Matrix representation of my corpus\n",
    "# corpora.MmCorpus.serialize('./the_data_strikes_back/full_bigram_corpus.mm', corpus)\n",
    "corp_matrix = corpora.MmCorpus('./the_data_strikes_back/full_bigram_corpus.mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# let's look at the most common words throughout the corpus\n",
    "tot_counts = np.zeros(len(dictionary))\n",
    "\n",
    "# goes through the (id, count) tuples in every document and increments a total count\n",
    "for doc in bow:\n",
    "    for w in doc:\n",
    "        tot_counts[w[0]] += w[1]\n",
    "\n",
    "masterlist = [(dictionary[i], tot_counts[i]) for i in range(len(dictionary))]\n",
    "\n",
    "# the most common\n",
    "reference = sorted(masterlist, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 23446.0),\n",
       " ('method', 18755.0),\n",
       " ('algorithm', 17368.0),\n",
       " ('data', 16627.0),\n",
       " ('problem', 11707.0),\n",
       " ('based', 10494.0),\n",
       " ('approach', 10204.0),\n",
       " ('learning', 10175.0),\n",
       " ('network', 9262.0),\n",
       " ('result', 6774.0),\n",
       " ('proposed', 6144.0),\n",
       " ('performance', 5449.0),\n",
       " ('function', 5355.0),\n",
       " ('task', 5323.0),\n",
       " ('feature', 5292.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[:15]\n",
    "# we can see in just these top 15 there is a steep fall-off in descending term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25533"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ref for ref in reference if ref[1] <=10])\n",
    "# In contrast, there are around 25K words that occur 10 times or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "max_ammt = (18/10)*1000 # set this to 25000 to see the whole thing\n",
    "prevalent_counts = [ref[1] for ref in reference if ref[1] <= max_ammt] \n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(prevalent_counts, bins=100);\n",
    "# plt.xlim(0, 5000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By visually inspecting this historgram of word terms, we can see that it is probably a good idea to filter the extreme terms. Words like \"model, algorithm\" on the high end, or \"solomonoff\" and \"bssd\" (which only appear twice) do not contribute to our understanding of a topic. We should instead use the central mass of our distribution as our words. Luckily, gensim makes it easy to do this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to figure out approx. how many words occur more than 1800 times (10% of docs, assuming it is a common ML term and not something special)\n",
    "max_total = (1/6) # the maximum document frequency I'd like -I'm going to set it roughly equal to 1/num_topics so I can get better overlap\n",
    "ref_cutoff = len([ref for ref in reference if ref[1] > len(corpus)*max_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(corpus)\n",
    "dictionary.filter_extremes(no_below=50) # remove terms that occur <50 times\n",
    "high_tokens = [dictionary.token2id[reference[i][0]] for i in range(ref_cutoff)] # ids of tokens that'll be excised\n",
    "dictionary.filter_tokens(bad_ids=high_tokens) # our 97 most common term ids, approximating words that occur more than once/10 documents\n",
    "filtered_corp = [[w for w in doc if w in dictionary.token2id] for doc in corpus]\n",
    "filterd_bow = [dictionary.doc2bow(doc) for doc in filtered_corp]\n",
    "\n",
    "# now let's check our distributions again\n",
    "# let's look at the most common words throughout the corpus\n",
    "new_tot_counts = np.zeros(len(dictionary))\n",
    "\n",
    "# goes through the (id, count) tuples in every document and increments a total count\n",
    "for doc in filterd_bow:\n",
    "    for w in doc:\n",
    "        new_tot_counts[w[0]] += w[1]\n",
    "\n",
    "new_masterlist = [(dictionary[i], new_tot_counts[i]) for i in range(len(dictionary))]\n",
    "\n",
    "# the most common\n",
    "new_reference = sorted(new_masterlist, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "new_prevalent_counts = [ref[1] for ref in new_reference]\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(new_prevalent_counts, bins=100);\n",
    "# plt.xlim(0, 5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"neural_network\" + 0.007*\"classifier\" + 0.007*\"deep_learning\" + 0.006*\"input\" + 0.006*\"accuracy\" + 0.006*\"image\" + 0.006*\"deep_neural\" + 0.005*\"architecture\" + 0.005*\"deep\" + 0.005*\"datasets\"'),\n",
       " (1,\n",
       "  '0.011*\"gradient\" + 0.010*\"optimization\" + 0.008*\"solution\" + 0.006*\"convergence\" + 0.006*\"space\" + 0.006*\"stochastic\" + 0.005*\"linear\" + 0.005*\"approximation\" + 0.005*\"point\" + 0.005*\"vector\"'),\n",
       " (2,\n",
       "  '0.012*\"estimator\" + 0.011*\"estimation\" + 0.010*\"rate\" + 0.009*\"bound\" + 0.009*\"error\" + 0.009*\"regression\" + 0.009*\"setting\" + 0.008*\"sparse\" + 0.007*\"estimate\" + 0.006*\"optimal\"'),\n",
       " (3,\n",
       "  '0.013*\"representation\" + 0.009*\"node\" + 0.007*\"inference\" + 0.006*\"time_series\" + 0.006*\"learn\" + 0.005*\"datasets\" + 0.005*\"multiple\" + 0.005*\"state_art\" + 0.005*\"dynamic\" + 0.005*\"complex\"'),\n",
       " (4,\n",
       "  '0.033*\"kernel\" + 0.010*\"approximation\" + 0.009*\"student\" + 0.008*\"inference\" + 0.008*\"topic\" + 0.008*\"prior\" + 0.007*\"gaussian_process\" + 0.007*\"bayesian\" + 0.007*\"process\" + 0.006*\"outlier\"'),\n",
       " (5,\n",
       "  '0.011*\"policy\" + 0.010*\"agent\" + 0.008*\"user\" + 0.008*\"state\" + 0.007*\"environment\" + 0.007*\"reinforcement_learning\" + 0.006*\"dynamic\" + 0.006*\"process\" + 0.006*\"system\" + 0.006*\"reward\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lda = LdaModel(corpus=filterd_bow, num_topics=6, id2word=dictionary, alpha= 'asymmetric', eta=.01)\n",
    "filtered_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My rationale for the `alpha` and `eta` parameters are as follows:**\n",
    "\n",
    "$\\alpha$ is a LDA parameter that relates to how evenly distributed topics are across documents. By default, the model assumes a document is approximately equally likely to have any topic. However, this is not the case here because all my documents relate to Machine Learning. So there are generic ML terms (and topics) that may be shared across more documents than others. Thus I set `alpha = 'asymmetric'`\n",
    "\n",
    "$\\eta$ in gensim is analogous to the parameter $\\beta$ in the formal LDA model. It signifies how many topics share a word. Low `eta` means that the model favors term exclusivity among topics. Again, because my corpus is so overlapping in nature, I want to set `eta` to be low\n",
    "\n",
    "Next up...\n",
    "\n",
    "## Visualizing the topics using pyLDAvis ##\n",
    "\n",
    "While some people (*ahem* Dave Yarrington *ahem*) question its statistcal rigor, the pyLDAvis library is a very popular way of getting a high-level intuition on the layout of your topics model. Of primary concern to me is the horizontal bar graph on the right, which shows a list of tokens in a given topic along with their relative frequencies in the corpus. \n",
    "\n",
    "By visually inspecting the tokens, you can see several topics emerge: \n",
    "    - Reinforcement Learning\n",
    "    - Gradient Descent\n",
    "    - Neural Networks (may combine or split up CNN, RNN, VAE, GAN into two)\n",
    "    - Medical Applications of ML\n",
    "    - Baysian/ Probablistic Methods and Estimators\n",
    "\n",
    "Depending on whether you ran the cell or not, you may get significantly different-looking topic distribution. This is becauxe LDA begins with a random seed every time it runs \n",
    "\n",
    "Of particular note is the relevance metric slider. When $\\lambda$ is turned low (around `0.1 - 0.2`), more discriminative words become ranked higher, even though they are not the most common. **Thus, I plan to manually inspect my topics to create a human-understandable label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el98571406002433036726943135956\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el98571406002433036726943135956_data = {\"mdsDat\": {\"x\": [-0.03652077308602618, 0.09182865789029314, 0.12814316773632747, -0.10786471748470958, 0.06337796253790849, -0.13896429759379303], \"y\": [0.004829344343861243, 0.006358848977616706, 0.07473638922144667, -0.06672064269827778, -0.09810099043120905, 0.0788970505865622], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [28.493940353393555, 17.189313888549805, 15.95258903503418, 14.144185066223145, 9.226558685302734, 14.993412971496582]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [3301.0, 1769.0, 2200.0, 1458.0, 3330.0, 2392.0, 2758.0, 1583.0, 2547.0, 814.0, 1606.0, 2603.0, 1168.0, 2379.0, 2248.0, 2221.0, 923.0, 2632.0, 1592.0, 3385.0, 2762.0, 2540.0, 831.0, 998.0, 1186.0, 1581.0, 829.0, 668.0, 1901.0, 1990.0, 261.49127197265625, 135.4268798828125, 182.85763549804688, 174.45591735839844, 99.5157241821289, 83.09378814697266, 239.44189453125, 77.02363586425781, 65.49629211425781, 67.55291748046875, 50.05603790283203, 45.24501037597656, 95.1006088256836, 117.28532409667969, 107.50568389892578, 39.38325119018555, 172.52529907226562, 79.7582778930664, 119.82036590576172, 80.39306640625, 354.9331970214844, 153.80511474609375, 183.01983642578125, 75.23849487304688, 133.22425842285156, 188.3191375732422, 73.60877990722656, 46.6207275390625, 48.63473129272461, 990.0955200195312, 175.1685791015625, 124.14765167236328, 152.78536987304688, 1518.5347900390625, 173.44566345214844, 1800.484130859375, 264.2674255371094, 450.0011901855469, 157.560546875, 164.6245574951172, 1266.7978515625, 769.5177001953125, 249.0139617919922, 758.4265747070312, 1800.1888427734375, 1435.3101806640625, 2579.2119140625, 1359.313720703125, 448.57574462890625, 207.18075561523438, 1387.231201171875, 934.5560302734375, 1703.6317138671875, 601.1294555664062, 505.24945068359375, 1657.907470703125, 1294.9361572265625, 1533.755859375, 341.49078369140625, 1365.8310546875, 1027.1168212890625, 367.2307434082031, 906.1868896484375, 1099.8597412109375, 680.2797241210938, 826.1699829101562, 779.036865234375, 882.9926147460938, 762.8436889648438, 697.0616455078125, 827.1155395507812, 874.5576171875, 890.2583618164062, 847.3944702148438, 851.3261108398438, 824.5708618164062, 790.1665649414062, 124.39857482910156, 106.40740966796875, 57.23216247558594, 236.78016662597656, 108.8211441040039, 103.92998504638672, 162.23202514648438, 53.05970764160156, 195.6566162109375, 113.59393310546875, 114.51966857910156, 174.4514923095703, 88.24030303955078, 119.13672637939453, 183.5634002685547, 801.2850341796875, 74.2183609008789, 44.630313873291016, 63.80326461791992, 140.5888671875, 94.55492401123047, 147.41444396972656, 365.3892822265625, 89.75303649902344, 48.66043472290039, 72.42431640625, 71.7600326538086, 57.02702713012695, 152.74827575683594, 165.1713409423828, 164.03173828125, 465.89794921875, 605.1030883789062, 298.4971618652344, 1030.540771484375, 414.2643737792969, 1803.561767578125, 226.29129028320312, 262.6463623046875, 798.4429321289062, 662.5992431640625, 417.5768737792969, 275.2087707519531, 1642.6495361328125, 598.8300170898438, 444.53619384765625, 471.2174987792969, 626.6264038085938, 1308.4425048828125, 996.6962280273438, 499.2322692871094, 262.9532775878906, 264.2288818359375, 728.0656127929688, 545.4482421875, 862.5885009765625, 519.1395263671875, 710.8696899414062, 888.5859375, 879.0733642578125, 1008.8924560546875, 868.5, 628.0553588867188, 603.8113403320312, 580.878173828125, 730.5852661132812, 737.1425170898438, 605.7261352539062, 597.3873901367188, 597.664794921875, 587.0746459960938, 591.1317138671875, 570.1586303710938, 564.6375122070312, 566.678466796875, 344.09515380859375, 157.23199462890625, 105.32450866699219, 229.18252563476562, 108.0897216796875, 58.23988342285156, 46.941768646240234, 75.17010498046875, 327.17364501953125, 181.88165283203125, 85.68115997314453, 118.0905532836914, 421.5499572753906, 75.53572845458984, 136.6537628173828, 53.4130744934082, 91.01792907714844, 84.80921173095703, 169.10093688964844, 59.25601577758789, 472.7647705078125, 67.83303833007812, 137.0345458984375, 308.5791931152344, 67.9030532836914, 297.7615661621094, 82.5271987915039, 663.4357299804688, 98.04121398925781, 334.45684814453125, 336.0608215332031, 340.2144775390625, 1798.015869140625, 228.814453125, 222.0809783935547, 1533.6607666015625, 1434.1270751953125, 1694.1986083984375, 414.3617248535156, 754.9860229492188, 354.57501220703125, 500.5150451660156, 1400.8123779296875, 246.4988250732422, 1411.095947265625, 1296.8699951171875, 596.3389892578125, 742.6184692382812, 848.4507446289062, 1318.634521484375, 858.8931884765625, 933.8684692382812, 899.8964233398438, 841.5658569335938, 1026.974609375, 705.0884399414062, 511.66119384765625, 504.3074951171875, 531.3260498046875, 731.7327880859375, 665.6038818359375, 865.2905883789062, 783.72802734375, 610.3548583984375, 572.6212768554688, 699.3347778320312, 566.7398681640625, 606.0983276367188, 571.77294921875, 216.50827026367188, 122.03426361083984, 138.1572265625, 111.35610961914062, 64.05957794189453, 143.5223388671875, 88.82037353515625, 54.84659194946289, 76.92362976074219, 143.72386169433594, 154.131103515625, 212.39767456054688, 88.27883911132812, 111.33362579345703, 283.3081359863281, 307.8022155761719, 115.63337707519531, 252.6348876953125, 384.2578125, 168.42897033691406, 119.29816436767578, 54.731597900390625, 121.88201141357422, 47.36123275756836, 63.32063293457031, 118.07066345214844, 81.69794464111328, 161.6050567626953, 97.48899841308594, 190.50750732421875, 346.568115234375, 395.7748718261719, 1283.989013671875, 172.83602905273438, 575.44970703125, 469.3512878417969, 853.609375, 604.6222534179688, 509.6995849609375, 470.3583679199219, 1821.63623046875, 407.5542297363281, 189.94935607910156, 449.83428955078125, 400.2454528808594, 544.2911987304688, 345.3322448730469, 978.1701049804688, 290.8827209472656, 381.3057556152344, 775.55224609375, 287.6419677734375, 634.6856689453125, 533.8466796875, 403.05841064453125, 647.4515991210938, 684.9166259765625, 573.1210327148438, 660.816162109375, 511.3009338378906, 697.3026123046875, 421.1399230957031, 440.8691101074219, 529.115478515625, 534.6948852539062, 517.7489013671875, 442.5049133300781, 474.075927734375, 466.0761413574219, 429.58746337890625, 424.15985107421875, 529.9563598632812, 257.814208984375, 95.54283142089844, 791.0418090820312, 130.6647491455078, 292.5787658691406, 51.4857063293457, 84.24758911132812, 377.83343505859375, 125.15941619873047, 2968.997802734375, 64.5625, 53.97688674926758, 577.9580078125, 89.65351104736328, 141.79896545410156, 68.44857025146484, 215.57186889648438, 61.16387939453125, 571.8772583007812, 335.12506103515625, 23.693571090698242, 123.5459976196289, 714.18505859375, 80.2096176147461, 78.7671127319336, 55.66225814819336, 109.03466033935547, 60.735633850097656, 84.10588836669922, 321.8275146484375, 158.0931854248047, 301.3612060546875, 645.5119018554688, 180.0912322998047, 928.0148315429688, 357.0853576660156, 708.6475219726562, 623.65673828125, 199.86692810058594, 191.5392608642578, 132.98851013183594, 714.2178344726562, 476.6748352050781, 571.4214477539062, 470.0477600097656, 582.16064453125, 465.39141845703125, 276.8096008300781, 470.7791748046875, 316.6691589355469, 442.4158020019531, 440.7305603027344, 429.4107360839844, 386.0928649902344, 381.0767822265625, 370.8500061035156, 336.1770324707031, 330.0926818847656, 320.6534729003906, 367.72076416015625, 61.616878509521484, 129.85235595703125, 111.74536895751953, 48.399269104003906, 180.25320434570312, 101.61815643310547, 114.31388854980469, 127.278076171875, 169.9040069580078, 154.5277557373047, 80.46994018554688, 212.2207489013672, 814.831298828125, 61.06935501098633, 96.9044189453125, 630.4625854492188, 148.82188415527344, 119.69502258300781, 264.47357177734375, 61.21951675415039, 150.509765625, 795.38720703125, 99.9741439819336, 136.21636962890625, 1381.27099609375, 64.32066345214844, 79.91020965576172, 143.64413452148438, 240.75421142578125, 1648.0615234375, 341.1421203613281, 304.9820251464844, 1053.4635009765625, 218.2765655517578, 169.95152282714844, 386.0523986816406, 303.0483093261719, 966.9595336914062, 237.31369018554688, 302.4173889160156, 408.9115905761719, 287.06451416015625, 247.11683654785156, 1153.4810791015625, 1133.06494140625, 254.57965087890625, 418.9279479980469, 412.06787109375, 599.255859375, 706.5249633789062, 835.3701171875, 869.1563720703125, 516.4254760742188, 656.3157348632812, 483.6567687988281, 562.0880126953125, 524.7713623046875, 574.3928833007812, 861.5571899414062, 436.7996826171875, 369.6385192871094, 582.3048095703125, 451.48809814453125, 666.7463989257812, 557.1741943359375, 587.2274169921875, 546.3486328125, 498.9895935058594, 503.3064880371094, 431.045654296875, 447.8204650878906, 451.32965087890625], \"Term\": [\"kernel\", \"policy\", \"estimator\", \"agent\", \"representation\", \"approximation\", \"estimation\", \"node\", \"gradient\", \"student\", \"user\", \"inference\", \"environment\", \"regression\", \"rate\", \"bound\", \"topic\", \"error\", \"deep_neural\", \"neural_network\", \"optimization\", \"sparse\", \"reward\", \"gaussian_process\", \"reinforcement_learning\", \"state\", \"action\", \"outlier\", \"prior\", \"classifier\", \"hardware\", \"attention_mechanism\", \"fully_connected\", \"attacker\", \"company\", \"labelled\", \"activation_function\", \"vulnerable\", \"gradually\", \"processor\", \"collective\", \"adoption\", \"drive\", \"gpu\", \"vulnerability\", \"hypothesis_test\", \"x\", \"softmax\", \"speech_recognition\", \"decision_boundary\", \"adversary\", \"shallow\", \"imagenet\", \"logic\", \"opportunity\", \"meta_learning\", \"training_testing\", \"cpu\", \"precision_recall\", \"attack\", \"mnist\", \"auc\", \"cifar\", \"deep_neural\", \"source_domain\", \"classifier\", \"explanation\", \"recurrent_neural\", \"decision_tree\", \"target_domain\", \"layer\", \"adversarial\", \"random_forest\", \"ensemble\", \"deep_learning\", \"architecture\", \"neural_network\", \"trained\", \"query\", \"domain_adaptation\", \"deep\", \"train\", \"input\", \"memory\", \"convolutional_neural\", \"accuracy\", \"test\", \"image\", \"black_box\", \"datasets\", \"example\", \"generator\", \"domain\", \"state_art\", \"output\", \"design\", \"weight\", \"experiment\", \"label\", \"high\", \"dataset\", \"large\", \"demonstrate\", \"present\", \"existing\", \"provide\", \"setting\", \"augment\", \"primal_dual\", \"global_convergence\", \"nmf\", \"saddle_point\", \"nonconvex_optimization\", \"hessian\", \"strong_convexity\", \"linear_convergence\", \"ica\", \"inversion\", \"svd\", \"primal\", \"fourier\", \"local_minimum\", \"manifold\", \"nonsmooth\", \"row_column\", \"smooth_function\", \"letter\", \"euclidean_space\", \"dictionary_learning\", \"vertex\", \"global_minimum\", \"nonnegative_matrix\", \"sparse_coding\", \"global_optimum\", \"block_coordinate\", \"stationary_point\", \"strongly_convex\", \"proximal\", \"descent\", \"non_convex\", \"geometry\", \"convergence\", \"dictionary\", \"gradient\", \"admm\", \"step_size\", \"stochastic_gradient\", \"gradient_descent\", \"equation\", \"initialization\", \"optimization\", \"convex\", \"objective_function\", \"optimization_problem\", \"solving\", \"solution\", \"stochastic\", \"iteration\", \"nonconvex\", \"convex_optimization\", \"constraint\", \"approximate\", \"vector\", \"distance\", \"prove\", \"linear\", \"approximation\", \"space\", \"point\", \"sampling\", \"theory\", \"complexity\", \"case\", \"provide\", \"property\", \"particular\", \"simple\", \"order\", \"given\", \"general\", \"known\", \"representation\", \"mini_batch\", \"character\", \"frank_wolfe\", \"team\", \"exact_recovery\", \"regression_coefficient\", \"oracle_inequality\", \"minimax_rate\", \"descent_sgd\", \"lipschitz\", \"achieves_optimal\", \"elastic_net\", \"quantization\", \"mixture_component\", \"pac\", \"excess_risk\", \"logarithmic_factor\", \"sparse_signal\", \"identifiability\", \"noise_level\", \"penalty\", \"minimax_optimal\", \"nuclear_norm\", \"covariates\", \"byproduct\", \"finite_sample\", \"gaussian_graphical\", \"sgd\", \"instability\", \"linear_regression\", \"quantum\", \"lasso\", \"estimator\", \"inequality\", \"simulation_study\", \"rate\", \"bound\", \"estimation\", \"regime\", \"low_rank\", \"recovery\", \"causal\", \"regression\", \"matrix_completion\", \"error\", \"sparse\", \"measurement\", \"regularization\", \"condition\", \"setting\", \"high_dimensional\", \"optimal\", \"noise\", \"statistical\", \"estimate\", \"loss\", \"norm\", \"mixture\", \"risk\", \"variable\", \"prove\", \"case\", \"linear\", \"consider\", \"assumption\", \"provide\", \"procedure\", \"general\", \"known\", \"skill\", \"text_classification\", \"machine_translation\", \"social_medium\", \"feedforward_neural\", \"optimal_transport\", \"python\", \"inform\", \"textual\", \"user_item\", \"link_prediction\", \"reward_function\", \"domain_specific\", \"behavioral\", \"sentence\", \"entity\", \"prevent\", \"multimodal\", \"latent_space\", \"policy_gradient\", \"contextual_bandit\", \"flexibly\", \"latent_representation\", \"exchange\", \"website\", \"wasserstein_distance\", \"twitter\", \"calibration\", \"recommender\", \"recommender_system\", \"semantic\", \"forecasting\", \"node\", \"forecast\", \"event\", \"language\", \"time_series\", \"latent\", \"text\", \"neural\", \"representation\", \"community\", \"mcmc\", \"embeddings\", \"generative_model\", \"probabilistic\", \"generation\", \"inference\", \"content\", \"latent_variable\", \"learn\", \"generative\", \"complex\", \"metric\", \"capture\", \"dynamic\", \"multiple\", \"real_world\", \"state_art\", \"learned\", \"datasets\", \"distributed\", \"modeling\", \"novel\", \"existing\", \"demonstrate\", \"context\", \"process\", \"space\", \"dataset\", \"present\", \"recurrent\", \"epoch\", \"deliver\", \"student\", \"subsampling\", \"drug\", \"expectation_propagation\", \"subspace_clustering\", \"teacher\", \"basis_function\", \"kernel\", \"topic_modeling\", \"gibbs_sampler\", \"outlier\", \"tail\", \"bootstrap\", \"variational_bayes\", \"multi_label\", \"latent_dirichlet\", \"subspace\", \"pca\", \"allocation_lda\", \"gamma\", \"topic\", \"gibbs_sampling\", \"variational_approximation\", \"reduce_computational\", \"outlier_detection\", \"prominent\", \"approximate_posterior\", \"variational_inference\", \"reproducing_kernel\", \"covariance\", \"gaussian_process\", \"bayesian_inference\", \"approximation\", \"variational\", \"prior\", \"bayesian\", \"hyperparameters\", \"gps\", \"chain\", \"inference\", \"label\", \"point\", \"robust\", \"process\", \"noise\", \"spectral\", \"estimation\", \"data_set\", \"estimate\", \"datasets\", \"existing\", \"sparse\", \"error\", \"regression\", \"term\", \"test\", \"demonstrate\", \"robot\", \"uncertain\", \"shot\", \"state_action\", \"unprecedented\", \"customer\", \"longitudinal\", \"manipulation\", \"molecular\", \"age\", \"health\", \"city\", \"vehicle\", \"reward\", \"acquire\", \"mdp\", \"patient\", \"speaker\", \"person\", \"planning\", \"phenotype\", \"cancer\", \"action\", \"road\", \"cloud\", \"agent\", \"stimulus\", \"bandit_problem\", \"spatio_temporal\", \"long_term\", \"policy\", \"disease\", \"player\", \"environment\", \"arm\", \"protein\", \"sensor\", \"d\", \"reinforcement_learning\", \"medical\", \"brain\", \"trajectory\", \"clinical\", \"traffic\", \"user\", \"state\", \"activity\", \"spatial\", \"game\", \"object\", \"human\", \"system\", \"dynamic\", \"detection\", \"behavior\", \"change\", \"pattern\", \"interaction\", \"control\", \"process\", \"predict\", \"temporal\", \"clustering\", \"goal\", \"image\", \"complex\", \"value\", \"dataset\", \"sequence\", \"learn\", \"decision\", \"real\", \"present\"], \"Total\": [3301.0, 1769.0, 2200.0, 1458.0, 3330.0, 2392.0, 2758.0, 1583.0, 2547.0, 814.0, 1606.0, 2603.0, 1168.0, 2379.0, 2248.0, 2221.0, 923.0, 2632.0, 1592.0, 3385.0, 2762.0, 2540.0, 831.0, 998.0, 1186.0, 1581.0, 829.0, 668.0, 1901.0, 1990.0, 261.5550537109375, 135.4897918701172, 182.965576171875, 174.56411743164062, 99.57867431640625, 83.15670013427734, 239.638671875, 77.08747863769531, 65.5592041015625, 67.6349105834961, 50.118934631347656, 45.30918884277344, 95.23600006103516, 117.48776245117188, 107.69901275634766, 39.4654655456543, 173.03892517089844, 80.01294708251953, 120.23216247558594, 80.68244934082031, 356.6767883300781, 154.8614959716797, 184.41493225097656, 75.95939636230469, 134.59616088867188, 190.51486206054688, 74.55504608154297, 47.240421295166016, 49.3910026550293, 1005.753662109375, 178.57760620117188, 126.12864685058594, 155.7976531982422, 1592.228271484375, 178.02008056640625, 1990.8668212890625, 276.23211669921875, 481.68597412109375, 162.10272216796875, 169.7212677001953, 1434.15673828125, 849.8187866210938, 262.6163330078125, 867.7572021484375, 2204.779296875, 1739.0364990234375, 3385.61083984375, 1690.62451171875, 508.6031494140625, 218.47300720214844, 1826.7366943359375, 1226.29443359375, 2474.467041015625, 753.40625, 641.9312744140625, 2792.203369140625, 2083.736572265625, 2752.395263671875, 430.41473388671875, 2879.57373046875, 2121.95068359375, 490.16265869140625, 1852.0592041015625, 2493.9560546875, 1274.0555419921875, 1807.185302734375, 1660.32275390625, 2087.6796875, 1643.8826904296875, 1455.0146484375, 2102.27099609375, 2471.517822265625, 2608.8017578125, 2686.0712890625, 2887.390625, 3069.789794921875, 3078.858154296875, 124.45585632324219, 106.76600646972656, 58.227294921875, 241.4792938232422, 111.66438293457031, 106.6783218383789, 166.58648681640625, 54.581275939941406, 201.7198486328125, 117.8863525390625, 119.98002624511719, 184.2215576171875, 93.24415588378906, 126.79617309570312, 195.55909729003906, 855.851318359375, 79.32646942138672, 47.86728286743164, 68.81446075439453, 152.30491638183594, 102.47364807128906, 159.9641571044922, 397.4792785644531, 97.84009552001953, 53.05388259887695, 79.18508911132812, 78.81427764892578, 63.5247917175293, 170.33554077148438, 187.28848266601562, 186.2098388671875, 555.339111328125, 751.3782348632812, 357.7879638671875, 1349.0362548828125, 507.78076171875, 2547.78173828125, 268.0543518066406, 318.4212341308594, 1103.08349609375, 898.12548828125, 543.6029052734375, 339.03118896484375, 2762.5654296875, 902.0476684570312, 637.723876953125, 688.08740234375, 986.0492553710938, 2506.381103515625, 1795.513671875, 774.8706665039062, 341.1378173828125, 358.9686584472656, 1504.4580078125, 999.45458984375, 1960.3646240234375, 955.133056640625, 1621.64892578125, 2406.3857421875, 2392.41845703125, 3001.430908203125, 2417.146484375, 1535.212646484375, 1601.6136474609375, 1484.2486572265625, 2932.238525390625, 3069.789794921875, 2066.355712890625, 1967.305908203125, 2017.171630859375, 2200.780029296875, 2534.860595703125, 2019.234619140625, 2115.86865234375, 3330.82421875, 344.14849853515625, 157.29742431640625, 105.37794494628906, 230.87216186523438, 109.3445053100586, 59.63998794555664, 48.1278076171875, 77.22452545166016, 336.1634216308594, 190.9999237060547, 90.31948852539062, 125.25404357910156, 451.6077575683594, 81.0892105102539, 148.86155700683594, 58.25521469116211, 100.41678619384766, 93.61189270019531, 189.38645935058594, 66.4961166381836, 530.806396484375, 76.32966613769531, 156.6305389404297, 354.6496276855469, 78.48171997070312, 348.00146484375, 96.93354034423828, 780.6549682617188, 115.66334533691406, 394.8011779785156, 397.79986572265625, 404.22174072265625, 2200.93115234375, 275.1963806152344, 273.09442138671875, 2248.585693359375, 2221.38671875, 2758.376220703125, 556.8173217773438, 1107.7667236328125, 471.0616149902344, 702.29052734375, 2379.894287109375, 312.1410217285156, 2632.42529296875, 2540.23291015625, 1009.3682861328125, 1355.9332275390625, 1633.8846435546875, 3078.858154296875, 1771.8040771484375, 2069.68701171875, 2083.716552734375, 1890.296630859375, 2642.24365234375, 1589.2950439453125, 902.9379272460938, 882.6087036132812, 982.7735595703125, 1944.07080078125, 1621.64892578125, 2932.238525390625, 2406.3857421875, 1450.0045166015625, 1346.0120849609375, 3069.789794921875, 1426.5826416015625, 2019.234619140625, 2115.86865234375, 216.61395263671875, 122.09598541259766, 138.22731018066406, 111.4570541381836, 64.12130737304688, 143.8499755859375, 89.07020568847656, 55.098567962646484, 77.840576171875, 145.5541229248047, 156.31068420410156, 215.8498077392578, 90.02425384521484, 113.94557189941406, 291.5843200683594, 316.9300842285156, 121.05074310302734, 265.2338562011719, 403.8520812988281, 177.8711700439453, 126.9765853881836, 58.47915267944336, 132.97218322753906, 51.67923355102539, 69.33415985107422, 130.77024841308594, 92.92497253417969, 183.86337280273438, 111.53157043457031, 219.1325225830078, 399.35748291015625, 461.8337097167969, 1583.8868408203125, 199.2298583984375, 717.88037109375, 580.319580078125, 1117.33740234375, 838.6465454101562, 690.2192993164062, 649.2470703125, 3330.82421875, 604.588623046875, 232.0336151123047, 712.998291015625, 620.74267578125, 988.7415771484375, 543.5037841796875, 2603.279052734375, 446.6101379394531, 677.3656005859375, 2152.585205078125, 450.50115966796875, 1691.39306640625, 1267.686279296875, 803.4278564453125, 1849.5146484375, 2197.40283203125, 1691.8629150390625, 2493.9560546875, 1432.497802734375, 2879.57373046875, 944.5695190429688, 1207.8355712890625, 2417.86572265625, 2887.390625, 2608.8017578125, 1303.16064453125, 2769.487548828125, 3001.430908203125, 2102.27099609375, 2686.0712890625, 530.0112915039062, 258.3006896972656, 97.16219329833984, 814.3571166992188, 137.77980041503906, 312.4476318359375, 55.74363327026367, 91.2994613647461, 413.5163269042969, 138.0373077392578, 3301.542236328125, 73.6703109741211, 62.29830551147461, 668.066650390625, 103.86376953125, 167.73289489746094, 81.38504028320312, 261.0388488769531, 74.70716857910156, 705.829833984375, 420.24456787109375, 29.87380027770996, 157.6632080078125, 923.7269897460938, 105.4743423461914, 104.52293395996094, 77.47406005859375, 153.4698486328125, 85.83523559570312, 119.09291076660156, 458.4379577636719, 224.70811462402344, 431.63677978515625, 998.8310546875, 275.024658203125, 2392.41845703125, 757.6763916015625, 1901.2049560546875, 1648.201904296875, 358.5539855957031, 340.2247314453125, 205.88143920898438, 2603.279052734375, 1643.8826904296875, 2417.146484375, 1736.5438232421875, 2769.487548828125, 2083.716552734375, 797.9032592773438, 2758.376220703125, 1151.2640380859375, 2642.24365234375, 2879.57373046875, 2887.390625, 2540.23291015625, 2632.42529296875, 2379.894287109375, 2061.5712890625, 2083.736572265625, 2608.8017578125, 367.9353942871094, 61.70219802856445, 130.14913940429688, 112.03404235839844, 48.54608917236328, 182.01193237304688, 102.62430572509766, 115.54104614257812, 128.88296508789062, 172.1573486328125, 156.67868041992188, 81.72712707519531, 215.9336395263672, 831.8646240234375, 62.38972854614258, 99.08182525634766, 645.1781005859375, 153.14625549316406, 123.55596923828125, 275.1045837402344, 63.71945571899414, 156.9296875, 829.346923828125, 104.4319839477539, 143.36146545410156, 1458.8143310546875, 68.29473114013672, 85.15229034423828, 153.28958129882812, 257.7784423828125, 1769.8331298828125, 368.36785888671875, 329.4360656738281, 1168.5830078125, 235.22230529785156, 182.40130615234375, 432.830810546875, 339.6492919921875, 1186.7076416015625, 263.28717041015625, 346.0812683105469, 484.3301696777344, 327.08831787109375, 277.99652099609375, 1606.894287109375, 1581.8114013671875, 299.3216552734375, 556.147705078125, 550.019287109375, 912.7181396484375, 1146.618896484375, 1674.439697265625, 1849.5146484375, 882.853271484375, 1282.7606201171875, 803.9696044921875, 1027.374755859375, 926.981201171875, 1082.81591796875, 2769.487548828125, 743.6339721679688, 551.1906127929688, 1712.85888671875, 925.3973388671875, 2752.395263671875, 1691.39306640625, 1994.1075439453125, 2102.27099609375, 1417.14453125, 2152.585205078125, 835.7005615234375, 1592.7427978515625, 2686.0712890625], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2552000284194946, 1.2549999952316284, 1.2548999786376953, 1.2548999786376953, 1.2547999620437622, 1.254699945449829, 1.254699945449829, 1.254699945449829, 1.2545000314712524, 1.2542999982833862, 1.2541999816894531, 1.25409996509552, 1.25409996509552, 1.2538000345230103, 1.2537000179290771, 1.2533999681472778, 1.252500057220459, 1.2523000240325928, 1.2519999742507935, 1.2518999576568604, 1.250599980354309, 1.2486000061035156, 1.2479000091552734, 1.24590003490448, 1.2452000379562378, 1.243899941444397, 1.2426999807357788, 1.242300033569336, 1.2400000095367432, 1.239799976348877, 1.236199975013733, 1.2395999431610107, 1.2359999418258667, 1.2080999612808228, 1.2294000387191772, 1.1549999713897705, 1.2111999988555908, 1.187399983406067, 1.2271000146865845, 1.225000023841858, 1.1313999891281128, 1.1562000513076782, 1.202299952507019, 1.1208000183105469, 1.0527000427246094, 1.0635000467300415, 0.9833999872207642, 1.0374000072479248, 1.1298999786376953, 1.2023999691009521, 0.9803000092506409, 0.9837999939918518, 0.8822000026702881, 1.0297000408172607, 1.0160000324249268, 0.7342000007629395, 0.7797999978065491, 0.6707000136375427, 1.0240999460220337, 0.5095999836921692, 0.5299000144004822, 0.96670001745224, 0.5407000184059143, 0.4368000030517578, 0.628000020980835, 0.47279998660087585, 0.49880000948905945, 0.39500001072883606, 0.4876999855041504, 0.519599974155426, 0.32260000705718994, 0.21660000085830688, 0.18029999732971191, 0.10180000215768814, 0.03420000150799751, -0.05900000035762787, -0.10459999740123749, 1.7604000568389893, 1.7575000524520874, 1.7436000108718872, 1.7411999702453613, 1.7351000308990479, 1.7347999811172485, 1.7344000339508057, 1.7325999736785889, 1.7303999662399292, 1.7237999439239502, 1.7143000364303589, 1.7064000368118286, 1.7057000398635864, 1.6986000537872314, 1.69760000705719, 1.6950000524520874, 1.6943000555038452, 1.6908999681472778, 1.6852999925613403, 1.680799961090088, 1.6805000305175781, 1.6792000532150269, 1.6766999959945679, 1.6746000051498413, 1.674399971961975, 1.6715999841690063, 1.667099952697754, 1.652999997138977, 1.651900053024292, 1.635200023651123, 1.6340999603271484, 1.5852999687194824, 1.5443999767303467, 1.579699993133545, 1.4916000366210938, 1.5572999715805054, 1.4154000282287598, 1.5915000438690186, 1.5683000087738037, 1.4377000331878662, 1.4566999673843384, 1.4970999956130981, 1.552299976348877, 1.2410000562667847, 1.351199984550476, 1.399999976158142, 1.3823000192642212, 1.3075000047683716, 1.1109000444412231, 1.1722999811172485, 1.3213000297546387, 1.500599980354309, 1.4544999599456787, 1.035099983215332, 1.1553000211715698, 0.9398999810218811, 1.1512000560760498, 0.9362000226974487, 0.7645999789237976, 0.7597000002861023, 0.6705999970436096, 0.7372999787330627, 0.8671000003814697, 0.7853999733924866, 0.8227999806404114, 0.37119999527931213, 0.3343000113964081, 0.5338000059127808, 0.5690000057220459, 0.5444999933242798, 0.43950000405311584, 0.3050000071525574, 0.49630001187324524, 0.4397999942302704, -0.010300000198185444, 1.8353999853134155, 1.8351000547409058, 1.8350000381469727, 1.8281999826431274, 1.8240000009536743, 1.8118000030517578, 1.8106000423431396, 1.8085999488830566, 1.80840003490448, 1.7865999937057495, 1.7827999591827393, 1.7767000198364258, 1.766700029373169, 1.7646000385284424, 1.75, 1.7488000392913818, 1.7373000383377075, 1.736799955368042, 1.7223000526428223, 1.7202999591827393, 1.7196999788284302, 1.7174999713897705, 1.7019000053405762, 1.6964000463485718, 1.6907999515533447, 1.6796000003814697, 1.6747000217437744, 1.6727999448776245, 1.670300006866455, 1.669700026512146, 1.6669000387191772, 1.6632000207901, 1.6333999633789062, 1.6510000228881836, 1.6288000345230103, 1.4529000520706177, 1.3980000019073486, 1.348099946975708, 1.5400999784469604, 1.4521000385284424, 1.5514999628067017, 1.4967999458312988, 1.3055000305175781, 1.5994999408721924, 1.2120000123977661, 1.1632000207901, 1.3092999458312988, 1.2335000038146973, 1.1801999807357788, 0.9876000285148621, 1.1114000082015991, 1.0397000312805176, 0.9958999752998352, 1.0262999534606934, 0.890500009059906, 1.0227999687194824, 1.2676000595092773, 1.2759000062942505, 1.2204999923706055, 0.8583999872207642, 0.9449999928474426, 0.6151000261306763, 0.713699996471405, 0.970300018787384, 0.98089998960495, 0.3562999963760376, 0.9124000072479248, 0.632099986076355, 0.5271000266075134, 1.955399990081787, 1.955399990081787, 1.955399990081787, 1.9550000429153442, 1.9549000263214111, 1.9536000490188599, 1.9530999660491943, 1.951300024986267, 1.944000005722046, 1.9431999921798706, 1.9417999982833862, 1.9397000074386597, 1.9363000392913818, 1.9327000379562378, 1.9270999431610107, 1.9265999794006348, 1.910099983215332, 1.9071999788284302, 1.9061000347137451, 1.9012999534606934, 1.8934999704360962, 1.8896000385284424, 1.8688000440597534, 1.8686000108718872, 1.8651000261306763, 1.853700041770935, 1.8271000385284424, 1.826799988746643, 1.8213000297546387, 1.8158999681472778, 1.8141000270843506, 1.8014999628067017, 1.746000051498413, 1.8137999773025513, 1.7346999645233154, 1.7436000108718872, 1.6865999698638916, 1.6287000179290771, 1.6526999473571777, 1.6334999799728394, 1.3523999452590942, 1.5614999532699585, 1.7556999921798706, 1.495300054550171, 1.5169999599456787, 1.3588999509811401, 1.5023000240325928, 0.9769999980926514, 1.5270999670028687, 1.3812999725341797, 0.9350000023841858, 1.507200002670288, 0.9757000207901001, 1.090999960899353, 1.2661000490188599, 0.9061999917030334, 0.7900999784469604, 0.8733999729156494, 0.6276999711990356, 0.925599992275238, 0.5376999974250793, 1.1481000185012817, 0.9480000138282776, 0.43639999628067017, 0.2694999873638153, 0.33869999647140503, 0.8758000135421753, 0.1907999962568283, 0.0934000015258789, 0.367900013923645, 0.11010000109672546, 2.382999897003174, 2.381200075149536, 2.366300106048584, 2.3540000915527344, 2.3301000595092773, 2.3173999786376953, 2.3036000728607178, 2.3027000427246094, 2.292799949645996, 2.285099983215332, 2.276900053024292, 2.2511000633239746, 2.2397000789642334, 2.2381999492645264, 2.2360000610351562, 2.215100049972534, 2.2100000381469727, 2.191699981689453, 2.1830999851226807, 2.172600030899048, 2.1568000316619873, 2.1512999534606934, 2.13919997215271, 2.1257998943328857, 2.109299898147583, 2.1001999378204346, 2.0524001121520996, 2.0411999225616455, 2.0371999740600586, 2.0353000164031982, 2.0292999744415283, 2.0315001010894775, 2.0237998962402344, 1.9464999437332153, 1.9596999883651733, 1.4361000061035156, 1.6308000087738037, 1.3961999416351318, 1.4112000465393066, 1.798699975013733, 1.8085999488830566, 1.9459999799728394, 1.0896999835968018, 1.1450999975204468, 0.9409000277519226, 1.076300024986267, 0.8234000205993652, 0.8841000199317932, 1.3243999481201172, 0.6151000261306763, 1.0923000574111938, 0.5960000157356262, 0.5060999989509583, 0.477400004863739, 0.4991999864578247, 0.4503999948501587, 0.5241000056266785, 0.5695000290870667, 0.5404999852180481, 0.28679999709129333, 1.8969999551773071, 1.8961999416351318, 1.895300030708313, 1.8949999809265137, 1.8945000171661377, 1.8877999782562256, 1.8876999616622925, 1.8868999481201172, 1.8849999904632568, 1.8844000101089478, 1.8837000131607056, 1.882099986076355, 1.8802000284194946, 1.8768999576568604, 1.8761999607086182, 1.8753000497817993, 1.874500036239624, 1.868899941444397, 1.8658000230789185, 1.8581000566482544, 1.8574999570846558, 1.8558000326156616, 1.8557000160217285, 1.8538999557495117, 1.8464000225067139, 1.842900037765503, 1.8375999927520752, 1.8339999914169312, 1.8325999975204468, 1.829200029373169, 1.826300024986267, 1.8207999467849731, 1.8203999996185303, 1.7939000129699707, 1.8228000402450562, 1.8269000053405762, 1.7832000255584717, 1.7834999561309814, 1.6928000450134277, 1.7936999797821045, 1.7626999616622925, 1.7282999753952026, 1.7669999599456787, 1.7798000574111938, 1.565999984741211, 1.5638999938964844, 1.735700011253357, 1.6141999959945679, 1.6088000535964966, 1.4767999649047852, 1.4133000373840332, 1.2022000551223755, 1.1424000263214111, 1.361299991607666, 1.2273999452590942, 1.3894000053405762, 1.2944999933242798, 1.3286000490188599, 1.2635999917984009, 0.7299000024795532, 1.3654999732971191, 1.4980000257492065, 0.8185999989509583, 1.179900050163269, 0.4796999990940094, 0.7871000170707703, 0.675000011920929, 0.550000011920929, 0.8536999821662903, 0.44429999589920044, 1.2354999780654907, 0.6287000179290771, 0.11389999836683273], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.958199977874756, -7.616199970245361, -7.315899848937988, -7.3628997802734375, -7.924300193786621, -8.104599952697754, -7.046299934387207, -8.180500030517578, -8.342599868774414, -8.311699867248535, -8.61139965057373, -8.712499618530273, -7.969699859619141, -7.760000228881836, -7.8470001220703125, -8.851300239562988, -7.374000072479248, -8.145600318908691, -7.73859977722168, -8.137700080871582, -6.652699947357178, -7.488900184631348, -7.315000057220459, -8.203900337219238, -7.632599830627441, -7.286499977111816, -8.225799560546875, -8.682499885559082, -8.640299797058105, -5.626800060272217, -7.358799934387207, -7.703100204467773, -7.49560022354126, -5.199100017547607, -7.36870002746582, -5.028800010681152, -6.9475998878479, -6.415299892425537, -7.464799880981445, -7.420899868011475, -5.380300045013428, -5.878799915313721, -7.0071001052856445, -5.8933000564575195, -5.028900146484375, -5.255499839782715, -4.669400215148926, -5.309899806976318, -6.418499946594238, -7.190999984741211, -5.2895002365112305, -5.684500217437744, -5.084099769592285, -6.125800132751465, -6.299499988555908, -5.111299991607666, -5.358399868011475, -5.1890997886657715, -6.691299915313721, -5.305099964141846, -5.590099811553955, -6.618599891662598, -5.7153000831604, -5.521699905395508, -6.002099990844727, -5.807799816131592, -5.866499900817871, -5.741300106048584, -5.887499809265137, -5.977700233459473, -5.806600093841553, -5.750899791717529, -5.733099937438965, -5.782400131225586, -5.7778000831604, -5.809700012207031, -5.85230016708374, -7.195700168609619, -7.351900100708008, -7.972099781036377, -6.55210018157959, -7.329500198364258, -7.375500202178955, -6.930200099945068, -8.047800064086914, -6.742800235748291, -7.286600112915039, -7.27839994430542, -6.857500076293945, -7.539100170135498, -7.238900184631348, -6.806600093841553, -5.333000183105469, -7.712200164794922, -8.220800399780273, -7.863399982452393, -7.073299884796143, -7.46999979019165, -7.025899887084961, -6.118199825286865, -7.52209997177124, -8.134300231933594, -7.736599922180176, -7.7459001541137695, -7.9756999015808105, -6.9903998374938965, -6.912199974060059, -6.919099807739258, -5.875199794769287, -5.613800048828125, -6.320400238037109, -5.081299781799316, -5.992700099945068, -4.521699905395508, -6.597400188446045, -6.448400020599365, -5.33650016784668, -5.5229997634887695, -5.9847002029418945, -6.401700019836426, -4.615099906921387, -5.624199867248535, -5.9222002029418945, -5.863900184631348, -5.578800201416016, -4.842599868774414, -5.114699840545654, -5.806099891662598, -6.447199821472168, -6.442399978637695, -5.428800106048584, -5.717599868774414, -5.259200096130371, -5.767000198364258, -5.452700138092041, -5.229599952697754, -5.240300178527832, -5.10260009765625, -5.252399921417236, -5.576600074768066, -5.615900039672852, -5.654600143432617, -5.425300121307373, -5.416399955749512, -5.612800121307373, -5.6265997886657715, -5.626200199127197, -5.644000053405762, -5.6371002197265625, -5.673299789428711, -5.683000087738037, -5.6793999671936035, -6.103600025177002, -6.8867998123168945, -7.287499904632568, -6.510000228881836, -7.261600017547607, -7.880000114440918, -8.095600128173828, -7.624800205230713, -6.1539998054504395, -6.741199970245361, -7.493899822235107, -7.173099994659424, -5.900599956512451, -7.619900226593018, -7.027100086212158, -7.9664998054504395, -7.433499813079834, -7.5040998458862305, -6.814000129699707, -7.86269998550415, -5.785900115966797, -7.727499961853027, -7.0243000984191895, -6.212500095367432, -7.726399898529053, -6.248199939727783, -7.531400203704834, -5.4471001625061035, -7.359099864959717, -6.131999969482422, -6.127200126647949, -6.1149001121521, -4.450099945068359, -6.511600017547607, -6.541500091552734, -4.609099864959717, -4.676199913024902, -4.5096001625061035, -5.917799949645996, -5.317800045013428, -6.073599815368652, -5.728899955749512, -4.699699878692627, -6.43720006942749, -4.692399978637695, -4.776800155639648, -5.553699970245361, -5.3343000411987305, -5.201099872589111, -4.760200023651123, -5.188899993896484, -5.105199813842773, -5.142199993133545, -5.2093000411987305, -5.0100998878479, -5.386199951171875, -5.706900119781494, -5.72130012512207, -5.669099807739258, -5.349100112915039, -5.44379997253418, -5.18149995803833, -5.2804999351501465, -5.5304999351501465, -5.594299793243408, -5.394400119781494, -5.604599952697754, -5.537499904632568, -5.595799922943115, -6.446599960327148, -7.019899845123291, -6.8958001136779785, -7.111499786376953, -7.664400100708008, -6.857699871063232, -7.337600231170654, -7.819699764251709, -7.481400012969971, -6.856299877166748, -6.786399841308594, -6.465700149536133, -7.343699932098389, -7.111700057983398, -6.177700042724609, -6.094699859619141, -7.073800086975098, -6.292300224304199, -5.872900009155273, -6.697700023651123, -7.042600154876709, -7.821800231933594, -7.021200180053711, -7.966400146484375, -7.676000118255615, -7.0528998374938965, -7.421199798583984, -6.738999843597412, -7.244500160217285, -6.57450008392334, -5.976099967956543, -5.843400001525879, -4.666500091552734, -6.671899795532227, -5.469099998474121, -5.672900199890137, -5.074699878692627, -5.419600009918213, -5.590400218963623, -5.6707000732421875, -4.316699981689453, -5.814000129699707, -6.577400207519531, -5.7153000831604, -5.832099914550781, -5.524700164794922, -5.979700088500977, -4.938499927520752, -6.151299953460693, -5.8805999755859375, -5.170599937438965, -6.162499904632568, -5.371099948883057, -5.544099807739258, -5.825099945068359, -5.351200103759766, -5.294899940490723, -5.473100185394287, -5.330699920654297, -5.587200164794922, -5.2769999504089355, -5.781199932098389, -5.735499858856201, -5.552999973297119, -5.542500019073486, -5.574699878692627, -5.731800079345703, -5.662799835205078, -5.679900169372559, -5.76140022277832, -5.774099826812744, -5.124199867248535, -5.844699859619141, -6.837399959564209, -4.723599910736084, -6.524400234222412, -5.718299865722656, -7.455699920654297, -6.963200092315674, -5.462500095367432, -6.567399978637695, -3.4010000228881836, -7.229400157928467, -7.408400058746338, -5.037499904632568, -6.901000022888184, -6.442599773406982, -7.170899868011475, -6.02370023727417, -7.283400058746338, -5.048099994659424, -5.582499980926514, -8.231800079345703, -6.580399990081787, -4.825799942016602, -7.01230001449585, -7.0304999351501465, -7.377699851989746, -6.7052998542785645, -7.290500164031982, -6.964900016784668, -5.623000144958496, -6.333799839019775, -5.688700199127197, -4.9268999099731445, -6.203499794006348, -4.563899993896484, -5.519000053405762, -4.833600044250488, -4.961400032043457, -6.099299907684326, -6.141900062561035, -6.506700038909912, -4.825799942016602, -5.230199813842773, -5.048900127410889, -5.244200229644775, -5.030200004577637, -5.2540998458862305, -5.77370023727417, -5.242599964141846, -5.639100074768066, -5.304699897766113, -5.308599948883057, -5.33459997177124, -5.440899848937988, -5.453999996185303, -5.481200218200684, -5.5792999267578125, -5.597599983215332, -5.6265997886657715, -5.975200176239014, -7.761600017547607, -7.01609992980957, -7.166299819946289, -8.003000259399414, -6.6880998611450195, -7.261300086975098, -7.143599987030029, -7.036099910736084, -6.747300148010254, -6.842100143432617, -7.49459981918335, -6.524899959564209, -5.179500102996826, -7.770500183105469, -7.308800220489502, -5.436100006103516, -6.879799842834473, -7.097599983215332, -6.304800033569336, -7.76800012588501, -6.868500232696533, -5.203700065612793, -7.277599811553955, -6.968299865722656, -4.651800155639648, -7.718599796295166, -7.5015997886657715, -6.915200233459473, -6.39870023727417, -4.475200176239014, -6.05019998550415, -6.162300109863281, -4.922699928283691, -6.496699810028076, -6.747000217437744, -5.926499843597412, -6.168600082397461, -5.008399963378906, -6.413099765777588, -6.1707000732421875, -5.86899995803833, -6.222799777984619, -6.372600078582764, -4.831999778747559, -4.849800109863281, -6.342899799346924, -5.844799995422363, -5.861299991607666, -5.486800193786621, -5.322199821472168, -5.154600143432617, -5.114999771118164, -5.6356000900268555, -5.395899772644043, -5.701099872589111, -5.550899982452393, -5.619500160217285, -5.529200077056885, -5.123799800872803, -5.802999973297119, -5.96999979019165, -5.515500068664551, -5.769999980926514, -5.380099773406982, -5.559599876403809, -5.5071001052856445, -5.5792999267578125, -5.669899940490723, -5.661300182342529, -5.816299915313721, -5.77810001373291, -5.770299911499023]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 1, 3, 4, 1, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 1, 2, 3, 5, 6, 1, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 1, 1, 1, 5, 2, 1, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 6, 1, 2, 3, 5, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 1, 3, 4, 5, 6, 1, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 4, 1, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 1, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 1, 4, 1, 1, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 4, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 4, 1, 2, 3, 5, 6, 2, 4, 1, 2, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 6, 3, 1, 1, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 1, 2, 3, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 1, 1, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 5, 6, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 1, 4, 5, 1, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 1, 2, 3, 5, 1, 3, 4, 1, 2, 3, 4, 1, 2, 4, 1, 2, 3, 6, 1, 1, 2, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 5, 6, 1, 6, 1, 2, 3, 4, 5, 6, 3, 1, 3, 2, 3, 5, 1, 2, 3, 4, 5, 6, 2, 3, 5, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 3, 5, 6, 1, 3, 4, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 5, 2, 3, 1, 2, 5, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 6, 3, 4, 5, 6, 1, 2, 4, 6, 1, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 6, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 5, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 4, 1, 3, 4, 5, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 5, 1, 2, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 4, 5, 6, 1, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 6, 4, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 6, 1, 4, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 5, 1, 2, 4, 5, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 4, 5, 6, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 2, 5, 1, 2, 4, 1, 2, 4, 5, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 4, 5, 6, 6, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 6, 1, 2, 3, 4, 5, 6, 1, 1, 4, 6, 1, 4, 1, 2, 3, 4, 5, 6, 1], \"Freq\": [0.5937963128089905, 0.05980939790606499, 0.07234430313110352, 0.07341872155666351, 0.0877443253993988, 0.1131722703576088, 0.04428723081946373, 0.9521754384040833, 0.011071807704865932, 0.016028279438614845, 0.9777250289916992, 0.009646143764257431, 0.006028840318322182, 0.0048230718821287155, 0.010851912200450897, 0.00844037625938654, 0.9585855603218079, 0.9973348379135132, 0.07015863806009293, 0.016704438254237175, 0.02672710083425045, 0.036749761551618576, 0.0033408876042813063, 0.851926326751709, 0.01492234691977501, 0.843112587928772, 0.08953408151865005, 0.05222821235656738, 0.9931760430335999, 0.9060755372047424, 0.002353442832827568, 0.042361970990896225, 0.017650822177529335, 0.03177148103713989, 0.9952988624572754, 0.0028036588337272406, 0.01161727961152792, 0.9874687194824219, 0.0061693936586380005, 0.00274195265956223, 0.0006854881648905575, 0.04181477800011635, 0.0020564645528793335, 0.9466592073440552, 0.1004224419593811, 0.1004224419593811, 0.8033795356750488, 0.15908676385879517, 0.545297384262085, 0.11306166648864746, 0.04302346706390381, 0.11406221240758896, 0.025013642385601997, 0.02519041672348976, 0.008396805264055729, 0.26030096411705017, 0.7053316831588745, 0.10825864970684052, 0.3674106299877167, 0.09488306939601898, 0.030513057485222816, 0.3878920078277588, 0.010867663659155369, 0.825169563293457, 0.015525838360190392, 0.0005750310374423862, 0.10753080993890762, 0.0011500620748847723, 0.05002770200371742, 0.008502595126628876, 0.008502595126628876, 0.05526686832308769, 0.9267828464508057, 0.15453055500984192, 0.18350504338741302, 0.42570197582244873, 0.03714676946401596, 0.13447129726409912, 0.06463538110256195, 0.9843364357948303, 0.003977117128670216, 0.001988558564335108, 0.008948513306677341, 0.9967684149742126, 0.9963850378990173, 0.9831231832504272, 0.015856826677918434, 0.9963371753692627, 0.011743665672838688, 0.023487331345677376, 0.03523099422454834, 0.9394932389259338, 0.007244418375194073, 0.07244418561458588, 0.007244418375194073, 0.9055522680282593, 0.007244418375194073, 0.1310518980026245, 0.09707548469305038, 0.10860320180654526, 0.14804011583328247, 0.37859439849853516, 0.1371191293001175, 0.036360375583171844, 0.09453697502613068, 0.010908112861216068, 0.15634961426258087, 0.6544867753982544, 0.04726848751306534, 0.13642451167106628, 0.1325266808271408, 0.10134392976760864, 0.10134392976760864, 0.016370942816138268, 0.5113970637321472, 0.017552239820361137, 0.9741492867469788, 0.7922590970993042, 0.06040685251355171, 0.0023233406245708466, 0.13243040442466736, 0.011616703122854233, 0.03148376941680908, 0.8972874879837036, 0.06296753883361816, 0.01574188470840454, 0.13712278008460999, 0.0059618595987558365, 0.0059618595987558365, 0.8465840816497803, 0.0059618595987558365, 0.08913350850343704, 0.1530575454235077, 0.6455426812171936, 0.009903723374009132, 0.08688266575336456, 0.015305754728615284, 0.005778989288955927, 0.026005452498793602, 0.005778989288955927, 0.08379534631967545, 0.005778989288955927, 0.8726274371147156, 0.08919274806976318, 0.8664438128471375, 0.012741820886731148, 0.012741820886731148, 0.012741820886731148, 0.08158231526613235, 0.021755283698439598, 0.8810890316963196, 0.0054388209246098995, 0.010877641849219799, 0.038233682513237, 0.9622143507003784, 0.12944534420967102, 0.06098867580294609, 0.04854200780391693, 0.5016007423400879, 0.04356333985924721, 0.21408268809318542, 0.24247686564922333, 0.2492975890636444, 0.2949964702129364, 0.04092436656355858, 0.05968136712908745, 0.11254200339317322, 0.04983692616224289, 0.0014239121228456497, 0.713379979133606, 0.018510857596993446, 0.0028478242456912994, 0.21501073241233826, 0.034000150859355927, 0.11657194793224335, 0.004857164341956377, 0.16514359414577484, 0.6460028886795044, 0.034000150859355927, 0.2189137488603592, 0.0858241394162178, 0.003731484292075038, 0.058459922671318054, 0.031095702201128006, 0.6020128130912781, 0.9981091618537903, 0.9820430278778076, 0.01925574615597725, 0.012235838919878006, 0.9788671731948853, 0.9041287899017334, 0.006529819220304489, 0.020091751590371132, 0.033653683960437775, 0.022603219375014305, 0.013059638440608978, 0.03668734058737755, 0.009171835146844387, 0.07643195986747742, 0.8774388432502747, 0.02092612534761429, 0.0069753751158714294, 0.02092612534761429, 0.9486510157585144, 0.07998323440551758, 0.14712245762348175, 0.18974125385284424, 0.11793149262666702, 0.12552113831043243, 0.33978280425071716, 0.9976269602775574, 0.10089505463838577, 0.051274534314870834, 0.09924103319644928, 0.6748390197753906, 0.026464276015758514, 0.04631248116493225, 1.0042310953140259, 0.16140541434288025, 0.0514368899166584, 0.05025443434715271, 0.375430166721344, 0.03192634508013725, 0.32931435108184814, 0.2304196059703827, 0.3914438486099243, 0.145528182387352, 0.037055786699056625, 0.13676953315734863, 0.05928925797343254, 0.11261504888534546, 0.19768837094306946, 0.5190085172653198, 0.040394529700279236, 0.038558412343263626, 0.09119370579719543, 0.1386202573776245, 0.2013786882162094, 0.42068833112716675, 0.05862050876021385, 0.05034467205405235, 0.12965476512908936, 0.16750217974185944, 0.4838951826095581, 0.14357329905033112, 0.11233281344175339, 0.053175296634435654, 0.0392167791724205, 0.08508539199829102, 0.0044781784527003765, 0.0044781784527003765, 0.651574969291687, 0.006717267911881208, 0.2507780194282532, 0.21332749724388123, 0.08671225607395172, 0.07520177960395813, 0.3399427533149719, 0.08057333528995514, 0.20411911606788635, 0.03150187060236931, 0.023626403883099556, 0.9371806383132935, 0.007875467650592327, 0.15607453882694244, 0.049869973212480545, 0.15607453882694244, 0.06556978076696396, 0.04340534657239914, 0.5300993323326111, 0.04892381653189659, 0.7642492651939392, 0.1312047690153122, 0.022979367524385452, 0.027426987886428833, 0.0051888893358409405, 0.02106318809092045, 0.6640447378158569, 0.2937760353088379, 0.0033257666509598494, 0.01773742213845253, 0.0557151697576046, 0.7354402542114258, 0.156002476811409, 0.0222860686480999, 0.025071825832128525, 0.0027857585810124874, 0.7866885662078857, 0.21186068654060364, 0.011583813466131687, 0.12047166377305984, 0.16449014842510223, 0.004633525386452675, 0.6973455548286438, 0.016918106004595757, 0.0056393686681985855, 0.8712824583053589, 0.022557474672794342, 0.02537715993821621, 0.06203305721282959, 0.9949106574058533, 0.010988290421664715, 0.9889461398124695, 0.01177685335278511, 0.08538218587636948, 0.008832640014588833, 0.0029442133381962776, 0.0029442133381962776, 0.8920966386795044, 0.34136390686035156, 0.07556910812854767, 0.09989020228385925, 0.05819690227508545, 0.27534952759742737, 0.149400994181633, 0.39338409900665283, 0.029016239568591118, 0.04661625623703003, 0.20454071462154388, 0.06707032769918442, 0.2597191333770752, 0.47437575459480286, 0.05417468398809433, 0.01979459635913372, 0.2420497089624405, 0.15314766764640808, 0.05625832825899124, 0.3553904592990875, 0.034701425582170486, 0.025128617882728577, 0.06461644917726517, 0.004786403384059668, 0.5157349705696106, 0.9915415644645691, 0.9746906161308289, 0.018506783992052078, 0.006168927997350693, 0.7592774629592896, 0.04488879069685936, 0.01368560642004013, 0.12481273710727692, 0.0010948484996333718, 0.055837277323007584, 0.8164082169532776, 0.0018142405897378922, 0.0009071202948689461, 0.08436218649148941, 0.0063498420640826225, 0.08980491012334824, 0.954008936882019, 0.04333549365401268, 0.003140253247693181, 0.020584138110280037, 0.9880385994911194, 0.34115278720855713, 0.12611153721809387, 0.09007967263460159, 0.19855858385562897, 0.12304499745368958, 0.12112841010093689, 0.09723788499832153, 0.8391269445419312, 0.04141613468527794, 0.01620631478726864, 0.005402104463428259, 0.0018007016042247415, 0.011898974888026714, 0.9727411866188049, 0.014873717911541462, 0.45706436038017273, 0.09185554832220078, 0.20473827421665192, 0.06086813658475876, 0.04592777416110039, 0.13944336771965027, 0.27411124110221863, 0.01812305673956871, 0.052103787660598755, 0.05889993533492088, 0.011326910927891731, 0.5844686031341553, 0.03544836863875389, 0.8153125047683716, 0.06695803254842758, 0.07286608964204788, 0.007877415046095848, 0.001969353761523962, 0.03125700354576111, 0.9189558625221252, 0.025005601346492767, 0.012502800673246384, 0.006251400336623192, 0.0027146777138113976, 0.0027146777138113976, 0.04886419698596001, 0.01085871085524559, 0.008144033141434193, 0.925705075263977, 0.254414826631546, 0.5433797836303711, 0.07224124670028687, 0.052348729223012924, 0.04292595759034157, 0.03455016016960144, 0.30384212732315063, 0.08998808264732361, 0.11857253313064575, 0.44570568203926086, 0.02011498250067234, 0.02223234996199608, 0.4891852140426636, 0.13876445591449738, 0.026996977627277374, 0.13336506485939026, 0.019977763295173645, 0.19167853891849518, 0.9474854469299316, 0.0457722432911396, 0.004577224608510733, 0.022216234356164932, 0.9775143265724182, 0.9975219368934631, 0.05440911650657654, 0.009601608850061893, 0.937757134437561, 0.038929130882024765, 0.08542782068252563, 0.011354329995810986, 0.3498215079307556, 0.04433595761656761, 0.4698529839515686, 0.01596754789352417, 0.01596754789352417, 0.942085325717926, 0.007983773946762085, 0.007983773946762085, 0.007983773946762085, 0.16409575939178467, 0.11781234294176102, 0.0028050558175891638, 0.6311375498771667, 0.07152892649173737, 0.014025279320776463, 0.8735162019729614, 0.013828752562403679, 0.011523961089551449, 0.0034571881406009197, 0.03687667474150658, 0.05877219885587692, 0.0031552701257169247, 0.022086890414357185, 0.0031552701257169247, 0.9718231558799744, 0.04364259913563728, 0.009413110092282295, 0.0008557372493669391, 0.032518014311790466, 0.011980321258306503, 0.9010913372039795, 0.9988359212875366, 0.02023535966873169, 0.7689436674118042, 0.07174354791641235, 0.06070607900619507, 0.025754094123840332, 0.053347766399383545, 0.21349133551120758, 0.04748472943902016, 0.5360076427459717, 0.032289616763591766, 0.1447334587574005, 0.025831691920757294, 0.11845993250608444, 0.11126907169818878, 0.38868480920791626, 0.11353986710309982, 0.1672820746898651, 0.10067202150821686, 0.06779351085424423, 0.060542866587638855, 0.6141294240951538, 0.041328661143779755, 0.1707526296377182, 0.04531651735305786, 0.03816566616296768, 0.00545223755761981, 0.8169269561767578, 0.009087063372135162, 0.1281275898218155, 0.002726118778809905, 0.06831024587154388, 0.9270676374435425, 0.01810886710882187, 0.006964948959648609, 0.800969123840332, 0.02646680548787117, 0.14626392722129822, 0.009145407006144524, 0.009145407006144524, 0.9877039790153503, 0.4839886426925659, 0.07822990417480469, 0.20688511431217194, 0.06126438453793526, 0.0673908218741417, 0.10179313272237778, 0.06866338104009628, 0.01716584526002407, 0.9097898006439209, 0.07740052789449692, 0.909456193447113, 0.2947297692298889, 0.15758173167705536, 0.1343773901462555, 0.18528839945793152, 0.14857704937458038, 0.07931036502122879, 0.035878535360097885, 0.017939267680048943, 0.017939267680048943, 0.9149026870727539, 0.4229575991630554, 0.10059014707803726, 0.05269007384777069, 0.16717123985290527, 0.09771613776683807, 0.1585492193698883, 0.9557179808616638, 0.007240287493914366, 0.003620143746957183, 0.003620143746957183, 0.028961149975657463, 0.9981081485748291, 0.028735511004924774, 0.02298840880393982, 0.8563182353973389, 0.0890800878405571, 0.0028735511004924774, 0.05130033195018768, 0.940506100654602, 0.055212609469890594, 0.020077312365174294, 0.055212609469890594, 0.868343710899353, 0.005019328091293573, 0.06712372601032257, 0.010826407931745052, 0.008661125786602497, 0.8574514985084534, 0.05629732087254524, 0.023660020902752876, 0.9385141134262085, 0.023660020902752876, 0.0157733466476202, 0.9964134097099304, 1.0001881122589111, 0.16544874012470245, 0.049089185893535614, 0.014544944278895855, 0.021817415952682495, 0.7490646243095398, 0.006342633627355099, 0.025370534509420395, 0.18393638730049133, 0.7864866256713867, 0.006342633627355099, 0.010316346772015095, 0.0515817329287529, 0.8562567830085754, 0.06189807876944542, 0.02063269354403019, 0.04204915463924408, 0.0400468111038208, 0.02402808703482151, 0.2252633273601532, 0.6467559933662415, 0.02302691712975502, 0.17679966986179352, 0.2822851836681366, 0.300113707780838, 0.08518078923225403, 0.09805695712566376, 0.05744750797748566, 0.2943861782550812, 0.005519740749150515, 0.01103948149830103, 0.6347702145576477, 0.012879394926130772, 0.04047809913754463, 0.23307375609874725, 0.015538251027464867, 0.008879000321030617, 0.6392880082130432, 0.04217525199055672, 0.05993325263261795, 0.24325700104236603, 0.04349628463387489, 0.004832920618355274, 0.6443893909454346, 0.01127681415528059, 0.05316212400794029, 0.7487310171127319, 0.002040139166638255, 0.24889697134494781, 0.08664349466562271, 0.8328955173492432, 0.025154562667012215, 0.005589902866631746, 0.025154562667012215, 0.022359611466526985, 0.03210360184311867, 0.016051800921559334, 0.016051800921559334, 0.06420720368623734, 0.8667972683906555, 0.016051800921559334, 0.02844293601810932, 0.02844293601810932, 0.018961956724524498, 0.1611766368150711, 0.7584782838821411, 0.009480978362262249, 0.2868007719516754, 0.23314891755580902, 0.1826530396938324, 0.11085422337055206, 0.057202357798814774, 0.1290011703968048, 0.9789223670959473, 0.017174076288938522, 0.010220758616924286, 0.9198682904243469, 0.07154531031847, 0.02537611313164234, 0.9135400652885437, 0.06344028562307358, 0.18154363334178925, 0.07240132987499237, 0.14588327705860138, 0.09617490321397781, 0.015128636732697487, 0.4873582124710083, 0.055845439434051514, 0.055845439434051514, 0.005878467578440905, 0.2615917921066284, 0.5643328428268433, 0.05878467485308647, 0.9958484172821045, 0.14993435144424438, 0.7080669403076172, 0.06162223219871521, 0.056519754230976105, 0.021979905664920807, 0.0019624915439635515, 0.1291579008102417, 0.7382041811943054, 0.12693104147911072, 0.001113430131226778, 0.004453720524907112, 0.991470217704773, 0.9978778958320618, 0.012764978222548962, 0.9892858266830444, 0.012005776166915894, 0.972467839717865, 0.006002888083457947, 0.006002888083457947, 0.479032963514328, 0.04810948297381401, 0.09896807372570038, 0.11065180599689484, 0.05223315209150314, 0.21030716598033905, 0.16480377316474915, 0.1354551613330841, 0.4848165810108185, 0.03386379033327103, 0.1140080913901329, 0.06772758066654205, 0.24855686724185944, 0.0017442586831748486, 0.004360646940767765, 0.1264587640762329, 0.003488517366349697, 0.616595447063446, 0.22869639098644257, 0.11713717132806778, 0.013944901525974274, 0.08088042587041855, 0.557796061038971, 0.9882057309150696, 0.9670330286026001, 0.025448238477110863, 0.03168125078082085, 0.021120833232998848, 0.8923552632331848, 0.026401042938232422, 0.021120833232998848, 0.005280208308249712, 0.5573327541351318, 0.057404544204473495, 0.007629718165844679, 0.12716196477413177, 0.008356357924640179, 0.24233438074588776, 0.9923274517059326, 0.005422554444521666, 0.021802611649036407, 0.13808320462703705, 0.8321330547332764, 0.007267537526786327, 0.0036337687633931637, 0.18822415173053741, 0.06453399360179901, 0.07298487424850464, 0.3756800591945648, 0.2742694914340973, 0.02458437904715538, 0.9982110857963562, 0.10323534160852432, 0.8111348152160645, 0.06784036755561829, 0.014747905544936657, 0.688633143901825, 0.13942396640777588, 0.021014627069234848, 0.10224423557519913, 0.018589861690998077, 0.030309557914733887, 0.15562406182289124, 0.847286581993103, 0.037756968289613724, 0.022654181346297264, 0.07119885832071304, 0.27077141404151917, 0.03128434717655182, 0.566354513168335, 0.9584928750991821, 0.008334720507264137, 0.03333888202905655, 0.12389164417982101, 0.6439784169197083, 0.12002003192901611, 0.02452022023499012, 0.06839851289987564, 0.019358068704605103, 0.027259986847639084, 0.06390952318906784, 0.0075722187757492065, 0.0018173324642702937, 0.8992766737937927, 0.00030288874404504895, 0.21976789832115173, 0.2670298218727112, 0.27033814787864685, 0.07136549055576324, 0.07609167695045471, 0.09546906501054764, 0.4641450345516205, 0.010949686169624329, 0.047448642551898956, 0.17519497871398926, 0.2901666760444641, 0.012774634175002575, 0.998115599155426, 0.1413014531135559, 0.010339130647480488, 0.001723188441246748, 0.8081753849983215, 0.008615942671895027, 0.031017392873764038, 0.3540334701538086, 0.15172863006591797, 0.1484917402267456, 0.13675807416439056, 0.12583360075950623, 0.08334959298372269, 0.05689946189522743, 0.0643211305141449, 0.8411225080490112, 0.02968667633831501, 0.004947779234498739, 0.031002333387732506, 0.02504034712910652, 0.06200466677546501, 0.7214004397392273, 0.10373857617378235, 0.05842747539281845, 0.08031357824802399, 0.09369917213916779, 0.8165213465690613, 0.06016295775771141, 0.9174851179122925, 0.02256110869348049, 0.039618466049432755, 0.9508432149887085, 0.007428462617099285, 0.04724184423685074, 0.10481784492731094, 0.09005476534366608, 0.5624731779098511, 0.15648861229419708, 0.03690769150853157, 0.8834459781646729, 0.048809170722961426, 0.005578191019594669, 0.04602007567882538, 0.0013945477548986673, 0.014642750844359398, 0.29685235023498535, 0.06503807753324509, 0.02090509608387947, 0.36049675941467285, 0.023227883502840996, 0.23367251455783844, 0.3134385347366333, 0.055148426443338394, 0.03350790590047836, 0.35671958327293396, 0.028621334582567215, 0.21291480958461761, 0.01969732902944088, 0.9257744550704956, 0.03939465805888176, 0.013131552375853062, 0.006565776187926531, 0.15541979670524597, 0.3694337010383606, 0.3257997930049896, 0.03615380451083183, 0.07272317260503769, 0.0407249741256237, 0.009914740920066833, 0.9716445803642273, 0.009914740920066833, 0.004957370460033417, 0.07092177122831345, 0.02532920427620411, 0.8459954261779785, 0.053191330283880234, 0.006397515535354614, 0.006397515535354614, 0.9852173328399658, 0.005235604010522366, 0.03664923086762428, 0.9528799653053284, 0.005235604010522366, 0.051135435700416565, 0.9408920407295227, 0.005113543942570686, 0.019916988909244537, 0.05975096672773361, 0.9062229990959167, 0.009958494454622269, 0.9873695969581604, 0.06206880509853363, 0.003879300318658352, 0.9349113702774048, 0.9939165711402893, 0.4479973614215851, 0.04404468461871147, 0.44359290599823, 0.029572859406471252, 0.023280762135982513, 0.01132577657699585, 0.040622271597385406, 0.1724189668893814, 0.681551456451416, 0.011735322885215282, 0.09207715094089508, 0.0018054342363029718, 0.998355507850647, 0.042063381522893906, 0.9359102249145508, 0.0011684272903949022, 0.0058421362191438675, 0.0011684272903949022, 0.012852699495851994, 0.008654932491481304, 0.9866623282432556, 0.012814721092581749, 0.16018401086330414, 0.788105309009552, 0.025629442185163498, 0.0064073605462908745, 0.0032036802731454372, 0.02585832215845585, 0.07326524704694748, 0.0043097203597426414, 0.8188468813896179, 0.06895552575588226, 0.012929161079227924, 0.020185336470603943, 0.9789888262748718, 0.046563778072595596, 0.05647096410393715, 0.5904683470726013, 0.009907186962664127, 0.007925749756395817, 0.2873084247112274, 0.07216455042362213, 0.003798134159296751, 0.018990671262145042, 0.003798134159296751, 0.9001578092575073, 0.7977104187011719, 0.01327305193990469, 0.006636525969952345, 0.08760213851928711, 0.09556597471237183, 0.9867996573448181, 0.010497868992388248, 0.26110559701919556, 0.25873908400535583, 0.03234238550066948, 0.42123985290527344, 0.011832580901682377, 0.014987935312092304, 0.999568521976471, 0.10480853170156479, 0.8908724784851074, 0.012949254363775253, 0.9711940288543701, 0.012949254363775253, 0.09970442950725555, 0.05211822688579559, 0.5710344910621643, 0.08497536927461624, 0.1450246274471283, 0.04645320028066635, 0.012332096695899963, 0.9372393488883972, 0.049328386783599854, 0.9799661040306091, 0.00559980608522892, 0.01119961217045784, 0.00559980608522892, 0.1473710536956787, 0.07120174169540405, 0.10183505713939667, 0.3651159107685089, 0.0803089439868927, 0.23513133823871613, 0.007758977357298136, 0.007758977357298136, 0.9853901267051697, 0.07278610020875931, 0.003830847330391407, 0.0957711860537529, 0.8274630308151245, 0.01885128952562809, 0.030162062495946884, 0.9538751840591431, 0.23436759412288666, 0.0527895912528038, 0.10375885665416718, 0.31173163652420044, 0.09647753089666367, 0.20069146156311035, 0.13246113061904907, 0.001540245721116662, 0.012321965768933296, 0.7239154577255249, 0.12784038484096527, 0.761753261089325, 0.12730346620082855, 0.00502125034108758, 0.0912686139345169, 0.0005907353479415178, 0.014177648350596428, 0.004141141660511494, 0.981450617313385, 0.012423424981534481, 0.004141141660511494, 0.05114001780748367, 0.09217830002307892, 0.027148403227329254, 0.8106639981269836, 0.0037881494499742985, 0.015152597799897194, 0.16221016645431519, 0.10462075471878052, 0.4319205582141876, 0.009118322283029556, 0.22315895557403564, 0.069107286632061, 0.060153890401124954, 0.015038472600281239, 0.8872698545455933, 0.015038472600281239, 0.030076945200562477, 0.023955976590514183, 0.8051870465278625, 0.15039029717445374, 0.003992662765085697, 0.01597065106034279, 0.005862733349204063, 0.7709494233131409, 0.21105839312076569, 0.011725466698408127, 0.9748934507369995, 0.02812192775309086, 0.018848761916160583, 0.9235893487930298, 0.05654628574848175, 0.01260613277554512, 0.9328538179397583, 0.03781839832663536, 0.01260613277554512, 0.03654736280441284, 0.30677634477615356, 0.5670378804206848, 0.0033224874641746283, 0.08195469528436661, 0.0033224874641746283, 0.2630419135093689, 0.17991073429584503, 0.09098933637142181, 0.2187879979610443, 0.11166873574256897, 0.13565683364868164, 0.019153352826833725, 0.08938231319189072, 0.8746697902679443, 0.012768901884555817, 0.006384450942277908, 0.08436339348554611, 0.04492076858878136, 0.007669399492442608, 0.14571858942508698, 0.06025956571102142, 0.6562814712524414, 0.1113334521651268, 0.6977941393852234, 0.06429114937782288, 0.09408460557460785, 0.028225382789969444, 0.004704230464994907, 0.9881411194801331, 0.007429632358253002, 0.1633097231388092, 0.2130756974220276, 0.4512759745121002, 0.01980975829064846, 0.07199156284332275, 0.08117169141769409, 1.0010429620742798, 0.2294968217611313, 0.5947370529174805, 0.06805268675088882, 0.06841466575860977, 0.027510659769177437, 0.011945418082177639, 0.12498412281274796, 0.6845060586929321, 0.10027796030044556, 0.039239201694726944, 0.04359911382198334, 0.005813214927911758, 0.020778007805347443, 0.9765663743019104, 0.28989720344543457, 0.2667236030101776, 0.14903806149959564, 0.07951726019382477, 0.076336570084095, 0.13858722150325775, 0.02245284989476204, 0.0044905696995556355, 0.08532083034515381, 0.8651831150054932, 0.020955992862582207, 0.24108970165252686, 0.04561156406998634, 0.7102372050285339, 0.5337287187576294, 0.07691972702741623, 0.12636812031269073, 0.19857847690582275, 0.03610517457127571, 0.02825622446835041, 0.013435302302241325, 0.013435302302241325, 0.9203181862831116, 0.020152952522039413, 0.04030590504407883, 0.22111457586288452, 0.3034606873989105, 0.20383204519748688, 0.09810370206832886, 0.05794726684689522, 0.1153862252831459, 0.007749798242002726, 0.0015499596484005451, 0.0061998385936021805, 0.007749798242002726, 0.9764745831489563, 0.037960831075906754, 0.020440448075532913, 0.02628057636320591, 0.3114734888076782, 0.05645456910133362, 0.5470253229141235, 0.016656966879963875, 0.12849660217761993, 0.026175234466791153, 0.002379566663876176, 0.7971548438072205, 0.026175234466791153, 0.02825888991355896, 0.05274992808699608, 0.8910970091819763, 0.011303556151688099, 0.011303556151688099, 0.0075357044115662575, 0.03237399086356163, 0.9712197780609131, 0.015693793073296547, 0.015693793073296547, 0.015693793073296547, 0.9573214054107666, 0.003634981345385313, 0.018174905329942703, 0.014539925381541252, 0.9596350193023682, 0.06678079068660736, 0.0030354904010891914, 0.9258245825767517, 0.18906590342521667, 0.3591011166572571, 0.05543727055191994, 0.0711582824587822, 0.23622895777225494, 0.0889478549361229, 0.005650250241160393, 0.005650250241160393, 0.0028251251205801964, 0.05311235040426254, 0.0011300500482320786, 0.9311612248420715, 0.9445038437843323, 0.050598420202732086, 0.9920835494995117, 0.020246602594852448, 0.21247011423110962, 0.0026894952170550823, 0.01075798086822033, 0.177506685256958, 0.009413233026862144, 0.587654709815979, 0.3153304159641266, 0.1772104799747467, 0.09121128171682358, 0.15785135328769684, 0.09009440988302231, 0.16790321469306946, 0.04130499064922333, 0.9582757949829102, 0.010724532417953014, 0.9437589049339294, 0.02144906483590603, 0.02144906483590603, 0.010724532417953014, 0.9928253889083862, 0.06837768852710724, 0.14832697808742523, 0.1062484085559845, 0.2035551220178604, 0.3729214072227478, 0.10151457041501999, 0.10518420487642288, 0.12642332911491394, 0.033375758677721024, 0.5501943230628967, 0.08799063414335251, 0.0970931127667427, 0.32104694843292236, 0.0757053941488266, 0.39745330810546875, 0.06519075483083725, 0.10865125805139542, 0.03224489092826843, 0.1971483826637268, 0.040079616010189056, 0.07004906237125397, 0.171150803565979, 0.2101471871137619, 0.31124892830848694, 1.005397915840149, 0.2912556827068329, 0.7106638550758362, 0.22600173950195312, 0.29326993227005005, 0.2163228690624237, 0.11517862230539322, 0.0793667808175087, 0.06968790292739868, 0.027412084862589836, 0.005482417065650225, 0.005482417065650225, 0.027412084862589836, 0.9320108890533447, 0.0795486569404602, 0.4384426176548004, 0.41069307923316956, 0.019116343930363655, 0.029599500820040703, 0.023432938382029533, 0.26874804496765137, 0.2400815784931183, 0.22770288586616516, 0.0706888809800148, 0.09349174052476883, 0.09935534000396729, 0.05907313898205757, 0.8807268142700195, 0.0322217121720314, 0.005370285362005234, 0.0161108560860157, 0.9992117881774902, 0.05535777285695076, 0.9344392418861389, 0.002214310923591256, 0.008857243694365025, 0.012569134123623371, 0.02765209600329399, 0.8446458578109741, 0.015082960948348045, 0.10055307298898697, 0.882810115814209, 0.019661694765090942, 0.021627863869071007, 0.043255727738142014, 0.02359403483569622, 0.01179701741784811, 0.9481512308120728, 0.0038078362122178078, 0.022847017273306847, 0.022847017273306847, 0.11874130368232727, 0.09739455580711365, 0.6822065711021423, 0.03157540410757065, 0.0177889596670866, 0.052477430552244186, 0.24234923720359802, 0.08852653205394745, 0.21346823871135712, 0.10861766338348389, 0.0659240186214447, 0.28127580881118774, 0.22224022448062897, 0.07920263707637787, 0.0336906723678112, 0.3386799097061157, 0.13890014588832855, 0.18736742436885834, 0.12552499771118164, 0.8697088956832886, 0.0867055207490921, 0.009126896969974041, 0.036507587879896164, 0.8716186881065369, 0.012737187556922436, 0.171952024102211, 0.7536169290542603, 0.010614322498440742, 0.036088697612285614, 0.016982916742563248, 0.9999787211418152, 0.9342185854911804, 0.0062281242571771145, 0.06020519882440567, 0.18070565164089203, 0.09035282582044601, 0.012907546013593674, 0.7228226065635681, 0.1221226379275322, 0.08800013363361359, 0.7435113787651062, 0.025142895057797432, 0.01795921102166176, 0.0035918422508984804, 0.1521075963973999, 0.05504446104168892, 0.5886816382408142, 0.03613605722784996, 0.15588927268981934, 0.01260560192167759, 0.9725018739700317, 0.19248735904693604, 0.18732485175132751, 0.5479620695114136, 0.02433748170733452, 0.04277496784925461, 0.0044249966740608215, 0.041290707886219025, 0.07921075075864792, 0.005056004971265793, 0.053088054060935974, 0.006741340272128582, 0.8148595094680786, 0.1897428333759308, 0.17022813856601715, 0.011708813719451427, 0.5470117330551147, 0.028521468862891197, 0.05313999950885773, 0.03115152381360531, 0.1646580547094345, 0.08900435268878937, 0.013350652530789375, 0.7031343579292297, 0.00240423739887774, 0.00120211869943887, 0.012021186761558056, 0.003606355981901288, 0.9797267317771912, 0.009265702217817307, 0.009265702217817307, 0.9821644425392151, 0.3296791911125183, 0.002035056706517935, 0.5403075814247131, 0.04680630564689636, 0.0071226987056434155, 0.07427956908941269, 0.028726831078529358, 0.009575610049068928, 0.009575610049068928, 0.9575610756874084, 1.0001755952835083, 0.2453148514032364, 0.13532626628875732, 0.2303425818681717, 0.027641110122203827, 0.27065253257751465, 0.09098532050848007, 0.02089109644293785, 0.94009929895401, 0.02089109644293785, 0.02089109644293785, 0.9761393666267395, 0.01791081391274929, 0.19150441884994507, 0.40906384587287903, 0.06057792901992798, 0.07034856081008911, 0.19410991668701172, 0.07425681501626968, 0.10016088932752609, 0.007512066513299942, 0.002504022093489766, 0.8688957095146179, 0.002504022093489766, 0.01752815581858158, 0.01848297193646431, 0.07162152230739594, 0.0069311149418354034, 0.01155185792595148, 0.8918034434318542, 0.017147699370980263, 0.9705597162246704, 0.0034295397344976664, 0.006859079468995333, 0.2074594348669052, 0.14465709030628204, 0.024697551503777504, 0.23991906642913818, 0.0303427055478096, 0.35211652517318726, 0.2565886378288269, 0.12342237681150436, 0.4284055829048157, 0.03247957304120064, 0.038975488394498825, 0.12017442286014557, 0.13194048404693604, 0.8492868542671204, 0.0012809756444767118, 0.016652682796120644, 0.9944369792938232, 0.006457383278757334, 0.9988541007041931, 0.3321482241153717, 0.29645469784736633, 0.13434652984142303, 0.08179769665002823, 0.1021231859922409, 0.05354031175374985, 0.0476025827229023, 0.01464694831520319, 0.8129056692123413, 0.0036617370788007975, 0.08055821806192398, 0.043940845876932144, 1.0017821788787842, 0.05812731757760048, 0.9300370812416077, 0.01453182939440012, 0.9958992600440979, 0.9998381733894348, 0.14682523906230927, 0.5218679904937744, 0.09336169809103012, 0.0857810452580452, 0.05984724313020706, 0.09216475486755371, 0.06186303496360779, 0.6358708739280701, 0.0831601470708847, 0.08011770248413086, 0.09228748083114624, 0.04766496270895004, 0.9718005061149597, 0.016852032393217087, 0.005617343820631504, 0.23288892209529877, 0.33617299795150757, 0.0599713958799839, 0.15525928139686584, 0.09861962497234344, 0.11727739870548248, 0.06456100940704346, 0.19880066812038422, 0.5105831027030945, 0.03424882888793945, 0.15195457637310028, 0.04054746404290199, 0.012628640048205853, 0.9092621207237244, 0.025257280096411705, 0.03788591921329498, 0.012628640048205853, 0.05341201648116112, 0.9080042839050293, 0.021364806219935417, 0.021364806219935417, 0.08630800992250443, 0.032365500926971436, 0.0053942506201565266, 0.03416358679533005, 0.08810608834028244, 0.7533969879150391, 0.013047201558947563, 0.006523600779473782, 0.03914160281419754, 0.939398467540741, 0.019589116796851158, 0.006529706064611673, 0.9729261994361877, 0.036345258355140686, 0.30078834295272827, 0.2042854130268097, 0.0839700773358345, 0.34715989232063293, 0.02882554940879345, 0.9980690479278564, 0.04235650226473808, 0.050574928522109985, 0.04235650226473808, 0.108103908598423, 0.04045994207262993, 0.716267466545105, 0.9996961355209351, 0.44106632471084595, 0.09583167731761932, 0.03688918426632881, 0.2650407552719116, 0.09543070942163467, 0.06615994870662689, 0.005870765540748835, 0.8982270956039429, 0.09393224865198135, 0.005870765540748835, 0.22536145150661469, 0.14547981321811676, 0.44543272256851196, 0.03491515666246414, 0.06612718850374222, 0.08305574953556061, 0.003140493994578719, 0.8259499669075012, 0.14760322868824005, 0.025123951956629753, 0.04392725229263306, 0.9371147751808167, 0.12531232833862305, 0.5552728772163391, 0.06739018857479095, 0.060706861317157745, 0.11473039537668228, 0.07630128413438797, 0.09790736436843872, 0.7234266400337219, 0.11966455727815628, 0.03898163512349129, 0.015411344356834888, 0.004532748367637396, 0.01832130178809166, 0.9710289835929871, 0.021357426419854164, 0.8809938430786133, 0.06941163539886475, 0.026696782559156418, 0.020875362679362297, 0.002455925103276968, 0.006139812525361776, 0.9713183641433716, 0.02903183177113533, 0.0072579579427838326, 0.021773874759674072, 0.9507924914360046, 0.045336704701185226, 0.0595044270157814, 0.06658829003572464, 0.0070838602259755135, 0.8103936314582825, 0.011334176175296307, 0.03285890072584152, 0.920049250125885, 0.043811868876218796, 0.01628473959863186, 0.9445148706436157, 0.01628473959863186, 0.021712984889745712, 0.27292710542678833, 0.0424022451043129, 0.017916440963745117, 0.15885910391807556, 0.008958220481872559, 0.49867427349090576, 0.13479195535182953, 0.8665196895599365, 0.9721822142601013, 0.011784027330577374, 0.017676040530204773, 0.04836568236351013, 0.0024182843044400215, 0.03385597839951515, 0.9141114354133606, 0.00866280309855938, 0.9918909072875977, 0.054427631199359894, 0.0018142544431611896, 0.003628508886322379, 0.2539956271648407, 0.014514035545289516, 0.6712741255760193, 0.22943665087223053, 0.1823851615190506, 0.2342873066663742, 0.1091400533914566, 0.16298247873783112, 0.08149123936891556, 0.6214796900749207, 0.02303554117679596, 0.07918467372655869, 0.023515447974205017, 0.15836934745311737, 0.09406179189682007, 0.1941411942243576, 0.004346444737166166, 0.0014488148735836148, 0.7388955950737, 0.026078667491674423, 0.03622037172317505, 0.9992138743400574, 0.9892013072967529, 0.16233627498149872, 0.37711966037750244, 0.3240481913089752, 0.035589106380939484, 0.03995969891548157, 0.06056391820311546, 0.021479634568095207, 0.01789969578385353, 0.025954559445381165, 0.7643170356750488, 0.038484346121549606, 0.13335274159908295, 0.0790276825428009, 0.0021651419810950756, 0.0032477127388119698, 0.13315622508525848, 0.7729556560516357, 0.00974313821643591, 0.040721967816352844, 0.08144393563270569, 0.8823093175888062, 0.050360340625047684, 0.04676317423582077, 0.010791501961648464, 0.003597167320549488, 0.8885003328323364, 0.7624596357345581, 0.002446394646540284, 0.0016309296479448676, 0.13618263602256775, 0.009785578586161137, 0.08807020634412766, 0.8038449883460999, 0.0047319792211055756, 0.0035489844158291817, 0.11711648851633072, 0.0023659896105527878, 0.06743070483207703, 0.9925552010536194, 0.039229437708854675, 0.004129414446651936, 0.014452951028943062, 0.05161768198013306, 0.045423559844493866, 0.8444652557373047, 0.8824323415756226, 0.010761369951069355, 0.1076136976480484, 1.0048264265060425, 0.9887511134147644, 0.05289706960320473, 0.0018669554265215993, 0.00560086639598012, 0.21656683087348938, 0.00560086639598012, 0.7175331711769104, 0.006870296783745289, 0.006870296783745289, 0.9893227219581604, 0.2216530442237854, 0.20159393548965454, 0.1584668755531311, 0.04463149607181549, 0.0787319615483284, 0.2943672835826874, 0.1733475923538208, 0.16357429325580597, 0.3765294849872589, 0.05298161134123802, 0.10904952883720398, 0.12448105961084366, 0.0290361437946558, 0.21381159126758575, 0.0118784224614501, 0.2652847468852997, 0.4711773991584778, 0.0092387730255723, 0.09567278623580933, 0.03826911374926567, 0.11480733752250671, 0.7558149695396423, 0.0860108956694603, 0.024574540555477142, 0.012287270277738571, 0.012287270277738571, 0.835534393787384, 0.024574540555477142, 0.03053848259150982, 0.02399452216923237, 0.0021813202183693647, 0.23558259010314941, 0.7023851275444031, 0.0021813202183693647, 0.1831291913986206, 0.44022423028945923, 0.20200324058532715, 0.1122240200638771, 0.019894259050488472, 0.04233906418085098, 0.013893157243728638, 0.0046310522593557835, 0.9817830920219421, 0.030190253630280495, 0.9182868599891663, 0.030190253630280495, 0.012579272501170635, 0.002515854313969612, 0.002515854313969612, 1.0027947425842285, 0.9988651871681213, 0.9023458957672119, 0.09941098839044571, 0.0865374282002449, 0.9086430072784424, 0.46918588876724243, 0.261997252702713, 0.14695937931537628, 0.019273361191153526, 0.051797159016132355, 0.0505925714969635, 0.9997750520706177], \"Term\": [\"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"achieves_optimal\", \"achieves_optimal\", \"achieves_optimal\", \"acquire\", \"acquire\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"activation_function\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"admm\", \"admm\", \"admm\", \"admm\", \"adoption\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversary\", \"adversary\", \"age\", \"age\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"allocation_lda\", \"allocation_lda\", \"allocation_lda\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate\", \"approximate_posterior\", \"approximate_posterior\", \"approximate_posterior\", \"approximate_posterior\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"arm\", \"arm\", \"arm\", \"arm\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"attack\", \"attack\", \"attack\", \"attack\", \"attacker\", \"attention_mechanism\", \"auc\", \"auc\", \"augment\", \"bandit_problem\", \"bandit_problem\", \"bandit_problem\", \"bandit_problem\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"basis_function\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"bayesian_inference\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavioral\", \"behavioral\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"black_box\", \"block_coordinate\", \"block_coordinate\", \"block_coordinate\", \"block_coordinate\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bootstrap\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"byproduct\", \"byproduct\", \"byproduct\", \"byproduct\", \"byproduct\", \"calibration\", \"calibration\", \"calibration\", \"calibration\", \"calibration\", \"cancer\", \"cancer\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"capture\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"chain\", \"change\", \"change\", \"change\", \"change\", \"change\", \"change\", \"character\", \"cifar\", \"cifar\", \"city\", \"city\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"cloud\", \"cloud\", \"cloud\", \"cloud\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"collective\", \"community\", \"community\", \"community\", \"community\", \"community\", \"community\", \"company\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"complexity\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"consider\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"constraint\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"context\", \"context\", \"context\", \"context\", \"context\", \"context\", \"contextual_bandit\", \"contextual_bandit\", \"contextual_bandit\", \"contextual_bandit\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex_optimization\", \"convex_optimization\", \"convex_optimization\", \"convex_optimization\", \"convex_optimization\", \"convex_optimization\", \"convolutional_neural\", \"convolutional_neural\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariates\", \"covariates\", \"covariates\", \"covariates\", \"covariates\", \"covariates\", \"cpu\", \"customer\", \"customer\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\", \"data_set\", \"data_set\", \"data_set\", \"data_set\", \"data_set\", \"data_set\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_boundary\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_neural\", \"deep_neural\", \"deep_neural\", \"deliver\", \"deliver\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent_sgd\", \"descent_sgd\", \"descent_sgd\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary_learning\", \"dictionary_learning\", \"dictionary_learning\", \"dictionary_learning\", \"dictionary_learning\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain_adaptation\", \"domain_adaptation\", \"domain_adaptation\", \"domain_specific\", \"domain_specific\", \"drive\", \"drug\", \"drug\", \"drug\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"elastic_net\", \"embeddings\", \"embeddings\", \"embeddings\", \"embeddings\", \"embeddings\", \"embeddings\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"entity\", \"entity\", \"entity\", \"entity\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"epoch\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"equation\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"euclidean_space\", \"euclidean_space\", \"event\", \"event\", \"event\", \"event\", \"event\", \"exact_recovery\", \"exact_recovery\", \"exact_recovery\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"excess_risk\", \"excess_risk\", \"excess_risk\", \"exchange\", \"exchange\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"expectation_propagation\", \"expectation_propagation\", \"expectation_propagation\", \"expectation_propagation\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"explanation\", \"explanation\", \"explanation\", \"explanation\", \"explanation\", \"feedforward_neural\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"flexibly\", \"flexibly\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecasting\", \"forecasting\", \"forecasting\", \"forecasting\", \"forecasting\", \"fourier\", \"fourier\", \"fourier\", \"fourier\", \"frank_wolfe\", \"fully_connected\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gamma\", \"gaussian_graphical\", \"gaussian_graphical\", \"gaussian_graphical\", \"gaussian_graphical\", \"gaussian_graphical\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"general\", \"general\", \"general\", \"general\", \"general\", \"general\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generation\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generator\", \"generator\", \"generator\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"geometry\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampling\", \"gibbs_sampling\", \"gibbs_sampling\", \"gibbs_sampling\", \"gibbs_sampling\", \"gibbs_sampling\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"global_convergence\", \"global_convergence\", \"global_minimum\", \"global_minimum\", \"global_minimum\", \"global_optimum\", \"global_optimum\", \"global_optimum\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gpu\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradually\", \"hardware\", \"health\", \"health\", \"hessian\", \"hessian\", \"hessian\", \"hessian\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hypothesis_test\", \"ica\", \"ica\", \"identifiability\", \"identifiability\", \"identifiability\", \"identifiability\", \"identifiability\", \"identifiability\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imagenet\", \"imagenet\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inequality\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inform\", \"initialization\", \"initialization\", \"initialization\", \"initialization\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"instability\", \"instability\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"inversion\", \"inversion\", \"inversion\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"known\", \"known\", \"known\", \"known\", \"known\", \"known\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labelled\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"lasso\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent_dirichlet\", \"latent_dirichlet\", \"latent_dirichlet\", \"latent_representation\", \"latent_representation\", \"latent_representation\", \"latent_space\", \"latent_space\", \"latent_space\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_convergence\", \"linear_convergence\", \"linear_convergence\", \"linear_convergence\", \"linear_regression\", \"linear_regression\", \"linear_regression\", \"linear_regression\", \"link_prediction\", \"link_prediction\", \"link_prediction\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"local_minimum\", \"local_minimum\", \"local_minimum\", \"logarithmic_factor\", \"logarithmic_factor\", \"logarithmic_factor\", \"logarithmic_factor\", \"logic\", \"long_term\", \"long_term\", \"long_term\", \"longitudinal\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"machine_translation\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manipulation\", \"manipulation\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"mcmc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mdp\", \"mdp\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"medical\", \"medical\", \"medical\", \"medical\", \"medical\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"meta_learning\", \"meta_learning\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"mini_batch\", \"minimax_optimal\", \"minimax_optimal\", \"minimax_rate\", \"minimax_rate\", \"minimax_rate\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture_component\", \"mixture_component\", \"mixture_component\", \"mnist\", \"mnist\", \"mnist\", \"mnist\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"molecular\", \"molecular\", \"molecular\", \"multi_label\", \"multi_label\", \"multi_label\", \"multi_label\", \"multimodal\", \"multimodal\", \"multimodal\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise_level\", \"noise_level\", \"noise_level\", \"noise_level\", \"noise_level\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex_optimization\", \"nonconvex_optimization\", \"nonnegative_matrix\", \"nonnegative_matrix\", \"nonnegative_matrix\", \"nonsmooth\", \"nonsmooth\", \"nonsmooth\", \"nonsmooth\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"nuclear_norm\", \"nuclear_norm\", \"nuclear_norm\", \"nuclear_norm\", \"nuclear_norm\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"opportunity\", \"opportunity\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal_transport\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"oracle_inequality\", \"oracle_inequality\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"outlier\", \"outlier\", \"outlier\", \"outlier\", \"outlier\", \"outlier_detection\", \"outlier_detection\", \"outlier_detection\", \"output\", \"output\", \"output\", \"output\", \"output\", \"output\", \"pac\", \"pac\", \"pac\", \"pac\", \"pac\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"particular\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"pca\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"person\", \"person\", \"phenotype\", \"phenotype\", \"phenotype\", \"phenotype\", \"planning\", \"planning\", \"planning\", \"planning\", \"player\", \"player\", \"player\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy_gradient\", \"policy_gradient\", \"precision_recall\", \"precision_recall\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"prevent\", \"prevent\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal\", \"primal_dual\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"probabilistic\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"procedure\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processor\", \"prominent\", \"prominent\", \"property\", \"property\", \"property\", \"property\", \"property\", \"property\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"python\", \"quantization\", \"quantization\", \"quantization\", \"quantization\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real_world\", \"real_world\", \"real_world\", \"real_world\", \"real_world\", \"real_world\", \"recommender\", \"recommender\", \"recommender_system\", \"recommender_system\", \"recommender_system\", \"recommender_system\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recurrent\", \"recurrent_neural\", \"recurrent_neural\", \"recurrent_neural\", \"reduce_computational\", \"reduce_computational\", \"reduce_computational\", \"reduce_computational\", \"regime\", \"regime\", \"regime\", \"regime\", \"regime\", \"regime\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression_coefficient\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"reproducing_kernel\", \"reproducing_kernel\", \"reproducing_kernel\", \"reproducing_kernel\", \"reproducing_kernel\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward_function\", \"reward_function\", \"reward_function\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"road\", \"road\", \"road\", \"road\", \"robot\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"robust\", \"row_column\", \"row_column\", \"row_column\", \"row_column\", \"saddle_point\", \"saddle_point\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"shallow\", \"shallow\", \"shot\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simple\", \"simulation_study\", \"simulation_study\", \"simulation_study\", \"simulation_study\", \"simulation_study\", \"simulation_study\", \"skill\", \"smooth_function\", \"smooth_function\", \"smooth_function\", \"social_medium\", \"softmax\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"source_domain\", \"source_domain\", \"source_domain\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse_coding\", \"sparse_coding\", \"sparse_coding\", \"sparse_coding\", \"sparse_coding\", \"sparse_signal\", \"sparse_signal\", \"sparse_signal\", \"sparse_signal\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"spatio_temporal\", \"speaker\", \"speaker\", \"speaker\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"spectral\", \"speech_recognition\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state_action\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"stationary_point\", \"stationary_point\", \"stationary_point\", \"stationary_point\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"statistical\", \"step_size\", \"step_size\", \"step_size\", \"step_size\", \"stimulus\", \"stimulus\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"strong_convexity\", \"strong_convexity\", \"strongly_convex\", \"strongly_convex\", \"strongly_convex\", \"strongly_convex\", \"student\", \"student\", \"student\", \"student\", \"subsampling\", \"subsampling\", \"subsampling\", \"subsampling\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace_clustering\", \"subspace_clustering\", \"subspace_clustering\", \"svd\", \"svd\", \"svd\", \"svd\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"tail\", \"tail\", \"target_domain\", \"target_domain\", \"target_domain\", \"teacher\", \"teacher\", \"teacher\", \"teacher\", \"team\", \"team\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"temporal\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text_classification\", \"textual\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic_modeling\", \"topic_modeling\", \"topic_modeling\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"training_testing\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"trajectory\", \"twitter\", \"twitter\", \"twitter\", \"uncertain\", \"unprecedented\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user_item\", \"user_item\", \"user_item\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational_approximation\", \"variational_approximation\", \"variational_approximation\", \"variational_approximation\", \"variational_bayes\", \"variational_bayes\", \"variational_bayes\", \"variational_bayes\", \"variational_bayes\", \"variational_bayes\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vehicle\", \"vehicle\", \"vehicle\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vulnerability\", \"vulnerable\", \"wasserstein_distance\", \"wasserstein_distance\", \"website\", \"website\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"x\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el98571406002433036726943135956\", ldavis_el98571406002433036726943135956_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el98571406002433036726943135956\", ldavis_el98571406002433036726943135956_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el98571406002433036726943135956\", ldavis_el98571406002433036726943135956_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.036521  0.004829       1        1  28.493940\n",
       "1      0.091829  0.006359       2        1  17.189314\n",
       "2      0.128143  0.074736       3        1  15.952589\n",
       "3     -0.107865 -0.066721       4        1  14.144185\n",
       "4      0.063378 -0.098101       5        1   9.226559\n",
       "5     -0.138964  0.078897       6        1  14.993413, topic_info=     Category         Freq                    Term        Total  loglift  \\\n",
       "term                                                                       \n",
       "125   Default  3301.000000                  kernel  3301.000000  30.0000   \n",
       "1256  Default  1769.000000                  policy  1769.000000  29.0000   \n",
       "555   Default  2200.000000               estimator  2200.000000  28.0000   \n",
       "919   Default  1458.000000                   agent  1458.000000  27.0000   \n",
       "317   Default  3330.000000          representation  3330.000000  26.0000   \n",
       "274   Default  2392.000000           approximation  2392.000000  25.0000   \n",
       "489   Default  2758.000000              estimation  2758.000000  24.0000   \n",
       "807   Default  1583.000000                    node  1583.000000  23.0000   \n",
       "2025  Default  2547.000000                gradient  2547.000000  22.0000   \n",
       "1605  Default   814.000000                 student   814.000000  21.0000   \n",
       "1838  Default  1606.000000                    user  1606.000000  20.0000   \n",
       "308   Default  2603.000000               inference  2603.000000  19.0000   \n",
       "531   Default  1168.000000             environment  1168.000000  18.0000   \n",
       "166   Default  2379.000000              regression  2379.000000  17.0000   \n",
       "493   Default  2248.000000                    rate  2248.000000  16.0000   \n",
       "486   Default  2221.000000                   bound  2221.000000  15.0000   \n",
       "400   Default   923.000000                   topic   923.000000  14.0000   \n",
       "385   Default  2632.000000                   error  2632.000000  13.0000   \n",
       "2777  Default  1592.000000             deep_neural  1592.000000  12.0000   \n",
       "1349  Default  3385.000000          neural_network  3385.000000  11.0000   \n",
       "56    Default  2762.000000            optimization  2762.000000  10.0000   \n",
       "352   Default  2540.000000                  sparse  2540.000000   9.0000   \n",
       "2101  Default   831.000000                  reward   831.000000   8.0000   \n",
       "1239  Default   998.000000        gaussian_process   998.000000   7.0000   \n",
       "1832  Default  1186.000000  reinforcement_learning  1186.000000   6.0000   \n",
       "1259  Default  1581.000000                   state  1581.000000   5.0000   \n",
       "1249  Default   829.000000                  action   829.000000   4.0000   \n",
       "2327  Default   668.000000                 outlier   668.000000   3.0000   \n",
       "540   Default  1901.000000                   prior  1901.000000   2.0000   \n",
       "90    Default  1990.000000              classifier  1990.000000   1.0000   \n",
       "...       ...          ...                     ...          ...      ...   \n",
       "2398   Topic6   247.116837                 traffic   277.996521   1.7798   \n",
       "1838   Topic6  1153.481079                    user  1606.894287   1.5660   \n",
       "1259   Topic6  1133.064941                   state  1581.811401   1.5639   \n",
       "877    Topic6   254.579651                activity   299.321655   1.7357   \n",
       "2271   Topic6   418.927948                 spatial   556.147705   1.6142   \n",
       "1903   Topic6   412.067871                    game   550.019287   1.6088   \n",
       "55     Topic6   599.255859                  object   912.718140   1.4768   \n",
       "1719   Topic6   706.524963                   human  1146.618896   1.4133   \n",
       "75     Topic6   835.370117                  system  1674.439697   1.2022   \n",
       "686    Topic6   869.156372                 dynamic  1849.514648   1.1424   \n",
       "734    Topic6   516.425476               detection   882.853271   1.3613   \n",
       "781    Topic6   656.315735                behavior  1282.760620   1.2274   \n",
       "1474   Topic6   483.656769                  change   803.969604   1.3894   \n",
       "374    Topic6   562.088013                 pattern  1027.374756   1.2945   \n",
       "818    Topic6   524.771362             interaction   926.981201   1.3286   \n",
       "912    Topic6   574.392883                 control  1082.815918   1.2636   \n",
       "460    Topic6   861.557190                 process  2769.487549   0.7299   \n",
       "756    Topic6   436.799683                 predict   743.633972   1.3655   \n",
       "135    Topic6   369.638519                temporal   551.190613   1.4980   \n",
       "201    Topic6   582.304810              clustering  1712.858887   0.8186   \n",
       "47     Topic6   451.488098                    goal   925.397339   1.1799   \n",
       "791    Topic6   666.746399                   image  2752.395264   0.4797   \n",
       "33     Topic6   557.174194                 complex  1691.393066   0.7871   \n",
       "230    Topic6   587.227417                   value  1994.107544   0.6750   \n",
       "382    Topic6   546.348633                 dataset  2102.270996   0.5500   \n",
       "24     Topic6   498.989594                sequence  1417.144531   0.8537   \n",
       "1402   Topic6   503.306488                   learn  2152.585205   0.4443   \n",
       "1485   Topic6   431.045654                decision   835.700562   1.2355   \n",
       "584    Topic6   447.820465                    real  1592.742798   0.6287   \n",
       "252    Topic6   451.329651                 present  2686.071289   0.1139   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "125   30.0000  \n",
       "1256  29.0000  \n",
       "555   28.0000  \n",
       "919   27.0000  \n",
       "317   26.0000  \n",
       "274   25.0000  \n",
       "489   24.0000  \n",
       "807   23.0000  \n",
       "2025  22.0000  \n",
       "1605  21.0000  \n",
       "1838  20.0000  \n",
       "308   19.0000  \n",
       "531   18.0000  \n",
       "166   17.0000  \n",
       "493   16.0000  \n",
       "486   15.0000  \n",
       "400   14.0000  \n",
       "385   13.0000  \n",
       "2777  12.0000  \n",
       "1349  11.0000  \n",
       "56    10.0000  \n",
       "352    9.0000  \n",
       "2101   8.0000  \n",
       "1239   7.0000  \n",
       "1832   6.0000  \n",
       "1259   5.0000  \n",
       "1249   4.0000  \n",
       "2327   3.0000  \n",
       "540    2.0000  \n",
       "90     1.0000  \n",
       "...       ...  \n",
       "2398  -6.3726  \n",
       "1838  -4.8320  \n",
       "1259  -4.8498  \n",
       "877   -6.3429  \n",
       "2271  -5.8448  \n",
       "1903  -5.8613  \n",
       "55    -5.4868  \n",
       "1719  -5.3222  \n",
       "75    -5.1546  \n",
       "686   -5.1150  \n",
       "734   -5.6356  \n",
       "781   -5.3959  \n",
       "1474  -5.7011  \n",
       "374   -5.5509  \n",
       "818   -5.6195  \n",
       "912   -5.5292  \n",
       "460   -5.1238  \n",
       "756   -5.8030  \n",
       "135   -5.9700  \n",
       "201   -5.5155  \n",
       "47    -5.7700  \n",
       "791   -5.3801  \n",
       "33    -5.5596  \n",
       "230   -5.5071  \n",
       "382   -5.5793  \n",
       "24    -5.6699  \n",
       "1402  -5.6613  \n",
       "1485  -5.8163  \n",
       "584   -5.7781  \n",
       "252   -5.7703  \n",
       "\n",
       "[455 rows x 6 columns], token_table=      Topic      Freq                   Term\n",
       "term                                        \n",
       "404       1  0.593796               accuracy\n",
       "404       2  0.059809               accuracy\n",
       "404       3  0.072344               accuracy\n",
       "404       4  0.073419               accuracy\n",
       "404       5  0.087744               accuracy\n",
       "404       6  0.113172               accuracy\n",
       "1373      1  0.044287       achieves_optimal\n",
       "1373      3  0.952175       achieves_optimal\n",
       "1373      4  0.011072       achieves_optimal\n",
       "2040      1  0.016028                acquire\n",
       "2040      6  0.977725                acquire\n",
       "1249      1  0.009646                 action\n",
       "1249      2  0.006029                 action\n",
       "1249      3  0.004823                 action\n",
       "1249      4  0.010852                 action\n",
       "1249      5  0.008440                 action\n",
       "1249      6  0.958586                 action\n",
       "2959      1  0.997335    activation_function\n",
       "877       1  0.070159               activity\n",
       "877       2  0.016704               activity\n",
       "877       3  0.026727               activity\n",
       "877       4  0.036750               activity\n",
       "877       5  0.003341               activity\n",
       "877       6  0.851926               activity\n",
       "2985      1  0.014922                   admm\n",
       "2985      2  0.843113                   admm\n",
       "2985      3  0.089534                   admm\n",
       "2985      5  0.052228                   admm\n",
       "2716      1  0.993176               adoption\n",
       "1900      1  0.906076            adversarial\n",
       "...     ...       ...                    ...\n",
       "321       5  0.702385  variational_inference\n",
       "321       6  0.002181  variational_inference\n",
       "231       1  0.183129                 vector\n",
       "231       2  0.440224                 vector\n",
       "231       3  0.202003                 vector\n",
       "231       4  0.112224                 vector\n",
       "231       5  0.019894                 vector\n",
       "231       6  0.042339                 vector\n",
       "2902      1  0.013893                vehicle\n",
       "2902      2  0.004631                vehicle\n",
       "2902      6  0.981783                vehicle\n",
       "262       1  0.030190                 vertex\n",
       "262       2  0.918287                 vertex\n",
       "262       3  0.030190                 vertex\n",
       "262       4  0.012579                 vertex\n",
       "262       5  0.002516                 vertex\n",
       "262       6  0.002516                 vertex\n",
       "2923      1  1.002795          vulnerability\n",
       "3015      1  0.998865             vulnerable\n",
       "2851      4  0.902346   wasserstein_distance\n",
       "2851      6  0.099411   wasserstein_distance\n",
       "1933      1  0.086537                website\n",
       "1933      4  0.908643                website\n",
       "1047      1  0.469186                 weight\n",
       "1047      2  0.261997                 weight\n",
       "1047      3  0.146959                 weight\n",
       "1047      4  0.019273                 weight\n",
       "1047      5  0.051797                 weight\n",
       "1047      6  0.050593                 weight\n",
       "2729      1  0.999775                      x\n",
       "\n",
       "[1691 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you set the lambda value to be lower, you can see more exclusive terms\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(filtered_lda, filterd_bow, dictionary, sort_topics = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, the bulk of the topic modeling is completed. I have chosen my number of topics, and have gotten a general human-readable tag for each topic. What's next? Well, there are several options:**\n",
    "\n",
    "1. Set up a small program that allows one to compare a new document against the existing models and corpus. \n",
    "2. Set up Dynamic Topic Modeling to track the Evolution of a Topic over time\n",
    "3. Set up a Grid-Search process for fine-tuning the number of topics and other parameters\n",
    "4. Include more abstracts, this time from different fields than ML, to see how well the process generalizes while mainting intra-topic distinctions\n",
    "\n",
    "**Let's begin with a technique to help us choose the best number of topics:\n",
    "\n",
    "### Coherence analys: ###\n",
    "\n",
    "*note:* I this is heavily \"inspired\" by [this notebook](https://nbviewer.jupyter.org/github/dsquareindia/gensim/blob/280375fe14adea67ce6384ba7eabf362b05e6029/docs/notebooks/topic_coherence_tutorial.ipynb#topic=2&lambda=1&term=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_vals = compute_coherence_values(dictionary=dictionary,\n",
    "                                                      corpus = filterd_bow ,\n",
    "                                                      texts=corpus, limit = 21, \n",
    "                                                      start=5, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit=21; start=5; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "#In this case itseems to suggest 11 topics...hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that this function is not run with the exact correct parameters... (still need to update that)**\n",
    "\n",
    "But if run correctly, it can be used to help us inform our choice of $t$ topics. \n",
    "\n",
    "Ok, so out of our list: \n",
    "\n",
    "1. Set up a small program that allows one to compare a new document against the existing models and corpus. \n",
    "2. Set up Dynamic Topic Modeling to track the Evolution of a Topic over time\n",
    "3. Set up a Grid-Search process for fine-tuning the number of topics and other parameters\n",
    "4. Include more abstracts, this time from different fields than ML, to see how well the process generalizes while mainting intra-topic distinctions\n",
    "\n",
    "I just showed (part of) #3. Now lets look at #1: \n",
    "\n",
    "## Querying a new document against the existing model and corpus ##\n",
    "\n",
    "Ideally, I will integrate this feature with a user interface, such that the user will be able to see how the words are distributed among topics, and also get a list of the most closely related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_lda.get_document_topics(dictionary.doc2bow(['general', 'topic', 'network']), per_word_topics = True)\n",
    "proc_ab = prep_text(myab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_bow = dictionary.doc2bow(bigrams[proc_ab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradient'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[3706]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"This\", -1], [\"paper\", -1], [\"presents\", 0], [\"a\", -1], [\"novel\", 0], [\"optimization\", 0], [\"method\", -1], [\"for\", -1], [\"maximizing\", 0], [\"generalization\", 0], [\"over\", -1], [\"tasks\", -1], [\"in\", -1], [\"meta-learning.\\\\\", -1], [\"The\", -1], [\"goal\", 0], [\"of\", -1], [\"meta-learning\", -1], [\"is\", -1], [\"to\", -1], [\"learn\", 0], [\"a\", -1], [\"model\", -1], [\"for\", -1], [\"an\", -1], [\"agent\", 5], [\"adapting\", 0], [\"rapidly\", 0], [\"when\", -1], [\"presented\", 0], [\"with\", -1], [\"previously\", 0], [\"unseen\", 0], [\"tasks.\\\\\", -1], [\"Tasks\", -1], [\"are\", -1], [\"sampled\", 1], [\"from\", -1], [\"a\", -1], [\"specific\", 0], [\"distribution\", -1], [\"which\", -1], [\"is\", -1], [\"assumed\", 0], [\"to\", -1], [\"be\", -1], [\"similar\", 0], [\"for\", -1], [\"both\", -1], [\"seen\", 0], [\"and\", -1], [\"unseen\", 0], [\"tasks.\", -1], [\"We\", -1], [\"focus\", 0], [\"on\", -1], [\"\\\\\", -1], [\"a\", -1], [\"family\", 0], [\"of\", -1], [\"meta-learning\", -1], [\"methods\", -1], [\"learning\", -1], [\"initial\", 0], [\"parameters\", -1], [\"of\", -1], [\"a\", -1], [\"base\", 0], [\"model\", -1], [\"which\", -1], [\"can\", -1], [\"be\", -1], [\"fine-tuned\", -1], [\"quickly\", 0], [\"on\", -1], [\"a\", -1], [\"new\", -1], [\"task,\", -1], [\"\\\\\", -1], [\"by\", -1], [\"few\", -1], [\"gradient\", 1], [\"steps\", 0], [\"(MAML).\", -1], [\"Our\", -1], [\"approach\", -1], [\"is\", -1], [\"based\", -1], [\"on\", -1], [\"pushing\", -1], [\"the\", -1], [\"parameters\", -1], [\"of\", -1], [\"the\", -1], [\"model\", -1], [\"to\", -1], [\"a\", -1], [\"direction\", 0], [\"in\", -1], [\"which\", -1], [\"tasks\", -1], [\"have\\\\\", -1], [\"more\", -1], [\"agreement\", 0], [\"upon.\", -1], [\"If\", -1], [\"the\", -1], [\"gradients\", 1], [\"of\", -1], [\"a\", -1], [\"task\", -1], [\"agree\", -1], [\"with\", -1], [\"the\", -1], [\"parameters\", -1], [\"update\", 0], [\"vector,\", 0], [\"then\", -1], [\"their\", -1], [\"inner\", -1], [\"product\", 0], [\"will\", -1], [\"be\", -1], [\"a\", -1], [\"large\\\\\", 0], [\"positive\", 0], [\"value.\", 0], [\"As\", -1], [\"a\", -1], [\"result,\", -1], [\"given\", 0], [\"a\", -1], [\"batch\", 0], [\"of\", -1], [\"tasks\", -1], [\"to\", -1], [\"be\", -1], [\"optimized\", 0], [\"for,\", -1], [\"we\", -1], [\"associate\", -1], [\"a\", -1], [\"positive\", 0], [\"(negative)\", 0], [\"weight\", 0], [\"to\", -1], [\"the\", -1], [\"loss\\\\\", 0], [\"function\", -1], [\"of\", -1], [\"a\", -1], [\"task,\", -1], [\"if\", -1], [\"the\", -1], [\"inner\", -1], [\"product\", 0], [\"between\", -1], [\"its\", -1], [\"gradients\", 1], [\"and\", -1], [\"the\", -1], [\"average\", 0], [\"of\", -1], [\"the\", -1], [\"gradients\", 1], [\"of\", -1], [\"all\", -1], [\"tasks\", -1], [\"in\", -1], [\"the\", -1], [\"batch\", 0], [\"is\", -1], [\"a\\\\\", -1], [\"positive\", 0], [\"(negative)\", 0], [\"value.\", 0], [\"Therefore,\", -1], [\"the\", -1], [\"degree\", 0], [\"of\", -1], [\"the\", -1], [\"contribution\", 0], [\"of\", -1], [\"a\", -1], [\"task\", -1], [\"to\", -1], [\"the\", -1], [\"parameter\", -1], [\"updates\", 0], [\"is\", -1], [\"controlled\", 0], [\"by\", -1], [\"introducing\\\\\", 0], [\"a\", -1], [\"set\", -1], [\"of\", -1], [\"weights\", 0], [\"on\", -1], [\"the\", -1], [\"loss\", 0], [\"function\", -1], [\"of\", -1], [\"the\", -1], [\"tasks.\", -1], [\"Our\", -1], [\"method\", -1], [\"can\", -1], [\"be\", -1], [\"easily\", 0], [\"integrated\", 0], [\"with\", -1], [\"the\", -1], [\"current\", 0], [\"meta-learning\", -1], [\"algorithms\", -1], [\"for\\\\\", -1], [\"neural\", 0], [\"networks.\", -1], [\"Our\", -1], [\"experiments\", 0], [\"demonstrate\", 0], [\"that\", -1], [\"it\", -1], [\"yields\", 0], [\"models\", -1], [\"with\", -1], [\"better\", 0], [\"generalization\", 0], [\"compared\", 0], [\"to\", -1], [\"MAML\", -1], [\"and\", -1], [\"Reptile.\", -1]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def colorize(ab, model, dictionary):\n",
    "    \"\"\"\n",
    "    takes an abstract, an LDA model, and dictionary, and returns json with each word mapped to it's topic (or -1 if n/a)\n",
    "    \n",
    "    Still needs refinement to take advantage of bigrams\n",
    "    \"\"\"\n",
    "    proc_ab = prep_text(ab)\n",
    "#     bigram_ab = bigrams[proc_ab]\n",
    "#     ab_bow = dictionary.doc2bow(bigram_ab)\n",
    "    ab_bow = dictionary.doc2bow(proc_ab) # not using bigrams for now b/c it complicates things\n",
    "#     print(len(ab_bow))\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(ab_bow, per_word_topics = True)\n",
    "#     print(model.get_document_topics(ab_bow, per_word_topics = True))\n",
    "    topic_colors = {0: 'red', 1: 'yellow', 2:'green', 3:'blue', 4:'aqua', 5: \"magenta\", 6:\"purple\"} # Have to figure out how to not hard-code this for the future\n",
    "    raw_ab = ab.split()\n",
    "    raw_proc = [\"\".join(prep_text(w)) for w in raw_ab] #this is a hack b/c the preprocessing is off\n",
    "#     print(\"bigrams:\", bigram_ab)\n",
    "#     print(\"raw_ab: \", raw_ab)\n",
    "#     print(\"raw_proc: \", raw_proc)\n",
    "#     print(\"word_topics: \", word_topics)\n",
    "    result = [];\n",
    "    for i in range(len(raw_proc)):\n",
    "#         print(raw_proc[i])\n",
    "        if not raw_proc[i] or raw_proc[i] not in dictionary.token2id:\n",
    "#             print(\"this isn't valid\")\n",
    "            result.append((raw_ab[i], -1))\n",
    "        else:\n",
    "#             print(\"searching for \", raw_proc[i])\n",
    "            #word_topics.filter(lambda w: w[0] == dictionary.token2id[raw_proc[i]])[0] \n",
    "            sample = [w for w in word_topics if w[0] == dictionary.token2id[raw_proc[i]]]\n",
    "#             print(sample)\n",
    "            match = sample[0] \n",
    "#             print(match)\n",
    "            result.append((raw_ab[i], match[1][0]))\n",
    "#             word_topics.remove(match)\n",
    "    return json.dumps(result)\n",
    "\n",
    "myab = r\"This paper presents a novel optimization method for maximizing generalization over tasks in meta-learning.\\\n",
    "        The goal of meta-learning is to learn a model for an agent adapting rapidly when presented with previously unseen tasks.\\\n",
    "        Tasks are sampled from a specific distribution which is assumed to be similar for both seen and unseen tasks. We focus on \\\n",
    "        a family of meta-learning methods learning initial parameters of a base model which can be fine-tuned quickly on a new task, \\\n",
    "        by few gradient steps (MAML). Our approach is based on pushing the parameters of the model to a direction in which tasks have\\\n",
    "        more agreement upon. If the gradients of a task agree with the parameters update vector, then their inner product will be a large\\\n",
    "        positive value. As a result, given a batch of tasks to be optimized for, we associate a positive (negative) weight to the loss\\\n",
    "        function of a task, if the inner product between its gradients and the average of the gradients of all tasks in the batch is a\\\n",
    "        positive (negative) value. Therefore, the degree of the contribution of a task to the parameter updates is controlled by introducing\\\n",
    "        a set of weights on the loss function of the tasks. Our method can be easily integrated with the current meta-learning algorithms for\\\n",
    "        neural networks. Our experiments demonstrate that it yields models with better generalization compared to MAML and Reptile.\"\n",
    "\n",
    "print(colorize(myab, filtered_lda, dictionary))\n",
    "\n",
    "# This particular choice of abstract may not have been ideal. but it gets the point across that the topics are sampled from topics 1 and 2, both of\n",
    "# which have relation to the idea of gradient descent and optimization (the topic of this abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, colorize, can be called by the front end, and the Flask server can return a json with the entire text nicely labled with each word'd topic distribution. Next up is the document similarity. To do this effectivley, we need to utilize the index matrix provided by Gensim: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The index of corp_matrix so that query lookup is efficient\n",
    "\n",
    "index = similarities.MatrixSimilarity(corp_matrix)\n",
    "# index.save('./the_data_strikes_back/first_sim.index')\n",
    "# index = similarities.MatrixSimilarity.load('./the_data_strikes_back/first_sim.index')\n",
    "\n",
    "def get_similar_docs(querydoc, index, tokenizer, phraser, dictionary, top_n_docs=10): \n",
    "    \"\"\"\n",
    "    does a (I think?) cosine similarity with all documents in my corpus\n",
    "    \"\"\"\n",
    "    # curr_text is a list of strings (including bigrams)\n",
    "    curr_text = phraser[tokenizer(querydoc)]\n",
    "    curr_bow = dictionary.doc2bow(curr_text)\n",
    "    #fit the model to the bow - convert to lda space\n",
    "    sims = index[curr_bow]\n",
    "    return sorted(enumerate(sims), key= lambda i: -i[1])[:top_n_docs]\n",
    "\n",
    "def display_similars(tup_array, corp_matrix, corpus= abstracts):\n",
    "    result = [{\"topics\":model[corp_matrix[sim[0]]], \"percentage\": float(sim[1]), \"text\":corpus[sim[0]]} for sim in tup_array]\n",
    "#     print(type(result[1][\"topics\"]), type(result[1][\"percentage\"]), type(result[1][\"text\"]))\n",
    "#     sim = tup_array[0]\n",
    "#     print(sim[0], type(sim[0]))\n",
    "#     print(corp_matrix[sim[0]], type(corp_matrix[sim[0]]))\n",
    "#     print(model[corp_matrix[sim[0]]], type(model[corp_matrix[sim[0]]]))\n",
    "#     return json.dumps(result) # for some reason it won't json-serialize at the moment...hmmm\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's select a new abstract at random, and then query it against our word colorizer and our similar documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topics': [(1, 0.07292722), (4, 0.91831183)],\n",
       "  'percentage': 0.16466586291790009,\n",
       "  'text': 'Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classification as C-learning, and we present efficient coordinate descent algorithms for the training of regularized ${\\\\cal C}$-learning models.'},\n",
       " {'topics': [(2, 0.2752507), (3, 0.016539626), (4, 0.7039611)],\n",
       "  'percentage': 0.14559419453144073,\n",
       "  'text': \"Stein's method for measuring convergence to a continuous target distribution relies on an operator characterizing the target and Stein factor bounds on the solutions of an associated differential equation. While such operators and bounds are readily available for a diversity of univariate targets, few multivariate targets have been analyzed. We introduce a new class of characterizing operators based on Ito diffusions and develop explicit multivariate Stein factor bounds for any target with a fast-coupling Ito diffusion. As example applications, we develop computable and convergence-determining diffusion Stein discrepancies for log-concave, heavy-tailed, and multimodal targets and use these quality measures to select the hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare random and deterministic quadrature rules, and quantify bias-variance tradeoffs in approximate MCMC. Our results establish a near-linear relationship between diffusion Stein discrepancies and Wasserstein distances, improving upon past work even for strongly log-concave targets. The exposed relationship between Stein factors and Markov process coupling may be of independent interest.\"},\n",
       " {'topics': [(2, 0.30414206), (4, 0.68635696)],\n",
       "  'percentage': 0.14113496243953705,\n",
       "  'text': 'Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.'},\n",
       " {'topics': [(0, 0.46043965), (3, 0.28854957), (4, 0.24198627)],\n",
       "  'percentage': 0.13725489377975464,\n",
       "  'text': 'We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\\\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\\\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\\\hat{y}$ through argmax function, i.e. argmax $f(x)$.'},\n",
       " {'topics': [(2, 0.41510826), (3, 0.07854465), (4, 0.5009047)],\n",
       "  'percentage': 0.13720646500587463,\n",
       "  'text': \"A latent force model is a Gaussian process with a covariance function inspired by a differential operator. Such covariance function is obtained by performing convolution integrals between Green's functions associated to the differential operators, and covariance functions associated to latent functions. In the classical formulation of latent force models, the covariance functions are obtained analytically by solving a double integral, leading to expressions that involve numerical solutions of different types of error functions. In consequence, the covariance matrix calculation is considerably expensive, because it requires the evaluation of one or more of these error functions. In this paper, we use random Fourier features to approximate the solution of these double integrals obtaining simpler analytical expressions for such covariance functions. We show experimental results using ordinary differential operators and provide an extension to build general kernel functions for convolved multiple output Gaussian processes.\"},\n",
       " {'topics': [(1, 0.072280586), (2, 0.07588256), (4, 0.8455522)],\n",
       "  'percentage': 0.12983012199401855,\n",
       "  'text': 'In this paper, we introduce the first principled adaptive-sampling procedure for learning a convex function in the $L_\\\\infty$ norm, a problem that arises often in the behavioral and social sciences. We present a function-specific measure of complexity and use it to prove that, for each convex function $f_{\\\\star}$, our algorithm nearly attains the information-theoretically optimal, function-specific error rate. We also corroborate our theoretical contributions with numerical experiments, finding that our method substantially outperforms passive, uniform sampling for favorable synthetic and data-derived functions in low-noise settings with large sampling budgets. Our results also suggest an idealized \"oracle strategy\", which we use to gauge the potential advance of any adaptive-sampling strategy over passive sampling, for any given convex function.'},\n",
       " {'topics': [(0, 0.13516293), (2, 0.7405239), (4, 0.11878457)],\n",
       "  'percentage': 0.12973344326019287,\n",
       "  'text': 'In this contribution we describe an approach to evolve composite covariance functions for Gaussian processes using genetic programming. A critical aspect of Gaussian processes and similar kernel-based models such as SVM is, that the covariance function should be adapted to the modeled data. Frequently, the squared exponential covariance function is used as a default. However, this can lead to a misspecified model, which does not fit the data well. In the proposed approach we use a grammar for the composition of covariance functions and genetic programming to search over the space of sentences that can be derived from the grammar. We tested the proposed approach on synthetic data from two-dimensional test functions, and on the Mauna Loa CO2 time series. The results show, that our approach is feasible, finding covariance functions that perform much better than a default covariance function. For the CO2 data set a composite covariance function is found, that matches the performance of a hand-tuned covariance function.'},\n",
       " {'topics': [(2, 0.3066402), (4, 0.68359596)],\n",
       "  'percentage': 0.12871918082237244,\n",
       "  'text': 'We propose an improved LASSO estimation technique based on Stein-rule. We shrink classical LASSO estimator using preliminary test, shrinkage, and positive-rule shrinkage principle. Simulation results have been carried out for various configurations of correlation coefficients ($r$), size of the parameter vector ($\\\\beta$), error variance ($\\\\sigma^2$) and number of non-zero coefficients ($k$) in the model parameter vector. Several real data examples have been used to demonstrate the practical usefulness of the proposed estimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$ Stein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly in the divergence parameter $\\\\Delta^2$ as in the traditional case.'},\n",
       " {'topics': [(1, 0.100783534), (3, 0.42506042), (4, 0.47069862)],\n",
       "  'percentage': 0.12846019864082336,\n",
       "  'text': 'Genome-wide association studies (GWASs) aim to detect genetic risk factors for complex human diseases by identifying disease-associated single-nucleotide polymorphisms (SNPs). SNP-wise approach, the standard method for analyzing GWAS, tests each SNP individually. Then the P-values are adjusted for multiple testing. Multiple testing adjustment (purely based on p-values) is over-conservative and causes lack of power in many GWASs, due to insufficiently modelling the relationship among SNPs. To address this problem, we propose a novel method, which borrows information across SNPs by grouping SNPs into three clusters. We pre-specify the patterns of clusters by minor allele frequencies of SNPs between cases and controls, and enforce the patterns with prior distributions. Therefore, compared with the traditional approach, it better controls false discovery rate (FDR) and shows higher sensitivity, which is confirmed by our simulation studies. We re-analyzed real data studies on identifying SNPs associated with severe bortezomib-induced peripheral neuropathy (BiPN) in patients with multiple myeloma. The original analysis in the literature failed to identify SNPs after FDR adjustment. Our proposed method not only detected the reported SNPs after FDR adjustment but also discovered a novel SNP rs4351714 that has been reported to be related to multiple myeloma in another study.'},\n",
       " {'topics': [(1, 0.19373415), (2, 0.7988715)],\n",
       "  'percentage': 0.12729819118976593,\n",
       "  'text': 'Due to the intractable partition function, the exact likelihood function for a Markov random field (MRF), in many situations, can only be approximated. Major approximation approaches include pseudolikelihood and Laplace approximation. In this paper, we propose a novel way of approximating the likelihood function through first approximating the marginal likelihood functions of individual parameters and then reconstructing the joint likelihood function from these marginal likelihood functions. For approximating the marginal likelihood functions, we derive a particular likelihood function from a modified scenario of coin tossing which is useful for capturing how one parameter interacts with the remaining parameters in the likelihood function. For reconstructing the joint likelihood function, we use an appropriate copula to link up these marginal likelihood functions. Numerical investigation suggests the superior performance of our approach. Especially as the size of the MRF increases, both the numerical performance and the computational cost of our approach remain consistently satisfactory, whereas Laplace approximation deteriorates and pseudolikelihood becomes computationally unbearable.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = get_similar_docs(myab, index, prep_text, bigrams, dictionary)\n",
    "\n",
    "display_similars(q, corp_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the ten most closely related docuements to our query abstract. \n",
    "\n",
    "### Further areas of investigation ###\n",
    "\n",
    "1. How exactly does the similarites finder work? Would it be better if I used a TF-IDF matrix?\n",
    "    - Are those really the most simlar documents? Wieht a percentage score of at most 16%? Worth investigating more criticall\n",
    "    - When tested with a paper from the corpus, the function predictably returns itself as it's perfect match. \n",
    "2. Would Grid-Searching over `eta` and `alpha` and other LDA parameters result in better performing models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
