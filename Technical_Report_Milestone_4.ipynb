{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello there! My capstone project has to do with the field of Natural Language Programming (NLP) known as topic modeling (TM). ##\n",
    "\n",
    "It involves extracting the key words that summarize _what_ exactly a document is about, especially within the context of a larger corpus of documents.\n",
    "\n",
    "To this end, I am using the `gensim` library which specializes in efficient implementations of TM algorithms. Specifically, I will be using Latent Dirichlet Allocation (LDA), a popular model that generates _t_ discriminative topics based on assumptions about the number of topics and their distributions throughout the corpus.\n",
    "\n",
    "The corpus I am working with is from the `stat.ML` category of the [arXiv](https://arxiv.org/list/stat.ML/recent) database. I have about 18,000 paper abstracts covering a range of Machine Learning topics such as Optimization, Neural Network Architecture, and Applications to medical data.\n",
    "\n",
    "### Problem : ### \n",
    "**Within the context of an overarching topic, how can we extract more detailed subtopics and provide similar documents/recommendations effectively? (Secondary Problem) Can we use Dynamic Topic Modeling to show the papers that have been most influential in their fields?**\n",
    "\n",
    "### Data: ###\n",
    "**I used the same data I collected from Milestone #2 (with function arx_and_recreation()). I isolate just the abstracts (no date for now) in a Pandas Series or list**\n",
    "\n",
    "### Pre-Processing : ###\n",
    "\n",
    "I use a combination of \n",
    "    - regular expressions : strip extraneous punctuation/formatting/etc. \n",
    "    - English stop words : strip useless connecting words\n",
    "    - lemmatizers : combine variations of words (tense, pluaral) into one\n",
    "    - phrasers : common words that go together (like bigrams) are combined into one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import re\n",
    "import json\n",
    "import gensim.parsing.preprocessing as genpre\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import CoherenceModel, LdaModel, LsiModel, HdpModel, TfidfModel\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processor:\n",
    "\n",
    "lmtzr= WordNetLemmatizer()\n",
    "def prep_text(text):\n",
    "     #this regex removes LATEX formatting, numbers, citations, splits hyphens into two words\n",
    "    myreg=r'\\\\[\\w]+[\\{| ]|\\$[^\\$]+\\$|\\(.+\\, *\\d{2,4}\\w*\\)|\\S*\\/\\/\\S*|[\\\\.,\\/#!$%\\^&\\*;:{}=_`\\'\\\"~()><\\|]|\\[.+\\]|\\d+|\\b\\w{1,2}\\b'\n",
    "    parsed_data = text.replace('-', ' ')\n",
    "    parsed_data = re.sub(myreg, '', parsed_data)\n",
    "    parsed_data = [lmtzr.lemmatize(w) for w in parsed_data.lower().split() if w not in genpre.STOPWORDS]\n",
    "    if len(parsed_data) ==1: return parsed_data[0]\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having transformed all my abstracts from things that looked like this:\n",
    "```\n",
    "In this article, we derive concentration inequalities for the cross-validation estimate of the generalization error for empirical risk minimizers. In the general setting, we prove sanity-check bounds in the spirit of \\cite{KR99} \\textquotedblleft\\textit{bounds showing that the worst-case error of this estimate is not much worse that of training error estimate} \\textquotedblright . General loss functions and class of predictors with finite VC-dimension are considered. We closely follow the formalism introduced by \\cite{DUD03} to cover a large variety of cross-validation procedures including leave-one-out cross-validation, $k$% -fold cross-validation, hold-out cross-validation (or split sample), and the leave-$\\upsilon$-out cross-validation.   In particular, we focus on proving the consistency of the various cross-validation procedures. We point out the interest of each cross-validation procedure in terms of rate of convergence. An estimation curve with transition phases depending on the cross-validation procedure and not only on the percentage of observations in the test sample gives a simple rule on how to choose the cross-validation. An interesting consequence is that the size of the test sample is not required to grow to infinity for the consistency of the cross-validation procedure.\n",
    "```\n",
    "to this (it is not perfect but the odd strings that get thru the cracks are too rare to be of any effect):\n",
    "```\n",
    "['article', 'derive', 'concentration', 'inequality', 'cross', 'validation', 'estimate', 'generalization', 'error', 'empirical', 'risk', 'minimizers', 'general', 'setting', 'prove', 'sanity', 'check', 'bound', 'spirit', 'kr', 'textquotedblleftbounds', 'showing', 'worst', 'case', 'error', 'estimate', 'worse', 'training', 'error', 'estimate', 'general', 'loss', 'function', 'class', 'predictor', 'finite', 'dimension', 'considered', 'closely', 'follow', 'formalism', 'introduced', 'dud', 'cover', 'large', 'variety', 'cross', 'validation', 'procedure', 'including', 'leave', 'cross', 'validation', 'fold', 'cross', 'validation', 'hold', 'cross', 'validation', 'split', 'sample', 'leave', 'cross', 'validation', 'particular', 'focus', 'proving', 'consistency', 'cross', 'validation', 'procedure', 'point', 'cross', 'validation', 'procedure', 'term', 'rate', 'convergence', 'estimation', 'curve', 'transition', 'phase', 'depending', 'cross', 'validation', 'procedure', 'percentage', 'observation', 'test', 'sample', 'give', 'simple', 'rule', 'choose', 'cross', 'validation', 'interesting', 'consequence', 'size', 'test', 'sample', 'required', 'grow', 'infinity', 'consistency', 'cross', 'validation', 'procedure']\n",
    "```\n",
    "\n",
    "I am now in good position to conduct a topic analysis of my pre-processed text. I would like to do some Exploratory Data Analysis, too, to see what the distribution of my words are, etc. But first, let's examine the key gensim object with which we will be working:\n",
    "\n",
    "    - Corpus: this is the list of list of tokens/n_grams (a list such as above for every abstract)\n",
    "    - Dictionary: this assigns every unique token an id. It is used to look up id->token and token->id\n",
    "    - Bag Of Words (BoW): sums the total occurances of a token in a document (word ordering is not considered)\n",
    "    - LdaModel: uses the dictionary and BOW to generate a probablity distribution of topics across docs, and of words across topics\n",
    "    - Market Matrix: a more efficient way of storing th corpus, useful when calculating similarities between documents. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up the relevant objects\n",
    "\n",
    "# The raw documents: \n",
    "abstracts = pd.read_csv('./new_hope_data/arxiv_csML.csv')['summary']\n",
    "\n",
    "with open('./the_data_strikes_back/bigrams', 'rb') as fp:\n",
    "    corpus = pickle.load(fp)\n",
    "\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "bow = [dictionary.doc2bow(text) for text in corpus]\n",
    "\n",
    "# The gensim phraser I'm using\n",
    "bigrams = gensim.utils.SaveLoad.load('./the_data_strikes_back/bigram_phrases')\n",
    "#define the lda model\n",
    "model = LdaModel(bow, id2word=dictionary, num_topics=5)\n",
    "\n",
    "# Matrix representation of my corpus\n",
    "# corpora.MmCorpus.serialize('./the_data_strikes_back/full_bigram_corpus.mm', corpus)\n",
    "corp_matrix = corpora.MmCorpus('./the_data_strikes_back/full_bigram_corpus.mm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1088"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import copy\n",
    "# q = copy.deepcopy(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I'm going to fit a model and visualize it now to make a point, later on I'll explain it in more detail**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"data\" + 0.012*\"method\" + 0.009*\"feature\" + 0.008*\"based\" + 0.007*\"proposed\" + 0.007*\"approach\" + 0.006*\"classification\" + 0.005*\"result\" + 0.005*\"algorithm\" + 0.005*\"signal\"'),\n",
       " (1,\n",
       "  '0.015*\"data\" + 0.011*\"method\" + 0.010*\"algorithm\" + 0.007*\"machine_learning\" + 0.006*\"based\" + 0.006*\"learning\" + 0.006*\"performance\" + 0.006*\"approach\" + 0.006*\"prediction\" + 0.006*\"model\"'),\n",
       " (2,\n",
       "  '0.030*\"method\" + 0.019*\"graph\" + 0.018*\"problem\" + 0.017*\"algorithm\" + 0.008*\"data\" + 0.008*\"based\" + 0.008*\"proposed\" + 0.007*\"approach\" + 0.007*\"optimization\" + 0.006*\"function\"'),\n",
       " (3,\n",
       "  '0.018*\"algorithm\" + 0.013*\"learning\" + 0.013*\"problem\" + 0.009*\"policy\" + 0.009*\"model\" + 0.008*\"agent\" + 0.007*\"based\" + 0.006*\"approach\" + 0.006*\"reinforcement_learning\" + 0.006*\"task\"'),\n",
       " (4,\n",
       "  '0.038*\"model\" + 0.021*\"network\" + 0.011*\"method\" + 0.010*\"data\" + 0.009*\"learning\" + 0.008*\"approach\" + 0.008*\"training\" + 0.008*\"task\" + 0.007*\"based\" + 0.006*\"representation\"'),\n",
       " (5,\n",
       "  '0.017*\"model\" + 0.014*\"data\" + 0.010*\"algorithm\" + 0.010*\"function\" + 0.010*\"distribution\" + 0.007*\"result\" + 0.007*\"parameter\" + 0.006*\"method\" + 0.006*\"sample\" + 0.006*\"problem\"')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's naively fit a model to this corpus, and see what happens\n",
    "full_lda = LdaModel(corpus=bow, num_topics=6, id2word=dictionary, alpha= 'auto', eta=.09)\n",
    "full_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is very high overlap between topics with a few generic ML terms taking the top spots. While there are discriminating terms \"beneath\" the most highly prevalent terms (you can see them by lowering $\\lambda$ on the slider below), I am inclined to filter out these common terms altogether. _Especially_ because as of now our corpus is only ML, so \"method\", \"model\", etc. won't help us. If we had different categories of articles, leaving these terms may be useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el55821401225507994803851409838\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el55821401225507994803851409838_data = {\"mdsDat\": {\"x\": [0.04319963315131378, 0.10670820389597672, -0.10412423870697586, -0.037186511252975375, 0.09186188939521113, -0.1004589764825506], \"y\": [0.05205032401853207, -0.089868414450792, 0.002116240975112082, -0.08394149385957253, 0.08558224091752333, 0.034061102399196984], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [13.913700103759766, 14.70833683013916, 18.489582061767578, 11.907635688781738, 23.139009475708008, 17.841733932495117]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [5543.0, 9507.0, 22239.0, 11081.0, 14962.0, 1781.0, 19338.0, 1466.0, 5656.0, 4629.0, 9530.0, 1159.0, 2359.0, 3420.0, 2131.0, 2791.0, 5865.0, 4902.0, 2500.0, 1883.0, 2876.0, 1504.0, 3710.0, 1912.0, 863.0, 1563.0, 1415.0, 1178.0, 5817.0, 15976.0, 128.2319793701172, 64.89403533935547, 52.63825988769531, 50.483646392822266, 55.58788299560547, 39.527381896972656, 39.947086334228516, 41.185577392578125, 34.7274284362793, 35.36539840698242, 78.03414154052734, 33.59824752807617, 33.96067428588867, 46.450347900390625, 32.96125030517578, 43.29863739013672, 31.410648345947266, 38.23227310180664, 49.401573181152344, 31.430471420288086, 55.0418701171875, 32.66789627075195, 27.6217098236084, 34.133914947509766, 56.571533203125, 151.7815704345703, 26.0045223236084, 29.81880760192871, 26.16819190979004, 26.770357131958008, 246.86329650878906, 59.31262969970703, 60.954872131347656, 35.190303802490234, 33.570030212402344, 142.86434936523438, 77.95082092285156, 38.905426025390625, 74.87297058105469, 46.25838851928711, 67.22162628173828, 52.57375717163086, 121.58982849121094, 63.92430114746094, 49.13812255859375, 504.5941162109375, 206.7431182861328, 159.55398559570312, 325.7262878417969, 87.48583221435547, 128.05609130859375, 68.02374267578125, 929.7334594726562, 322.6237487792969, 118.6745834350586, 317.4268798828125, 300.3804016113281, 607.7490234375, 432.7447204589844, 114.02501678466797, 108.96868896484375, 1792.56103515625, 250.4334259033203, 239.939697265625, 907.6334838867188, 223.2957763671875, 1136.8345947265625, 221.27655029296875, 126.98151397705078, 235.0684051513672, 764.1693725585938, 467.7301025390625, 247.81173706054688, 199.87974548339844, 309.3204345703125, 2572.625732421875, 688.728271484375, 1395.4732666015625, 367.7746276855469, 1715.1834716796875, 2545.318603515625, 1348.9271240234375, 681.8401489257812, 381.86639404296875, 767.6807250976562, 806.2830810546875, 757.3945922851562, 756.5341186523438, 1001.6276245117188, 750.3631591796875, 646.944580078125, 870.9227905273438, 697.66943359375, 675.0375366210938, 925.7928466796875, 920.77197265625, 659.6415405273438, 982.9112548828125, 700.8902587890625, 611.431640625, 826.0761108398438, 721.7850952148438, 100.23280334472656, 65.92169189453125, 52.211036682128906, 186.11419677734375, 53.80818557739258, 65.71054077148438, 60.61216735839844, 45.1380500793457, 47.77810287475586, 97.71431732177734, 66.5968017578125, 78.50499725341797, 57.996673583984375, 61.33190155029297, 37.4259033203125, 43.434730529785156, 84.23175048828125, 39.536773681640625, 36.43149948120117, 36.77108383178711, 32.899009704589844, 80.15191650390625, 31.084047317504883, 31.04313087463379, 29.840749740600586, 71.38069915771484, 28.050180435180664, 30.338010787963867, 32.66205978393555, 60.118526458740234, 173.59434509277344, 67.05359649658203, 96.96942138671875, 52.4569091796875, 310.3210754394531, 79.25247192382812, 53.4019889831543, 276.0527648925781, 179.14219665527344, 120.63451385498047, 160.38275146484375, 326.5806579589844, 184.5854949951172, 155.53395080566406, 128.27310180664062, 235.93699645996094, 180.74063110351562, 194.85797119140625, 152.6767120361328, 78.95097351074219, 294.05389404296875, 268.54693603515625, 365.6320495605469, 634.5753173828125, 307.7798767089844, 756.002197265625, 157.9295654296875, 142.9668731689453, 244.77899169921875, 249.14352416992188, 139.3880157470703, 169.25405883789062, 847.060791015625, 290.50848388671875, 1050.89453125, 467.7003479003906, 1552.3856201171875, 1339.3975830078125, 495.91937255859375, 583.7329711914062, 762.799072265625, 474.5809326171875, 3293.340576171875, 346.3487854003906, 375.6094665527344, 1397.375, 2275.530029296875, 1085.7640380859375, 416.1506652832031, 2287.53369140625, 1398.7476806640625, 1413.4595947265625, 1366.5859375, 853.9907836914062, 729.6763916015625, 718.6737670898438, 522.7847290039062, 506.75372314453125, 908.8553466796875, 697.9048461914062, 743.5794067382812, 851.0607299804688, 1229.203857421875, 701.8814086914062, 663.5692749023438, 865.5979614257812, 653.5233154296875, 634.45068359375, 686.3892822265625, 632.3726196289062, 98.34278106689453, 142.72073364257812, 76.32746887207031, 75.05987548828125, 67.59877014160156, 76.51959991455078, 61.738929748535156, 120.02967071533203, 98.93492126464844, 111.54983520507812, 52.96747970581055, 49.14564895629883, 52.87152862548828, 49.46220779418945, 54.30241012573242, 105.80132293701172, 44.90138626098633, 50.80436325073242, 43.82632064819336, 94.69415283203125, 41.44285202026367, 106.32077026367188, 37.38484573364258, 37.583744049072266, 54.24381637573242, 41.9827880859375, 39.037471771240234, 39.6032600402832, 41.05229568481445, 35.75291442871094, 259.0390319824219, 199.44482421875, 89.7316665649414, 58.96728515625, 216.62913513183594, 44.69093322753906, 5156.34033203125, 387.367431640625, 313.21099853515625, 227.2844696044922, 109.2278060913086, 655.2671508789062, 180.96835327148438, 85.88240051269531, 71.73746490478516, 144.08395385742188, 158.8286895751953, 115.5251235961914, 76.34614562988281, 173.32530212402344, 100.80805969238281, 139.43882751464844, 695.0287475585938, 168.986083984375, 129.62225341796875, 958.413330078125, 131.62100219726562, 525.4422607421875, 1826.495849609375, 209.27906799316406, 662.2385864257812, 477.5433044433594, 191.8724822998047, 543.3770141601562, 1465.6759033203125, 8187.04052734375, 4844.79638671875, 667.2296142578125, 1385.2230224609375, 320.995361328125, 1082.0777587890625, 553.8244018554688, 537.08447265625, 1261.860595703125, 990.7418212890625, 371.1484069824219, 409.1424865722656, 4612.8544921875, 410.3484802246094, 657.9224853515625, 2092.64599609375, 916.2650756835938, 915.1280517578125, 789.3527221679688, 2133.642822265625, 1514.819091796875, 1984.4541015625, 2181.056884765625, 1388.0721435546875, 978.00537109375, 1049.2491455078125, 964.85107421875, 1306.790283203125, 836.1656494140625, 1053.6883544921875, 882.7869262695312, 1263.27978515625, 870.05029296875, 896.03857421875, 852.2925415039062, 252.17430114746094, 119.67052459716797, 193.80862426757812, 70.98715209960938, 74.14453125, 65.62041473388672, 49.07678985595703, 76.35208892822266, 63.2938232421875, 49.175968170166016, 50.85127258300781, 53.30573654174805, 40.36883544921875, 46.31476593017578, 36.752811431884766, 36.833282470703125, 33.97935104370117, 41.1578369140625, 36.779136657714844, 34.12876892089844, 31.714263916015625, 44.13071823120117, 31.408658981323242, 30.403900146484375, 29.635892868041992, 38.47937774658203, 28.977609634399414, 26.553359985351562, 26.4418888092041, 28.83319091796875, 1410.935546875, 51.14743423461914, 64.36160278320312, 47.27873229980469, 1638.285888671875, 157.61521911621094, 95.9766845703125, 1054.9993896484375, 143.48265075683594, 64.70246124267578, 183.67442321777344, 74.8777084350586, 124.07756042480469, 144.81227111816406, 307.47802734375, 698.6703491210938, 412.66363525390625, 439.1603088378906, 621.5043334960938, 93.21908569335938, 124.86752319335938, 97.95144653320312, 76.48308563232422, 728.4176635742188, 215.76988220214844, 375.5121154785156, 351.12744140625, 374.0006408691406, 179.4984130859375, 206.40875244140625, 150.59938049316406, 206.50509643554688, 308.8837585449219, 3241.321044921875, 2346.810302734375, 2325.3017578125, 635.5299072265625, 392.884033203125, 829.5585327148438, 708.8064575195312, 677.4005737304688, 1038.2052001953125, 569.39990234375, 980.89501953125, 567.1571044921875, 389.9486083984375, 1139.9560546875, 1148.2999267578125, 1563.6593017578125, 697.8414916992188, 695.3864135742188, 774.4874267578125, 695.3734741210938, 624.2897338867188, 663.8555297851562, 777.6366577148438, 699.0309448242188, 506.44427490234375, 496.0985107421875, 502.0191345214844, 187.93527221679688, 200.3683624267578, 541.7178955078125, 170.634765625, 92.92183685302734, 111.85848999023438, 548.0781860351562, 338.5955810546875, 74.60842895507812, 79.99994659423828, 61.79426956176758, 59.24312973022461, 59.19674301147461, 157.77565002441406, 52.32790756225586, 119.22811126708984, 56.697288513183594, 44.757389068603516, 62.90351104736328, 44.78406524658203, 41.77592849731445, 92.41756439208984, 46.832481384277344, 70.80461120605469, 37.52378463745117, 37.73036575317383, 52.0546989440918, 34.253509521484375, 43.248451232910156, 121.73768615722656, 314.29638671875, 247.7114715576172, 119.17205810546875, 99.00890350341797, 443.6490478515625, 156.00973510742188, 82.34888458251953, 355.0932922363281, 1387.2576904296875, 558.476318359375, 152.7372589111328, 534.4949340820312, 513.532470703125, 400.0355529785156, 307.2928466796875, 159.99005126953125, 252.97434997558594, 586.9552001953125, 7340.37548828125, 1957.385498046875, 1552.98486328125, 392.09765625, 1387.8177490234375, 400.0498962402344, 478.0304260253906, 12950.7958984375, 463.62249755859375, 549.9157104492188, 1309.291259765625, 1293.99365234375, 2090.677734375, 1574.8193359375, 1876.25830078125, 601.5606079101562, 1964.8289794921875, 2804.75439453125, 2802.3974609375, 1214.7542724609375, 3139.109619140625, 800.0831909179688, 848.7208251953125, 2849.74951171875, 1435.18359375, 3535.0078125, 3897.92138671875, 2474.571533203125, 1131.3265380859375, 1468.48974609375, 1514.506591796875, 885.5186157226562, 1052.9248046875, 1220.349853515625, 1316.384521484375, 1122.0458984375, 1321.9359130859375, 1171.144775390625, 1183.388916015625, 1130.06396484375, 1137.9090576171875, 232.88331604003906, 213.86135864257812, 90.16050720214844, 154.04266357421875, 79.3114242553711, 71.6286849975586, 73.69667053222656, 69.06968688964844, 78.29846954345703, 118.43425750732422, 62.740482330322266, 70.32577514648438, 66.7521743774414, 80.61894226074219, 54.17850875854492, 54.42407989501953, 59.03786849975586, 53.34498977661133, 48.98834991455078, 56.925567626953125, 44.56489562988281, 44.5284309387207, 59.04058074951172, 50.679500579833984, 40.7702522277832, 46.464813232421875, 41.822776794433594, 90.65106201171875, 37.800811767578125, 37.17991638183594, 88.98856353759766, 141.17318725585938, 54.169891357421875, 51.879756927490234, 204.10047912597656, 55.837745666503906, 110.7025375366211, 90.61676025390625, 64.0934066772461, 62.21805191040039, 557.557861328125, 84.24612426757812, 1395.8328857421875, 138.33201599121094, 67.98290252685547, 731.0369262695312, 992.1184692382812, 1143.259521484375, 328.34661865234375, 254.4978485107422, 292.77801513671875, 2531.726318359375, 545.0632934570312, 444.6414794921875, 182.15167236328125, 530.3587646484375, 275.9054260253906, 2670.208984375, 219.1478271484375, 1863.6260986328125, 588.896728515625, 358.0568542480469, 1119.350830078125, 1158.278076171875, 1164.01123046875, 837.91357421875, 1607.6151123046875, 916.7989501953125, 1390.812744140625, 813.5283813476562, 569.7178955078125, 3695.272705078125, 4406.60302734375, 714.093994140625, 900.6280517578125, 1869.0506591796875, 2711.5029296875, 598.8226318359375, 932.9110107421875, 990.8901977539062, 691.6397705078125, 949.2089233398438, 1467.3067626953125, 830.407470703125, 1642.7767333984375, 1253.2183837890625, 918.4194946289062, 1225.0281982421875, 900.3109741210938, 923.014892578125, 830.1292724609375], \"Term\": [\"graph\", \"network\", \"model\", \"problem\", \"algorithm\", \"policy\", \"method\", \"agent\", \"function\", \"distribution\", \"learning\", \"reinforcement_learning\", \"deep_learning\", \"neural_network\", \"classifier\", \"optimization\", \"task\", \"feature\", \"matrix\", \"deep\", \"image\", \"layer\", \"machine_learning\", \"bound\", \"reward\", \"user\", \"signal\", \"environment\", \"training\", \"data\", \"programming_language\", \"circuit\", \"sampling_strategy\", \"mobility\", \"capsule\", \"id\", \"metadata\", \"inception\", \"aim_learn\", \"transfer_knowledge\", \"urban\", \"stabilize\", \"fixed_length\", \"linearized\", \"radiation\", \"journey\", \"neutral\", \"prognostic\", \"knockoff\", \"video_game\", \"mental\", \"conditional_expectation\", \"bridging\", \"constellation\", \"cardiac\", \"eeg\", \"look_like\", \"extract_useful\", \"card\", \"finger\", \"quantization\", \"energy_consumption\", \"vital\", \"saturation\", \"pitfall\", \"molecular\", \"mtl\", \"aka\", \"fraud\", \"curriculum_learning\", \"velocity\", \"indoor\", \"pathway\", \"high_confidence\", \"voltage\", \"adversarial_example\", \"gene\", \"health\", \"brain\", \"water\", \"molecule\", \"signature\", \"signal\", \"channel\", \"quantification\", \"sensor\", \"filter\", \"pattern\", \"cnn\", \"invariance\", \"mobile\", \"feature\", \"disease\", \"device\", \"test\", \"cnns\", \"classification\", \"clinical\", \"fusion\", \"dictionary\", \"classifier\", \"group\", \"subject\", \"activity\", \"spatial\", \"data\", \"kernel\", \"proposed\", \"measurement\", \"based\", \"method\", \"approach\", \"accuracy\", \"detection\", \"set\", \"sample\", \"machine_learning\", \"analysis\", \"result\", \"study\", \"image\", \"task\", \"paper\", \"information\", \"learning\", \"network\", \"different\", \"algorithm\", \"performance\", \"application\", \"model\", \"problem\", \"company\", \"cross_domain\", \"architectural\", \"skill\", \"curriculum\", \"tweet\", \"white_box\", \"automation\", \"sigmoid\", \"source_target\", \"client\", \"city\", \"bart\", \"auction\", \"provider\", \"rotation_forest\", \"malware\", \"privacy_guarantee\", \"wordvec\", \"cyber_attack\", \"thread\", \"softmax\", \"relative_importance\", \"attentive\", \"sketched\", \"privacy_preserving\", \"invalid\", \"browsing\", \"feed\", \"api\", \"customer\", \"revenue\", \"smart\", \"advertising\", \"fairness\", \"gradient_boosting\", \"workload\", \"privacy\", \"imagenet\", \"market\", \"target_domain\", \"adversary\", \"momentum\", \"age\", \"attention_mechanism\", \"random_forest\", \"service\", \"domain_adaptation\", \"library\", \"participant\", \"dnns\", \"platform\", \"end_end\", \"ensemble\", \"dnn\", \"online\", \"price\", \"decision_tree\", \"meta\", \"player\", \"worker\", \"private\", \"user\", \"expert\", \"classifier\", \"memory\", \"machine_learning\", \"prediction\", \"distributed\", \"human\", \"system\", \"adversarial\", \"data\", \"query\", \"patient\", \"performance\", \"algorithm\", \"time\", \"decision\", \"method\", \"learning\", \"based\", \"approach\", \"different\", \"accuracy\", \"datasets\", \"label\", \"strategy\", \"training\", \"classification\", \"information\", \"result\", \"model\", \"study\", \"application\", \"problem\", \"work\", \"use\", \"task\", \"new\", \"transportation\", \"critic\", \"variational_auto\", \"wrong\", \"breakthrough\", \"lagrangian\", \"representational\", \"multitask\", \"irl\", \"saddle_point\", \"attributed\", \"mcts\", \"graph_topology\", \"road_segment\", \"hamming_distance\", \"nonconvex_optimization\", \"detrimental\", \"nested_dichotomy\", \"pruned\", \"encoders\", \"pascal_voc\", \"primal_dual\", \"posted\", \"valley\", \"verify_effectiveness\", \"concatenation\", \"discontinuous\", \"continuous_relaxation\", \"strain\", \"spn\", \"admm\", \"linear_convergence\", \"differential_equation\", \"random_fourier\", \"adam\", \"sinkhorn\", \"graph\", \"vertex\", \"nonconvex\", \"spectral_clustering\", \"semantic_segmentation\", \"non_convex\", \"proximal\", \"subgraph\", \"code_available\", \"alternating_direction\", \"stationary_point\", \"hypergraph\", \"gnn\", \"svd\", \"graph_laplacian\", \"gnns\", \"convex\", \"strongly_convex\", \"method_multiplier\", \"convergence\", \"non_smooth\", \"optimization_problem\", \"optimization\", \"community_detection\", \"solving\", \"objective_function\", \"solver\", \"iteration\", \"gradient\", \"method\", \"problem\", \"stochastic_gradient\", \"solution\", \"penalty\", \"node\", \"gradient_descent\", \"low_rank\", \"matrix\", \"clustering\", \"descent\", \"convergence_rate\", \"algorithm\", \"edge\", \"regularization\", \"proposed\", \"sparse\", \"approximation\", \"stochastic\", \"based\", \"function\", \"approach\", \"data\", \"result\", \"structure\", \"propose\", \"analysis\", \"learning\", \"existing\", \"performance\", \"application\", \"model\", \"set\", \"framework\", \"paper\", \"activation_function\", \"imitation_learning\", \"reward_function\", \"cold_start\", \"continuous_control\", \"agent_learn\", \"artist\", \"experience_replay\", \"exposure\", \"differentiate\", \"tree_search\", \"hypergraphs\", \"transferable\", \"deterministic_policy\", \"curiosity\", \"poi\", \"extracted_feature\", \"grasp\", \"winning\", \"specifically_designed\", \"incur\", \"realize\", \"interaction_environment\", \"visible\", \"eeg_signal\", \"query_complexity\", \"constrains\", \"context_aware\", \"traffic_flow\", \"defensive\", \"agent\", \"maze\", \"catastrophic_forgetting\", \"resilient\", \"policy\", \"policy_gradient\", \"dqn\", \"reinforcement_learning\", \"multi_agent\", \"inverse_reinforcement\", \"recommender_system\", \"car\", \"user_item\", \"safety\", \"deep_reinforcement\", \"reward\", \"tensor\", \"item\", \"action\", \"mdp\", \"rating\", \"road\", \"personalized\", \"environment\", \"planning\", \"recommendation\", \"recurrent_neural\", \"exploration\", \"quantum\", \"hardware\", \"demonstration\", \"bandit\", \"game\", \"algorithm\", \"learning\", \"problem\", \"state\", \"goal\", \"neural_network\", \"setting\", \"matrix\", \"task\", \"value\", \"training\", \"kernel\", \"control\", \"approach\", \"based\", \"model\", \"set\", \"framework\", \"result\", \"function\", \"propose\", \"performance\", \"method\", \"data\", \"study\", \"work\", \"proposed\", \"attacker\", \"deep_generative\", \"gan\", \"network_gans\", \"token\", \"sequence_sequence\", \"generative_adversarial\", \"gans\", \"app\", \"mnist_cifar\", \"finite_sum\", \"feedforward_neural\", \"abnormal\", \"vaes\", \"macro\", \"variational_autoencoders\", \"united_state\", \"saliency\", \"intermediate_representation\", \"able_generate\", \"public_health\", \"image_generation\", \"meta_path\", \"action_space\", \"fragment\", \"stack_overflow\", \"downstream_task\", \"bidirectional\", \"generator_discriminator\", \"latent_representation\", \"vae\", \"discriminator\", \"variational_autoencoder\", \"convolutional_layer\", \"generator\", \"music\", \"sentiment\", \"semantic\", \"layer\", \"generative_model\", \"speaker\", \"topic\", \"word\", \"generative\", \"document\", \"shallow\", \"sentence\", \"text\", \"network\", \"deep_learning\", \"deep\", \"lstm\", \"architecture\", \"visual\", \"latent_variable\", \"model\", \"language\", \"embeddings\", \"deep_neural\", \"trained\", \"representation\", \"inference\", \"image\", \"latent\", \"neural_network\", \"training\", \"task\", \"learn\", \"learning\", \"train\", \"learned\", \"approach\", \"structure\", \"data\", \"method\", \"based\", \"input\", \"framework\", \"feature\", \"complex\", \"state_art\", \"different\", \"distribution\", \"use\", \"proposed\", \"new\", \"performance\", \"propose\", \"algorithm\", \"optimal_transport\", \"wasserstein_distance\", \"custom\", \"concept_drift\", \"able_capture\", \"intrusion\", \"authentication\", \"shifting\", \"ann\", \"struggle\", \"command\", \"cps\", \"convex_concave\", \"coded\", \"carefully_designed\", \"d_d\", \"nystr_method\", \"gas\", \"subjected\", \"differentiability\", \"semidefinite_programming\", \"tracker\", \"confusion_matrix\", \"oja\", \"non_stationarity\", \"importance_weighted\", \"hint\", \"time_horizon\", \"exceptional\", \"intrusion_detection\", \"title\", \"engine\", \"sum_square\", \"sda\", \"batch_size\", \"mixing_time\", \"pde\", \"collective\", \"fully_automated\", \"geyser\", \"subspace\", \"warping\", \"bound\", \"lipschitz\", \"acceptance\", \"gaussian_process\", \"dimension\", \"estimator\", \"markov_chain\", \"moment\", \"covariance\", \"distribution\", \"mixture\", \"lower_bound\", \"mixing\", \"variance\", \"asymptotic\", \"function\", \"finite_sample\", \"parameter\", \"gaussian\", \"monte_carlo\", \"error\", \"estimate\", \"point\", \"high_dimensional\", \"sample\", \"regression\", \"number\", \"noise\", \"unknown\", \"data\", \"model\", \"sampling\", \"estimation\", \"result\", \"algorithm\", \"assumption\", \"case\", \"space\", \"prior\", \"class\", \"problem\", \"setting\", \"method\", \"approach\", \"study\", \"based\", \"analysis\", \"time\", \"new\"], \"Total\": [5543.0, 9507.0, 22239.0, 11081.0, 14962.0, 1781.0, 19338.0, 1466.0, 5656.0, 4629.0, 9530.0, 1159.0, 2359.0, 3420.0, 2131.0, 2791.0, 5865.0, 4902.0, 2500.0, 1883.0, 2876.0, 1504.0, 3710.0, 1912.0, 863.0, 1563.0, 1415.0, 1178.0, 5817.0, 15976.0, 128.69256591796875, 65.35469055175781, 53.09877395629883, 50.956695556640625, 56.1506233215332, 39.98797607421875, 40.421348571777344, 41.729976654052734, 35.188323974609375, 35.83909606933594, 79.10140228271484, 34.05988311767578, 34.437076568603516, 47.102691650390625, 33.439109802246094, 43.929039001464844, 31.87110710144043, 38.80027770996094, 50.15544128417969, 31.91860580444336, 55.92555618286133, 33.20392990112305, 28.082496643066406, 34.70899963378906, 57.53059005737305, 154.4657745361328, 26.465198516845703, 30.34811782836914, 26.635480880737305, 27.250022888183594, 255.8046417236328, 60.658756256103516, 62.379947662353516, 35.82279586791992, 34.180564880371094, 147.81692504882812, 80.17878723144531, 39.70050048828125, 78.19722747802734, 47.50230407714844, 70.15292358398438, 54.40289306640625, 131.59463500976562, 66.88297271728516, 50.719547271728516, 592.7727661132812, 233.96437072753906, 177.63819885253906, 384.05419921875, 94.25057983398438, 143.26937866210938, 72.38504028320312, 1415.40625, 427.7513122558594, 142.886474609375, 466.1687927246094, 442.9410400390625, 1047.5584716796875, 708.9727783203125, 141.22146606445312, 133.25424194335938, 4902.892578125, 388.1455993652344, 371.3601989746094, 2154.373046875, 343.04156494140625, 3089.08740234375, 350.4543762207031, 163.95359802246094, 386.9468078613281, 2131.133056640625, 1072.9005126953125, 439.9879455566406, 321.6091003417969, 618.2518920898438, 15976.3349609375, 2107.69580078125, 6449.35888671875, 815.0765380859375, 10110.185546875, 19338.228515625, 9942.8916015625, 2921.93408203125, 972.0888671875, 3747.7568359375, 4143.5498046875, 3710.638427734375, 3756.220703125, 6864.3203125, 3865.41015625, 2876.515625, 5865.64404296875, 3738.476806640625, 3705.78076171875, 9530.4423828125, 9507.970703125, 3830.99560546875, 14962.029296875, 5614.33544921875, 3627.378173828125, 22239.619140625, 11081.1015625, 100.70551300048828, 66.51693725585938, 52.687744140625, 187.95269775390625, 54.344566345214844, 66.40898132324219, 61.2655143737793, 45.665287017822266, 48.33806228637695, 98.87647247314453, 67.3995361328125, 79.49109649658203, 58.7314338684082, 62.111236572265625, 37.9152717590332, 44.02425003051758, 85.38227844238281, 40.086639404296875, 36.94198989868164, 37.29362487792969, 33.37406921386719, 81.32438659667969, 31.556724548339844, 31.52499771118164, 30.334854125976562, 72.59529113769531, 28.532499313354492, 30.864017486572266, 33.2332878112793, 61.1827278137207, 176.78396606445312, 68.30686950683594, 99.41100311279297, 53.54954147338867, 326.517578125, 81.54825592041016, 54.70713806152344, 295.1504211425781, 189.7129364013672, 126.34890747070312, 170.60513305664062, 363.80010986328125, 199.54811096191406, 168.13710021972656, 137.41969299316406, 266.39483642578125, 200.40966796875, 218.40321350097656, 170.63621520996094, 82.70909118652344, 361.14984130859375, 329.7080383300781, 469.5596008300781, 899.7837524414062, 397.3035888671875, 1134.412841796875, 184.1112060546875, 164.2308807373047, 315.9237060546875, 323.4299621582031, 160.8046417236328, 204.69424438476562, 1563.11572265625, 407.9767761230469, 2131.133056640625, 764.6629638671875, 3710.638427734375, 3151.0107421875, 901.361083984375, 1146.073974609375, 1715.1038818359375, 874.6300659179688, 15976.3349609375, 578.0314331054688, 658.4911499023438, 5614.33544921875, 14962.029296875, 4321.908203125, 829.5424194335938, 19338.228515625, 9530.4423828125, 10110.185546875, 9942.8916015625, 3830.99560546875, 2921.93408203125, 2903.112060546875, 1432.5911865234375, 1360.721435546875, 5817.93896484375, 3089.08740234375, 3705.78076171875, 6864.3203125, 22239.619140625, 3865.41015625, 3627.378173828125, 11081.1015625, 3880.669921875, 3666.35400390625, 5865.64404296875, 4283.38720703125, 98.76512908935547, 143.38671875, 76.75761413574219, 75.48782348632812, 68.0205307006836, 77.036376953125, 62.16584777832031, 120.90632629394531, 99.69738006591797, 112.42032623291016, 53.397605895996094, 49.56782913208008, 53.32944107055664, 49.8907470703125, 54.79171371459961, 106.79730987548828, 45.325321197509766, 51.285133361816406, 44.25620651245117, 95.63190460205078, 41.87221908569336, 107.50003814697266, 37.80677032470703, 38.00940704345703, 54.86077880859375, 42.47166061401367, 39.49348449707031, 40.08061981201172, 41.55296325683594, 36.193328857421875, 264.1212158203125, 203.51170349121094, 90.88371276855469, 59.70751953125, 222.4917755126953, 45.24492645263672, 5543.33544921875, 401.8806457519531, 324.8979797363281, 236.46368408203125, 112.57241821289062, 714.42431640625, 189.7237548828125, 88.02494049072266, 73.17205810546875, 150.63473510742188, 167.33815002441406, 120.17163848876953, 78.12580108642578, 184.81680297851562, 104.46941375732422, 147.1038818359375, 818.7119750976562, 181.20025634765625, 137.48776245117188, 1306.4290771484375, 141.18106079101562, 676.4234619140625, 2791.00048828125, 240.01177978515625, 924.7202758789062, 633.6243286132812, 217.52499389648438, 742.1341552734375, 2453.10400390625, 19338.228515625, 11081.1015625, 993.69287109375, 2448.13720703125, 409.15570068359375, 1837.7987060546875, 832.98046875, 810.4024658203125, 2500.086669921875, 1833.1734619140625, 510.5023193359375, 582.7732543945312, 14962.029296875, 591.3598022460938, 1146.3367919921875, 6449.35888671875, 1985.90478515625, 2101.1083984375, 1702.787353515625, 10110.185546875, 5656.76171875, 9942.8916015625, 15976.3349609375, 6864.3203125, 3310.362548828125, 4335.13427734375, 3756.220703125, 9530.4423828125, 2734.363525390625, 5614.33544921875, 3627.378173828125, 22239.619140625, 3747.7568359375, 4460.1494140625, 3738.476806640625, 252.8091278076172, 120.53419494628906, 195.3992156982422, 71.58633422851562, 74.7777328491211, 66.23770904541016, 49.566932678222656, 77.13985443115234, 63.95969009399414, 49.700138092041016, 51.40192794799805, 53.91015625, 40.83833312988281, 46.866912841796875, 37.22475814819336, 37.33930206298828, 34.448822021484375, 41.72896194458008, 37.29241180419922, 34.63288497924805, 32.18367385864258, 44.797115325927734, 31.892959594726562, 30.87330436706543, 30.105323791503906, 39.09489440917969, 29.471981048583984, 27.02646827697754, 26.9190616607666, 29.35483169555664, 1466.382568359375, 52.14019775390625, 65.86015319824219, 48.23622512817383, 1781.0860595703125, 164.88204956054688, 99.40287017822266, 1159.47705078125, 150.99305725097656, 66.67445373535156, 197.7719268798828, 77.7398452758789, 131.76356506347656, 155.1168670654297, 340.1339416503906, 863.0355834960938, 491.9739990234375, 532.4620971679688, 834.28369140625, 103.15200805664062, 144.56947326660156, 110.01092529296875, 82.61355590820312, 1178.637451171875, 280.0551452636719, 548.1978759765625, 524.7315673828125, 576.5948486328125, 233.7821044921875, 285.5854797363281, 192.80291748046875, 305.9446716308594, 532.6083374023438, 14962.029296875, 9530.4423828125, 11081.1015625, 1721.728271484375, 878.6112060546875, 3420.185302734375, 2705.272216796875, 2500.086669921875, 5865.64404296875, 1949.76708984375, 5817.93896484375, 2107.69580078125, 1063.62060546875, 9942.8916015625, 10110.185546875, 22239.619140625, 3747.7568359375, 4460.1494140625, 6864.3203125, 5656.76171875, 4335.13427734375, 5614.33544921875, 19338.228515625, 15976.3349609375, 3865.41015625, 3880.669921875, 6449.35888671875, 188.42747497558594, 201.28404235839844, 544.2803344726562, 171.44593811035156, 93.40912628173828, 112.45681762695312, 551.057373046875, 340.5740661621094, 75.15058898925781, 80.60000610351562, 62.26253128051758, 59.71092224121094, 59.66461944580078, 159.1162109375, 52.79641342163086, 120.29813385009766, 57.242164611816406, 45.235355377197266, 63.57831954956055, 45.27362823486328, 42.243778228759766, 93.4567642211914, 47.39012145996094, 71.7317123413086, 38.017677307128906, 38.2374267578125, 52.76039505004883, 34.72493362426758, 43.84748840332031, 123.4384536743164, 318.9546813964844, 252.82191467285156, 120.95106506347656, 101.04971313476562, 460.8119812011719, 160.5592803955078, 84.14054107666016, 373.936767578125, 1504.5283203125, 596.8012084960938, 158.59425354003906, 580.3543701171875, 557.8170776367188, 432.126220703125, 328.5753479003906, 166.7971954345703, 269.4752197265625, 652.0538940429688, 9507.970703125, 2359.95751953125, 1883.7808837890625, 442.88751220703125, 1822.8231201171875, 459.564697265625, 563.5494995117188, 22239.619140625, 546.6552124023438, 664.9613647460938, 1794.04052734375, 1779.5240478515625, 3168.810791015625, 2287.80078125, 2876.515625, 775.7498168945312, 3420.185302734375, 5817.93896484375, 5865.64404296875, 2119.28759765625, 9530.4423828125, 1275.76953125, 1403.4541015625, 9942.8916015625, 3310.362548828125, 15976.3349609375, 19338.228515625, 10110.185546875, 2644.36328125, 4460.1494140625, 4902.892578125, 1685.254150390625, 2544.23388671875, 3830.99560546875, 4629.4873046875, 3666.35400390625, 6449.35888671875, 4283.38720703125, 5614.33544921875, 4335.13427734375, 14962.029296875, 233.31265258789062, 214.50299072265625, 90.57340240478516, 154.7877655029297, 79.73321533203125, 72.0416030883789, 74.13499450683594, 69.48289489746094, 78.77046966552734, 119.16984558105469, 63.153438568115234, 70.80308532714844, 67.2126235961914, 81.22228240966797, 54.591522216796875, 54.8737907409668, 59.53033447265625, 53.80808639526367, 49.415184020996094, 57.449134826660156, 44.977779388427734, 44.94144058227539, 59.59698486328125, 51.188472747802734, 41.183815002441406, 46.948299407958984, 42.262508392333984, 91.60603332519531, 38.21677017211914, 37.59294128417969, 90.02557373046875, 143.7813720703125, 54.82163619995117, 52.483890533447266, 213.581298828125, 56.7041015625, 114.82657623291016, 93.69865417480469, 65.3963851928711, 63.52833938598633, 654.1312255859375, 87.7837905883789, 1912.73046875, 152.18153381347656, 70.04620361328125, 966.7924194335938, 1391.3604736328125, 1691.8331298828125, 412.91326904296875, 309.9297790527344, 368.8712158203125, 4629.4873046875, 782.00439453125, 627.112548828125, 218.6397247314453, 781.7214965820312, 360.76715087890625, 5656.76171875, 276.502197265625, 4048.93603515625, 979.8529663085938, 527.7750244140625, 2254.961181640625, 2403.36865234375, 2522.775390625, 1671.668212890625, 4143.5498046875, 1977.349609375, 3562.100830078125, 1730.80908203125, 1048.93505859375, 15976.3349609375, 22239.619140625, 1480.997314453125, 2162.36474609375, 6864.3203125, 14962.029296875, 1233.335205078125, 2747.542236328125, 3074.60498046875, 1651.046630859375, 3321.08154296875, 11081.1015625, 2705.272216796875, 19338.228515625, 9942.8916015625, 3865.41015625, 10110.185546875, 3756.220703125, 4321.908203125, 4283.38720703125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.9687000513076782, 1.9651999473571777, 1.9636000394821167, 1.9630000591278076, 1.9622000455856323, 1.9607000350952148, 1.9605000019073486, 1.9592000246047974, 1.9591000080108643, 1.9589999914169312, 1.9586999416351318, 1.9586000442504883, 1.958400011062622, 1.958400011062622, 1.957900047302246, 1.957800030708313, 1.9577000141143799, 1.9574999809265137, 1.957200050354004, 1.9569000005722046, 1.9564000368118286, 1.9559999704360962, 1.9558000564575195, 1.9556000232696533, 1.9555000066757202, 1.954800009727478, 1.954699993133545, 1.954699993133545, 1.9545999765396118, 1.9544999599456787, 1.9366999864578247, 1.9499000310897827, 1.9492000341415405, 1.9544999599456787, 1.954300045967102, 1.9381999969482422, 1.944100022315979, 1.9521000385284424, 1.9289000034332275, 1.9457999467849731, 1.9296000003814697, 1.938099980354309, 1.8932000398635864, 1.9270999431610107, 1.940600037574768, 1.8112000226974487, 1.8486000299453735, 1.86489999294281, 1.8076000213623047, 1.8977999687194824, 1.8600000143051147, 1.9101999998092651, 1.5520000457763672, 1.6901999711990356, 1.7865999937057495, 1.5880000591278076, 1.583899974822998, 1.4278000593185425, 1.478600025177002, 1.7583999633789062, 1.7711000442504883, 0.9660999774932861, 1.53410005569458, 1.5355000495910645, 1.1079000234603882, 1.5428999662399292, 0.9726999998092651, 1.5125000476837158, 1.7167999744415283, 1.4738999605178833, 0.9466999769210815, 1.1420999765396118, 1.3982000350952148, 1.4967000484466553, 1.2798000574111938, 0.1460999995470047, 0.8537999987602234, 0.4415000081062317, 1.1764999628067017, 0.19830000400543213, -0.05550000071525574, -0.025299999862909317, 0.5170999765396118, 1.0378999710083008, 0.38679999113082886, 0.3353999853134155, 0.3831999897956848, 0.3698999881744385, 0.047600001096725464, 0.3330000042915344, 0.48019999265670776, 0.06499999761581421, 0.2935999929904938, 0.2694000005722046, -0.35929998755455017, -0.36239999532699585, 0.21310000121593475, -0.7505000233650208, -0.10840000212192535, 0.19179999828338623, -1.3206000328063965, -0.7590000033378601, 1.9120999574661255, 1.9077999591827393, 1.9076999425888062, 1.9069000482559204, 1.9068000316619873, 1.9062000513076782, 1.906000018119812, 1.9050999879837036, 1.9050999879837036, 1.9048999547958374, 1.9048000574111938, 1.9042999744415283, 1.9041999578475952, 1.904099941253662, 1.9038000106811523, 1.9033000469207764, 1.9032000303268433, 1.902899980545044, 1.9027999639511108, 1.9026000499725342, 1.902400016784668, 1.9021999835968018, 1.9017000198364258, 1.9013999700546265, 1.9003000259399414, 1.899899959564209, 1.8997000455856323, 1.8996000289916992, 1.899399995803833, 1.8991999626159668, 1.8984999656677246, 1.8982000350952148, 1.8918999433517456, 1.8961000442504883, 1.8659000396728516, 1.888200044631958, 1.8926000595092773, 1.8499000072479248, 1.8594000339508057, 1.8704999685287476, 1.8550000190734863, 1.8087999820709229, 1.8387999534606934, 1.8387999534606934, 1.8479000329971313, 1.795300006866455, 1.8135000467300415, 1.8027000427246094, 1.8055000305175781, 1.870300054550171, 1.7111999988555908, 1.7115999460220337, 1.666599988937378, 1.5676000118255615, 1.6613999605178833, 1.5109000205993652, 1.7633999586105347, 1.7781000137329102, 1.6615999937057495, 1.6557999849319458, 1.773800015449524, 1.7266000509262085, 1.3041000366210938, 1.5772000551223755, 1.2096999883651733, 1.4250999689102173, 1.045300006866455, 1.0613000392913818, 1.3193000555038452, 1.2421000003814697, 1.1065000295639038, 1.305400013923645, 0.3375000059604645, 1.4046000242233276, 1.3553999662399292, 0.5260000228881836, 0.03350000083446503, 0.5353000164031982, 1.2268999814987183, -0.21789999306201935, -0.002199999988079071, -0.050700001418590546, -0.06780000030994415, 0.415800005197525, 0.5293999910354614, 0.5206000208854675, 0.9086999893188477, 0.9290000200271606, 0.06019999831914902, 0.4291999936103821, 0.31060001254081726, -0.17090000212192535, -0.9787999987602234, 0.21070000529289246, 0.21809999644756317, -0.6327999830245972, 0.13539999723434448, 0.16259999573230743, -0.22869999706745148, 0.003700000001117587, 1.6836999654769897, 1.6833000183105469, 1.6822999715805054, 1.6822999715805054, 1.6816999912261963, 1.6812000274658203, 1.6811000108718872, 1.6806999444961548, 1.680299997329712, 1.6801999807357788, 1.679900050163269, 1.6793999671936035, 1.6792999505996704, 1.6792999505996704, 1.6790000200271606, 1.6785999536514282, 1.6785999536514282, 1.6785000562667847, 1.6782000064849854, 1.6780999898910522, 1.6777000427246094, 1.676900029182434, 1.6766999959945679, 1.6766999959945679, 1.6766999959945679, 1.6763999462127686, 1.676300048828125, 1.6759999990463257, 1.6757999658584595, 1.6756999492645264, 1.6684999465942383, 1.667799949645996, 1.6751999855041504, 1.6755000352859497, 1.6612999439239502, 1.6756000518798828, 1.6155999898910522, 1.6512000560760498, 1.6512999534606934, 1.6483999490737915, 1.6577999591827393, 1.6015000343322754, 1.6406999826431274, 1.6633000373840332, 1.6682000160217285, 1.6434999704360962, 1.6358000040054321, 1.6484999656677246, 1.6648999452590942, 1.6238000392913818, 1.6523000001907349, 1.6344000101089478, 1.5241999626159668, 1.6181999444961548, 1.62909996509552, 1.3782000541687012, 1.617799997329712, 1.4354000091552734, 1.2640000581741333, 1.5508999824523926, 1.354099988937378, 1.4052000045776367, 1.5625, 1.3761999607086182, 1.1728999614715576, 0.8284000158309937, 0.8605999946594238, 1.2897000312805176, 1.118499994277954, 1.4452999830245972, 1.1583000421524048, 1.2798000574111938, 1.2766000032424927, 1.0041999816894531, 1.07260000705719, 1.3691999912261963, 1.3342000246047974, 0.5113000273704529, 1.3224999904632568, 1.132699966430664, 0.5623999834060669, 0.9143999814987183, 0.8568000197410583, 0.9192000031471252, 0.13230000436306, 0.37040001153945923, 0.07639999687671661, -0.30329999327659607, 0.08950000256299973, 0.46869999170303345, 0.26930001378059387, 0.3287999927997589, -0.29899999499320984, 0.5030999779701233, 0.01489999983459711, 0.27480000257492065, -1.1801999807357788, 0.22759999334812164, 0.08299999684095383, 0.2094999998807907, 2.125499963760376, 2.120800018310547, 2.119800090789795, 2.1196000576019287, 2.119499921798706, 2.1185998916625977, 2.1180999279022217, 2.1177000999450684, 2.117500066757202, 2.1173999309539795, 2.1171998977661133, 2.1166999340057373, 2.1164000034332275, 2.1161000728607178, 2.1152000427246094, 2.114300012588501, 2.114300012588501, 2.1142001152038574, 2.1140999794006348, 2.113300085067749, 2.113300085067749, 2.11299991607666, 2.1126999855041504, 2.1126999855041504, 2.112299919128418, 2.1120998859405518, 2.1110999584198, 2.110300064086914, 2.110100030899048, 2.110100030899048, 2.089400053024292, 2.108799934387207, 2.1050000190734863, 2.1078999042510986, 2.0443999767303467, 2.082900047302246, 2.092900037765503, 2.033600091934204, 2.0769999027252197, 2.0980000495910645, 2.053999900817871, 2.0905001163482666, 2.0678999423980713, 2.0592000484466553, 2.027100086212158, 1.916700005531311, 1.9522000551223755, 1.9352999925613403, 1.8336000442504883, 2.026700019836426, 1.9815000295639038, 2.011899948120117, 2.0508999824523926, 1.6468000411987305, 1.8672000169754028, 1.7496000528335571, 1.7263000011444092, 1.695099949836731, 1.863800048828125, 1.8033000230789185, 1.8809000253677368, 1.7348999977111816, 1.5831999778747559, 0.5985000133514404, 0.7265999913215637, 0.5666000247001648, 1.1313999891281128, 1.323199987411499, 0.7113999724388123, 0.7886000275611877, 0.8222000002861023, 0.39640000462532043, 0.8970999717712402, 0.34779998660087585, 0.8152999877929688, 1.1246000528335571, -0.03790000081062317, -0.04729999974370003, -0.5268999934196472, 0.4471000134944916, 0.2694999873638153, -0.05389999970793724, 0.03180000185966492, 0.19009999930858612, -0.007000000216066837, -1.0856000185012817, -1.0011999607086182, 0.09560000151395798, 0.07100000232458115, -0.4250999987125397, 1.4609999656677246, 1.4591000080108643, 1.458899974822998, 1.458899974822998, 1.458400011062622, 1.458299994468689, 1.4581999778747559, 1.457800030708313, 1.4564000368118286, 1.4562000036239624, 1.4560999870300293, 1.4558000564575195, 1.4558000564575195, 1.455199956893921, 1.454699993133545, 1.454699993133545, 1.4541000127792358, 1.4529999494552612, 1.4529999494552612, 1.4528000354766846, 1.4524999856948853, 1.4524999856948853, 1.451799988746643, 1.450600028038025, 1.450600028038025, 1.4502999782562256, 1.4501999616622925, 1.4500000476837158, 1.4499000310897827, 1.4498000144958496, 1.4488999843597412, 1.4431999921798706, 1.448799967765808, 1.4431999921798706, 1.4256999492645264, 1.4349000453948975, 1.4421000480651855, 1.4119000434875488, 1.3825000524520874, 1.3973000049591064, 1.4259999990463257, 1.3812999725341797, 1.3809000253677368, 1.3865000009536743, 1.3967000246047974, 1.4220000505447388, 1.4005000591278076, 1.3585000038146973, 1.2049000263214111, 1.2766000032424927, 1.2704999446868896, 1.3417999744415283, 1.190999984741211, 1.3250000476837158, 1.2991000413894653, 0.9229000210762024, 1.2989000082015991, 1.2736999988555908, 1.1486999988555908, 1.1449999809265137, 1.0477999448776245, 1.0901999473571777, 1.0363999605178833, 1.2093000411987305, 0.9093999862670898, 0.734000027179718, 0.7250000238418579, 0.9071000218391418, 0.3531000018119812, 0.9970999956130981, 0.9606999754905701, 0.21400000154972076, 0.6279000043869019, -0.04470000043511391, -0.1379999965429306, 0.05620000138878822, 0.6146000027656555, 0.35269999504089355, 0.2888999879360199, 0.8202000260353088, 0.5813999772071838, 0.3197000026702881, 0.2061000019311905, 0.27959999442100525, -0.12120000272989273, 0.16689999401569366, -0.093299999833107, 0.11919999867677689, -1.1126999855041504, 1.7217999696731567, 1.7206000089645386, 1.719099998474121, 1.7187999486923218, 1.7182999849319458, 1.717900037765503, 1.7177000045776367, 1.7177000045776367, 1.7175999879837036, 1.7173999547958374, 1.7171000242233276, 1.7168999910354614, 1.7167999744415283, 1.7161999940872192, 1.715999960899353, 1.715399980545044, 1.7152999639511108, 1.715000033378601, 1.715000033378601, 1.7144999504089355, 1.714400053024292, 1.714400053024292, 1.7142000198364258, 1.7136000394821167, 1.7135000228881836, 1.7132999897003174, 1.7131999731063843, 1.7131999731063843, 1.7127000093460083, 1.7125999927520752, 1.7120000123977661, 1.705299973487854, 1.7116999626159668, 1.7121000289916992, 1.6782000064849854, 1.7081999778747559, 1.6871000528335571, 1.6901999711990356, 1.7035000324249268, 1.7028000354766846, 1.5638999938964844, 1.6825000047683716, 1.4085999727249146, 1.6282000541687012, 1.6936999559402466, 1.444100022315979, 1.3854000568389893, 1.3316999673843384, 1.4945000410079956, 1.5266000032424927, 1.4925999641418457, 1.1201000213623047, 1.3626999855041504, 1.3797999620437622, 1.5410000085830688, 1.3357000350952148, 1.4555000066757202, 0.9728999733924866, 1.4911999702453613, 0.947700023651123, 1.2144999504089355, 1.3357000350952148, 1.0232000350952148, 0.9937000274658203, 0.9501000046730042, 1.0329999923706055, 0.7767999768257141, 0.9549999833106995, 0.7832000255584717, 0.9686999917030334, 1.1131999492645264, 0.2596000134944916, 0.10490000247955322, 0.9941999912261963, 0.8478000164031982, 0.4226999878883362, 0.015599999576807022, 1.001099944114685, 0.6434999704360962, 0.5913000106811523, 0.8535000085830688, 0.47119998931884766, -0.29820001125335693, 0.5425999760627747, -0.7421000003814697, -0.3474999964237213, 0.2865000069141388, -0.38690000772476196, 0.295199990272522, 0.17980000376701355, 0.08269999921321869], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -7.381100177764893, -8.062199592590332, -8.271499633789062, -8.313300132751465, -8.217000007629395, -8.557999610900879, -8.54740047454834, -8.516900062561035, -8.687399864196777, -8.66919994354248, -7.877799987792969, -8.720499992370605, -8.709799766540527, -8.396599769592285, -8.73960018157959, -8.46679973602295, -8.787799835205078, -8.591300010681152, -8.335000038146973, -8.787199974060059, -8.226900100708008, -8.748600006103516, -8.916399955749512, -8.704700469970703, -8.19950008392334, -7.212500095367432, -8.976699829101562, -8.839799880981445, -8.970399856567383, -8.947699546813965, -6.726099967956543, -8.152099609375, -8.124799728393555, -8.674200057983398, -8.72130012512207, -7.273099899291992, -7.878900051116943, -8.573800086975098, -7.9191999435424805, -8.400699615478516, -8.027000427246094, -8.272700309753418, -7.434299945831299, -8.077300071716309, -8.340299606323242, -6.011199951171875, -6.903500080108643, -7.162600040435791, -6.44890022277832, -7.763500213623047, -7.382500171661377, -8.01509952545166, -5.400100231170654, -6.458499908447266, -7.458600044250488, -6.474699974060059, -6.529900074005127, -5.825200080871582, -6.16480016708374, -7.498499870300293, -7.543900012969971, -4.743599891662598, -6.7118000984191895, -6.7546000480651855, -5.424099922180176, -6.826499938964844, -5.198999881744385, -6.835599899291992, -7.390900135040283, -6.775100231170654, -5.596199989318848, -6.087100028991699, -6.722300052642822, -6.93720006942749, -6.5005998611450195, -4.382299900054932, -5.700099945068359, -4.99399995803833, -6.327499866485596, -4.787700176239014, -4.39300012588501, -5.027900218963623, -5.71019983291626, -6.289899826049805, -5.591599941253662, -5.542500019073486, -5.605100154876709, -5.606200218200684, -5.3256001472473145, -5.6143999099731445, -5.762700080871582, -5.465400218963623, -5.68720006942749, -5.720200061798096, -5.404300212860107, -5.409800052642822, -5.743299961090088, -5.344399929046631, -5.682600021362305, -5.819200038909912, -5.5183000564575195, -5.653200149536133, -7.683000087738037, -8.10200023651123, -8.335200309753418, -7.0640997886657715, -8.305100440979004, -8.105199813842773, -8.185999870300293, -8.480799674987793, -8.42389965057373, -7.708499908447266, -8.09179973602295, -7.927299976348877, -8.23009967803955, -8.174200057983398, -8.668100357055664, -8.519200325012207, -7.856900215148926, -8.613300323486328, -8.695099830627441, -8.685799598693848, -8.797100067138672, -7.906599998474121, -8.853799819946289, -8.85509967803955, -8.894599914550781, -8.022500038146973, -8.956500053405762, -8.878100395202637, -8.804300308227539, -8.194199562072754, -7.133800029754639, -8.085000038146973, -7.716100215911865, -8.330499649047852, -6.5528998374938965, -7.917900085449219, -8.312700271606445, -6.669899940490723, -7.10230016708374, -7.497700214385986, -7.212900161743164, -6.501800060272217, -7.072400093078613, -7.243599891662598, -7.436299800872803, -6.826900005340576, -7.093400001525879, -7.018199920654297, -7.262199878692627, -7.9217000007629395, -6.6066999435424805, -6.697500228881836, -6.388899803161621, -5.837500095367432, -6.561100006103516, -5.662499904632568, -7.228400230407715, -7.327899932861328, -6.790200233459473, -6.772500038146973, -7.3531999588012695, -7.15910005569458, -5.548699855804443, -6.618899822235107, -5.333099842071533, -6.1427001953125, -4.942999839782715, -5.0904998779296875, -6.084099769592285, -5.92110013961792, -5.653500080108643, -6.1280999183654785, -4.190800189971924, -6.4430999755859375, -6.361999988555908, -5.0482001304626465, -4.560500144958496, -5.30049991607666, -6.259500026702881, -4.555300235748291, -5.0472002029418945, -5.0366997718811035, -5.070400238037109, -5.540599822998047, -5.69789981842041, -5.713099956512451, -6.031300067901611, -6.0625, -5.478300094604492, -5.742400169372559, -5.678999900817871, -5.544000148773193, -5.176400184631348, -5.736700057983398, -5.792900085449219, -5.527100086212158, -5.80810022354126, -5.837699890136719, -5.759099960327148, -5.841000080108643, -7.930799961090088, -7.5584001541137695, -8.184300422668457, -8.201000213623047, -8.305700302124023, -8.18179988861084, -8.396400451660156, -7.731599807739258, -7.924799919128418, -7.804800033569336, -8.549599647521973, -8.624500274658203, -8.551400184631348, -8.6181001663208, -8.524700164794922, -7.857699871063232, -8.714799880981445, -8.591300010681152, -8.739100456237793, -7.968599796295166, -8.795000076293945, -7.852799892425537, -8.89799976348877, -8.8927001953125, -8.525799751281738, -8.781999588012695, -8.8548002243042, -8.840399742126465, -8.804499626159668, -8.942700386047363, -6.962299823760986, -7.223800182342529, -8.022500038146973, -8.442299842834473, -7.14109992980957, -8.719499588012695, -3.971299886703491, -6.559899806976318, -6.77239990234375, -7.093100070953369, -7.825900077819824, -6.034299850463867, -7.321000099182129, -8.066300392150879, -8.246299743652344, -7.548900127410889, -7.451499938964844, -7.769800186157227, -8.184000015258789, -7.364099979400635, -7.906099796295166, -7.581699848175049, -5.975299835205078, -7.389500141143799, -7.654699802398682, -5.6539998054504395, -7.639400005340576, -6.255099773406982, -5.009099960327148, -7.175600051879883, -6.02370023727417, -6.350599765777588, -7.262499809265137, -6.221499919891357, -5.2291998863220215, -3.509000062942505, -4.033599853515625, -6.016200065612793, -5.285699844360352, -6.747900009155273, -5.532700061798096, -6.202499866485596, -6.233099937438965, -5.379000186920166, -5.620800018310547, -6.602700233459473, -6.505199909210205, -4.082699775695801, -6.502299785614014, -6.030200004577637, -4.8730998039245605, -5.698999881744385, -5.700200080871582, -5.848100185394287, -4.853700160980225, -5.196199893951416, -4.926199913024902, -4.831699848175049, -5.283599853515625, -5.633800029754639, -5.563499927520752, -5.647299766540527, -5.343999862670898, -5.790500164031982, -5.559199810028076, -5.736199855804443, -5.377799987792969, -5.750699996948242, -5.72130012512207, -5.771399974822998, -6.549200057983398, -7.29449987411499, -6.812399864196777, -7.816800117492676, -7.7733001708984375, -7.895400047302246, -8.18589973449707, -7.743899822235107, -7.93149995803833, -8.183899879455566, -8.150400161743164, -8.10319995880127, -8.381199836730957, -8.243800163269043, -8.475099563598633, -8.472900390625, -8.553500175476074, -8.361900329589844, -8.474300384521484, -8.549099922180176, -8.6225004196167, -8.292099952697754, -8.632200241088867, -8.66469955444336, -8.690299987792969, -8.429100036621094, -8.712699890136719, -8.800100326538086, -8.804300308227539, -8.717700004577637, -4.827300071716309, -8.144599914550781, -7.91480016708374, -8.223199844360352, -4.6778998374938965, -7.019100189208984, -7.515200138092041, -5.118000030517578, -7.113100051879883, -7.9095001220703125, -6.866099834442139, -7.763400077819824, -7.258399963378906, -7.103799819946289, -6.350900173187256, -5.530099868774414, -6.056600093841553, -5.9944000244140625, -5.64709997177124, -7.544300079345703, -7.251999855041504, -7.494800090789795, -7.742199897766113, -5.488399982452393, -6.705100059509277, -6.151000022888184, -6.218100070953369, -6.15500020980835, -6.889100074768066, -6.7494001388549805, -7.064599990844727, -6.748899936676025, -6.34630012512207, -3.995500087738037, -4.31850004196167, -4.327700138092041, -5.624800205230713, -6.105800151824951, -5.358399868011475, -5.515699863433838, -5.560999870300293, -5.133999824523926, -5.7347002029418945, -5.190800189971924, -5.73859977722168, -6.11329984664917, -5.040500164031982, -5.033199787139893, -4.7245001792907715, -5.531300067901611, -5.534800052642822, -5.42710018157959, -5.534800052642822, -5.6427001953125, -5.581200122833252, -5.422999858856201, -5.529600143432617, -5.851900100708008, -5.872499942779541, -5.860599994659424, -7.507500171661377, -7.44350004196167, -6.44890022277832, -7.604100227355957, -8.211899757385254, -8.026399612426758, -6.43720006942749, -6.918799877166748, -8.431400299072266, -8.361599922180176, -8.619799613952637, -8.661999702453613, -8.662699699401855, -7.682400226593018, -8.786100387573242, -7.962600231170654, -8.705900192260742, -8.942399978637695, -8.60200023651123, -8.941800117492676, -9.011300086975098, -8.217300415039062, -8.897000312805176, -8.483699798583984, -9.118599891662598, -9.113100051879883, -8.791299819946289, -9.209799766540527, -8.97659969329834, -7.941699981689453, -6.993299961090088, -7.231299877166748, -7.9629998207092285, -8.14840030670166, -6.648600101470947, -7.693699836730957, -8.332599639892578, -6.871200084686279, -5.508500099182129, -6.418399810791016, -7.714900016784668, -6.462299823760986, -6.502299785614014, -6.752099990844727, -7.0157999992370605, -7.668499946594238, -7.210299968719482, -6.36870002746582, -3.8424999713897705, -5.1641998291015625, -5.395699977874756, -6.77209997177124, -5.5081000328063965, -6.751999855041504, -6.57390022277832, -3.2746999263763428, -6.604499816894531, -6.433800220489502, -5.566400051116943, -5.578100204467773, -5.098400115966797, -5.381700038909912, -5.206600189208984, -6.344099998474121, -5.1605000495910645, -4.804500102996826, -4.8053998947143555, -5.641300201416016, -4.69189977645874, -6.058899879455566, -5.999899864196777, -4.788599967956543, -5.474599838256836, -4.5731000900268555, -4.475399971008301, -4.929800033569336, -5.712500095367432, -5.451600074768066, -5.42080020904541, -5.957399845123291, -5.784299850463867, -5.63670015335083, -5.560999870300293, -5.720699787139893, -5.55679988861084, -5.6778998374938965, -5.667500019073486, -5.713600158691406, -5.706699848175049, -7.033100128173828, -7.118299961090088, -7.98199987411499, -7.446400165557861, -8.110199928283691, -8.2121000289917, -8.183699607849121, -8.248499870300293, -8.123100280761719, -7.7093000411987305, -8.344599723815918, -8.230500221252441, -8.282600402832031, -8.093899726867676, -8.491299629211426, -8.486800193786621, -8.405500411987305, -8.5068998336792, -8.592000007629395, -8.441900253295898, -8.686699867248535, -8.6875, -8.405400276184082, -8.558099746704102, -8.775699615478516, -8.64490032196045, -8.750200271606445, -7.976600170135498, -8.851300239562988, -8.867899894714355, -7.995100021362305, -7.533599853515625, -8.491499900817871, -8.534700393676758, -7.164999961853027, -8.461199760437012, -7.776800155639648, -7.9770002365112305, -8.3233003616333, -8.352999687194824, -6.160099983215332, -8.04990005493164, -5.242400169372559, -7.553999900817871, -8.264399528503418, -5.889200210571289, -5.583799839019775, -5.441999912261963, -6.689599990844727, -6.944300174713135, -6.804200172424316, -4.646999835968018, -6.182700157165527, -6.38640022277832, -7.278800010681152, -6.210100173950195, -6.86359977722168, -4.593699932098389, -7.093900203704834, -4.953400135040283, -6.105400085449219, -6.60290002822876, -5.463099956512451, -5.428899765014648, -5.423999786376953, -5.752699851989746, -5.101099967956543, -5.662700176239014, -5.245999813079834, -5.782299995422363, -6.138500213623047, -4.268799781799316, -4.092800140380859, -5.912600040435791, -5.680500030517578, -4.950399875640869, -4.578400135040283, -6.088699817657471, -5.645299911499023, -5.585000038146973, -5.9446001052856445, -5.627999782562256, -5.192399978637695, -5.76170015335083, -5.079500198364258, -5.350200176239014, -5.660999774932861, -5.372900009155273, -5.6809000968933105, -5.656000137329102, -5.76200008392334]}, \"token.table\": {\"Topic\": [6, 5, 5, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 4, 1, 2, 3, 4, 5, 6, 3, 6, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 2, 3, 4, 5, 6, 2, 5, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 1, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 2, 5, 2, 3, 2, 6, 2, 2, 4, 6, 2, 1, 2, 3, 4, 5, 6, 3, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 1, 4, 5, 6, 1, 1, 4, 6, 1, 2, 3, 4, 5, 6, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 1, 4, 6, 3, 4, 6, 4, 1, 3, 6, 6, 1, 3, 5, 2, 1, 2, 3, 4, 5, 6, 3, 6, 1, 6, 1, 4, 4, 4, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 2, 5, 1, 2, 3, 4, 5, 6, 6, 3, 2, 4, 2, 1, 5, 6, 2, 4, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 3, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 2, 3, 4, 1, 2, 3, 4, 5, 6, 3, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4, 5, 6, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 5, 2, 4, 5, 1, 2, 3, 4, 5, 6, 1, 5, 4, 1, 3, 4, 5, 6, 3, 1, 2, 1, 5, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 4, 1, 2, 4, 5, 1, 2, 3, 4, 5, 6, 2, 5, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 5, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 5, 1, 2, 3, 4, 5, 6, 1, 5, 6, 5, 6, 3, 5, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 3, 4, 1, 2, 3, 4, 5, 6, 3, 1, 3, 4, 6, 1, 3, 4, 1, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 4, 1, 1, 2, 3, 4, 5, 6, 2, 5, 2, 3, 4, 6, 1, 4, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 5, 6, 6, 2, 1, 3, 4, 5, 6, 3, 4, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 6, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 1, 2, 4, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 5, 1, 2, 3, 4, 5, 6, 5, 1, 2, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4, 3, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 5, 1, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 6, 1, 3, 4, 5, 6, 3, 6, 1, 2, 3, 4, 5, 6, 5, 1, 2, 5, 1, 1, 2, 3, 4, 5, 6, 1, 3, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 4, 3, 1, 3, 4, 5, 3, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 6, 1, 3, 4, 5, 6, 3, 4, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 3, 1, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 1, 2, 4, 5, 6, 1, 2, 3, 5, 1, 2, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 4, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 2, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 3, 5, 1, 2, 5, 6, 1, 2, 6, 1, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 1, 1, 2, 3, 4, 5, 6, 3, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 2, 4, 5, 1, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 3, 3, 4, 1, 2, 3, 4, 5, 6, 2, 4, 1, 2, 3, 4, 5, 6, 4, 6, 3, 4, 3, 2, 3, 4, 2, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 6, 1, 2, 3, 4, 5, 2, 3, 6, 1, 2, 3, 4, 5, 6, 2, 4, 5, 1, 5, 6, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 3, 2, 1, 2, 6, 1, 2, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 4, 1, 2, 3, 4, 5, 6, 3, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 2, 3, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 1, 6, 5, 6, 5, 1, 2, 3, 4, 5, 6, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 3, 4, 2, 5, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4, 5, 5, 6, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 5, 6, 3, 5, 1, 6, 3, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 5, 6, 1, 4, 1, 5, 4, 6, 6, 1, 5, 6, 2, 4, 1, 2, 3, 4, 5, 6, 2, 1, 2, 3, 4, 5, 6, 2, 3, 4, 6, 2, 5, 3], \"Freq\": [0.990804135799408, 0.9939561486244202, 0.9888607263565063, 0.02855258248746395, 0.9707878232002258, 0.23340705037117004, 0.24983452260494232, 0.14066025614738464, 0.0667366161942482, 0.20500120520591736, 0.10438291728496552, 0.00839043129235506, 0.19417855143547058, 0.0035958990920335054, 0.745549738407135, 0.02636992745101452, 0.023972660303115845, 0.9897993206977844, 0.9967994689941406, 0.6218729615211487, 0.16790570318698883, 0.012437459081411362, 0.009328094311058521, 0.16790570318698883, 0.02176555246114731, 0.9753169417381287, 0.026967288926243782, 0.9806103706359863, 0.0037861403543502092, 0.011358421295881271, 0.03544355556368828, 0.5430867671966553, 0.011433404870331287, 0.07431713491678238, 0.2835484445095062, 0.0525936633348465, 0.8519284725189209, 0.0016869871178641915, 0.14676786959171295, 0.8988452553749084, 0.002748762257397175, 0.0742165818810463, 0.008246286772191525, 0.01924133487045765, 0.9710633754730225, 0.018674295395612717, 0.927814245223999, 0.011895054951310158, 0.04758021980524063, 0.011895054951310158, 0.002727801213040948, 0.030005812644958496, 0.002045850735157728, 0.9622318148612976, 0.002045850735157728, 0.000681950303260237, 0.9964112639427185, 0.9946480989456177, 0.9823553562164307, 0.06569964438676834, 0.15211839973926544, 0.30831378698349, 0.2166150063276291, 0.07605919986963272, 0.18125884234905243, 0.013277149759232998, 0.9559547901153564, 0.013277149759232998, 0.006638574879616499, 0.006638574879616499, 0.20153234899044037, 0.14908602833747864, 0.2569071650505066, 0.049251630902290344, 0.10382776707410812, 0.23960253596305847, 0.9902188181877136, 0.016344482079148293, 0.9806689023971558, 0.9979961514472961, 0.16844122111797333, 0.18305231630802155, 0.24342650175094604, 0.08298004418611526, 0.1885659396648407, 0.13342970609664917, 0.13567481935024261, 0.13748516142368317, 0.19953954219818115, 0.1146547719836235, 0.28663694858551025, 0.1260196715593338, 0.024748841300606728, 0.01570599526166916, 0.4354844391345978, 0.14278177917003632, 0.09851942956447601, 0.28270792961120605, 0.9869467616081238, 0.014263588935136795, 0.14537888765335083, 0.015360788442194462, 0.059248752892017365, 0.7614561915397644, 0.004388796631246805, 0.988562285900116, 0.0754052922129631, 0.046216145157814026, 0.20432400703430176, 0.10864848643541336, 0.08027014881372452, 0.4856749475002289, 0.06929677724838257, 0.02494683861732483, 0.080384261906147, 0.04157806560397148, 0.016631225124001503, 0.765036404132843, 0.9977313280105591, 0.9314531087875366, 0.06549279391765594, 0.9833466410636902, 0.9925538897514343, 0.9821089506149292, 0.9981790781021118, 0.9854312539100647, 0.019611388444900513, 0.6765929460525513, 0.30070796608924866, 0.9875461459159851, 0.16963091492652893, 0.13976004719734192, 0.21107426285743713, 0.11354885250329971, 0.24480262398719788, 0.12116494029760361, 0.04213852062821388, 0.9551398158073425, 0.9791235327720642, 0.03398283198475838, 0.026140641421079636, 0.07842192053794861, 0.1150188222527504, 0.016730010509490967, 0.729846715927124, 0.8488385081291199, 0.04686838388442993, 0.036453187465667725, 0.010415196418762207, 0.052075982093811035, 0.0052075982093811035, 0.9996981620788574, 0.9970623254776001, 0.9720056653022766, 0.9973174929618835, 0.9647562503814697, 0.025726832449436188, 0.012863416224718094, 0.9761415719985962, 0.9907772541046143, 0.017382057383656502, 0.9891645908355713, 0.12192714959383011, 0.11792357265949249, 0.1688781976699829, 0.1470405012369156, 0.10445699095726013, 0.33957621455192566, 0.015183688141405582, 0.9717560410499573, 0.7551116347312927, 0.10520131140947342, 0.01870245486497879, 0.03039149008691311, 0.07714763283729553, 0.011689035221934319, 0.9945728182792664, 0.9938219785690308, 0.16831865906715393, 0.07106118649244308, 0.1490478366613388, 0.07196450978517532, 0.25413408875465393, 0.28575029969215393, 0.3680698573589325, 0.22595670819282532, 0.10715138912200928, 0.023307854309678078, 0.200382798910141, 0.07542680948972702, 0.35849475860595703, 0.49316489696502686, 0.027215570211410522, 0.026746336370706558, 0.05490037426352501, 0.03941565379500389, 0.9940721392631531, 0.630609929561615, 0.21115444600582123, 0.002853438491001725, 0.03709470108151436, 0.11128409951925278, 0.00570687698200345, 0.07255177944898605, 0.1887437254190445, 0.540592610836029, 0.010910042561590672, 0.052368201315402985, 0.13473902642726898, 0.6107427477836609, 0.15092258155345917, 0.2143946886062622, 0.023978354409337044, 0.6500669717788696, 0.32940614223480225, 0.01749059185385704, 0.9839821457862854, 0.013666419312357903, 0.9972633123397827, 0.9918094277381897, 0.021345023065805435, 0.010672511532902718, 0.971198558807373, 0.9975703954696655, 0.06666339188814163, 0.8707906007766724, 0.0583304688334465, 0.9929943084716797, 0.16852056980133057, 0.05637132003903389, 0.03500955551862717, 0.11986322700977325, 0.5257367491722107, 0.0949411690235138, 0.9888947010040283, 0.9949106574058533, 0.993858277797699, 0.9899829626083374, 0.9795730113983154, 0.983985424041748, 0.9990206360816956, 0.9895994067192078, 0.9979885816574097, 0.13256606459617615, 0.2077808529138565, 0.0817960873246193, 0.3666720986366272, 0.07333441823720932, 0.13820718228816986, 0.023728804662823677, 0.020667023956775665, 0.7332965731620789, 0.040568601340055466, 0.020667023956775665, 0.16074351966381073, 0.03603459894657135, 0.024023065343499184, 0.7018166780471802, 0.04118239879608154, 0.0034318664111196995, 0.19218452274799347, 0.019542891532182693, 0.010992877185344696, 0.8488943576812744, 0.05130009353160858, 0.004885722883045673, 0.06473582983016968, 0.9968365430831909, 0.009896119125187397, 0.9797158241271973, 0.04066459834575653, 0.008132919669151306, 0.10301697999238968, 0.02439875900745392, 0.029820706695318222, 0.7943151593208313, 0.9886574745178223, 0.9973029494285583, 0.9922285079956055, 0.9939621090888977, 0.9936596155166626, 0.9683740735054016, 0.02105160988867283, 0.9936692118644714, 0.9842521548271179, 0.011313242837786674, 0.005656621418893337, 0.9921267032623291, 0.9840763807296753, 0.1610507071018219, 0.20611736178398132, 0.13651441037654877, 0.04375221207737923, 0.2212647646665573, 0.23127958178520203, 0.117804616689682, 0.24766525626182556, 0.22148644924163818, 0.043401699513196945, 0.3337797522544861, 0.035479169338941574, 0.10728806257247925, 0.501481294631958, 0.018082257360219955, 0.2941380739212036, 0.06509612500667572, 0.012054838240146637, 0.06088988855481148, 0.8707253932952881, 0.006088988855481148, 0.05480089783668518, 0.006088988855481148, 0.01645626686513424, 0.012209488078951836, 0.015925418585538864, 0.12846504151821136, 0.82440584897995, 0.0026542365085333586, 0.993620753288269, 0.10423916578292847, 0.002542418660596013, 0.008050992153584957, 0.04915342852473259, 0.8292522430419922, 0.006356046535074711, 0.08695455640554428, 0.03623106703162193, 0.07803614437580109, 0.0044592078775167465, 0.7296379208564758, 0.0646585151553154, 0.07350045442581177, 0.9025856256484985, 0.020580127835273743, 0.002940018195658922, 0.9879122972488403, 0.005186643451452255, 0.06742636859416962, 0.01037328690290451, 0.7831831574440002, 0.11929280310869217, 0.015559930354356766, 0.007835419848561287, 0.0626833587884903, 0.7267351746559143, 0.16258496046066284, 0.02742396853864193, 0.015670839697122574, 0.39296817779541016, 0.2150009125471115, 0.06686631590127945, 0.004114849958568811, 0.20985735952854156, 0.11110095679759979, 0.9815026521682739, 0.992822527885437, 0.6462728381156921, 0.24773791432380676, 0.09424811601638794, 0.01346401683986187, 0.607318639755249, 0.01550600677728653, 0.11371071636676788, 0.10337337851524353, 0.02842767909169197, 0.1318010687828064, 0.1722789704799652, 0.2229185551404953, 0.11798499524593353, 0.09684166312217712, 0.3184550702571869, 0.07152187824249268, 0.9921820163726807, 0.011003071442246437, 0.9902764558792114, 0.9859127402305603, 0.06684105098247528, 0.012218257412314415, 0.10924559086561203, 0.054622795432806015, 0.04456070065498352, 0.7129712104797363, 0.9875046610832214, 0.007910707034170628, 0.007910707034170628, 0.9809276461601257, 0.003955353517085314, 0.6440882086753845, 0.2215663343667984, 0.01030541118234396, 0.00515270559117198, 0.11335952579975128, 0.00515270559117198, 0.041049033403396606, 0.550278902053833, 0.21744892001152039, 0.05769053101539612, 0.07433202862739563, 0.05879996344447136, 0.08121849596500397, 0.010800331830978394, 0.03412904962897301, 0.042553309351205826, 0.2842647433280945, 0.5469288229942322, 0.775225818157196, 0.10822957754135132, 0.050339337438344955, 0.06544113904237747, 0.0027689337730407715, 0.8140665292739868, 0.13844668865203857, 0.044302940368652344, 0.006086884066462517, 0.02739097736775875, 0.015217210166156292, 0.0030434420332312584, 0.9343366622924805, 0.012173768132925034, 0.03662949800491333, 0.892844021320343, 0.009157374501228333, 0.004578687250614166, 0.05036555975675583, 0.9855877757072449, 0.01006007194519043, 0.9657669067382812, 0.02012014389038086, 0.08793292939662933, 0.0033820357639342546, 0.6933173537254333, 0.008455089293420315, 0.16571974754333496, 0.03889341279864311, 0.9840367436408997, 0.006473925895988941, 0.9965015053749084, 0.04060386121273041, 0.11730004847049713, 0.007519233971834183, 0.8271157145500183, 0.007519233971834183, 0.9933923482894897, 0.22148412466049194, 0.7794537544250488, 0.9726542830467224, 0.0164856668561697, 0.013910007663071156, 0.9806555509567261, 0.1089150607585907, 0.7057251334190369, 0.06557130813598633, 0.018893428146839142, 0.07335095852613449, 0.027784453704953194, 0.038179680705070496, 0.20362496376037598, 0.003393749240785837, 0.6176623702049255, 0.11284216493368149, 0.023756245151162148, 0.13348345458507538, 0.10421465337276459, 0.08603252470493317, 0.14678744971752167, 0.032816529273986816, 0.49623915553092957, 0.16726522147655487, 0.05201033130288124, 0.10776540637016296, 0.10734932124614716, 0.08363261073827744, 0.48182371258735657, 0.17157141864299774, 0.018498266115784645, 0.19885636866092682, 0.11237697303295135, 0.0823172852396965, 0.4166734516620636, 0.16136343777179718, 0.007683972828090191, 0.09161660075187683, 0.0531967356801033, 0.01063934713602066, 0.6755985617637634, 0.9943279027938843, 0.09325753152370453, 0.1495777666568756, 0.3057384192943573, 0.09581754356622696, 0.2790411710739136, 0.07680032402276993, 0.9852235317230225, 0.022060079500079155, 0.7132759094238281, 0.004902239888906479, 0.22060079872608185, 0.031864557415246964, 0.009804479777812958, 0.03468640148639679, 0.11966808140277863, 0.003468640148639679, 0.6486356854438782, 0.06416983902454376, 0.13007400929927826, 0.9849953651428223, 0.9885292053222656, 0.9869713187217712, 0.009187866933643818, 0.9494128823280334, 0.03981408849358559, 0.003062622155994177, 0.36570248007774353, 0.10830341279506683, 0.12727180123329163, 0.060168564319610596, 0.30900126695632935, 0.029574377462267876, 0.9929803013801575, 0.988093912601471, 0.6772910356521606, 0.02257636748254299, 0.03160691633820534, 0.08804783225059509, 0.051925648003816605, 0.12642766535282135, 0.9908248782157898, 0.10488162934780121, 0.018083039671182632, 0.0542491152882576, 0.02169964648783207, 0.007233215495944023, 0.7920371294021606, 0.9957835078239441, 0.9873079657554626, 0.9995350241661072, 0.12981627881526947, 0.09461566805839539, 0.20089012384414673, 0.15582437813282013, 0.3291369676589966, 0.0894588828086853, 0.9591132998466492, 0.038364533334970474, 0.0152913648635149, 0.9786473512649536, 0.08361674100160599, 0.02351168543100357, 0.2678210735321045, 0.12286181002855301, 0.03022930957376957, 0.47200149297714233, 0.774609386920929, 0.012198572978377342, 0.12808501720428467, 0.0792907252907753, 0.006099286489188671, 0.0075102089904248714, 0.38677576184272766, 0.0037551044952124357, 0.5801636576652527, 0.015020417980849743, 0.009387761354446411, 0.0036745769903063774, 0.9958103895187378, 0.9953781962394714, 0.0029362188652157784, 0.9849820733070374, 0.029596276581287384, 0.0030616838485002518, 0.23268796503543854, 0.04388413578271866, 0.08980939537286758, 0.601110577583313, 0.01965261623263359, 0.009309133514761925, 0.028961749747395515, 0.026893053203821182, 0.1592896282672882, 0.7561085224151611, 0.8847501277923584, 0.012822465039789677, 0.07693479210138321, 0.008548310026526451, 0.012822465039789677, 0.0042741550132632256, 0.01388483215123415, 0.009256554767489433, 0.006942416075617075, 0.009256554767489433, 0.9256554841995239, 0.03702621906995773, 0.003629386192187667, 0.9944518208503723, 0.013404798693954945, 0.001675599836744368, 0.005026799626648426, 0.016755998134613037, 0.9349846839904785, 0.02680959738790989, 0.004340164829045534, 0.9635165929794312, 0.030381154268980026, 0.9806718826293945, 0.9759423732757568, 0.972790002822876, 0.012799868360161781, 0.013595834374427795, 0.94491046667099, 0.02719166874885559, 0.006797917187213898, 0.11950678378343582, 0.21511220932006836, 0.04325007274746895, 0.4472968280315399, 0.08763831108808517, 0.08763831108808517, 0.01956704631447792, 0.04850997030735016, 0.5976102352142334, 0.10598816722631454, 0.10476522892713547, 0.1239246279001236, 0.9687515497207642, 0.02452535554766655, 0.007203049957752228, 0.031213216483592987, 0.6650816202163696, 0.19208133220672607, 0.014406099915504456, 0.09003812819719315, 0.015874918550252914, 0.00541190430521965, 0.9301259517669678, 0.010643411427736282, 0.035538170486688614, 0.0023451584856957197, 0.019144359976053238, 0.9667901396751404, 0.009572179988026619, 0.009572179988026619, 0.9938225150108337, 0.9825310111045837, 0.43620073795318604, 0.07736039161682129, 0.1332835555076599, 0.07736039161682129, 0.15938104689121246, 0.11650660634040833, 0.9855504631996155, 0.26611998677253723, 0.007003157399594784, 0.721325159072876, 0.003501578699797392, 0.9007071852684021, 0.011258839629590511, 0.09007071703672409, 0.9568952918052673, 0.04485446587204933, 0.1070786640048027, 0.039481520652770996, 0.2332998812198639, 0.07956124097108841, 0.03888331353664398, 0.5012956261634827, 0.9937885999679565, 0.06544080376625061, 0.5095657110214233, 0.0017450880259275436, 0.034029215574264526, 0.38304683566093445, 0.006107808090746403, 0.016642862930893898, 0.9652860164642334, 0.016642862930893898, 0.9831171631813049, 1.000300645828247, 0.22492490708827972, 0.0712667778134346, 0.02016328275203705, 0.010081641376018524, 0.6521779298782349, 0.021206211298704147, 0.010700135491788387, 0.984412431716919, 0.943530797958374, 0.05271121859550476, 0.9955680966377258, 0.9798011779785156, 0.9825071692466736, 0.9942929744720459, 0.9742128849029541, 0.018381375819444656, 0.029285766184329987, 0.013550130650401115, 0.0939767137169838, 0.024914756417274475, 0.6884340643882751, 0.14948853850364685, 0.18214784562587738, 0.2007674127817154, 0.12170174717903137, 0.11387613415718079, 0.2971033751964569, 0.08473248034715652, 0.15466861426830292, 0.12063395231962204, 0.07563257217407227, 0.08886827528476715, 0.4277021884918213, 0.13235700130462646, 0.9720013737678528, 0.9909037947654724, 0.9994224905967712, 0.9842272996902466, 0.9813370704650879, 0.8072426915168762, 0.007081076502799988, 0.0566486120223999, 0.10621614754199982, 0.021243229508399963, 0.014998247846961021, 0.9748861193656921, 0.9930050373077393, 0.018780680373311043, 0.09202533215284348, 0.0131464758887887, 0.8244718313217163, 0.01502454373985529, 0.035683292895555496, 0.021559445187449455, 0.07680551707744598, 0.7316736578941345, 0.08893270790576935, 0.017517048865556717, 0.06333086639642715, 0.9788513779640198, 0.3268972635269165, 0.003795614233240485, 0.26616743206977844, 0.2690141499042511, 0.0431751124560833, 0.0910947397351265, 0.9769628047943115, 0.05514483153820038, 0.36507275700569153, 0.08516037464141846, 0.06701143831014633, 0.40067258477211, 0.02722339704632759, 0.9995278120040894, 0.0036586131900548935, 0.09329463541507721, 0.014634452760219574, 0.036586131900548935, 0.8487982749938965, 0.0018293065950274467, 0.027070583775639534, 0.0012890753569081426, 0.032226886600255966, 0.025781508535146713, 0.7760233879089355, 0.13793106377124786, 0.9883468151092529, 0.008101203478872776, 0.0035489341244101524, 0.035489339381456375, 0.049685075879096985, 0.8481952548027039, 0.0638808161020279, 0.008640581741929054, 0.007975921966135502, 0.011963882483541965, 0.0438675694167614, 0.9218835830688477, 0.004652620758861303, 0.07172221690416336, 0.0731377825140953, 0.04529824107885361, 0.20384208858013153, 0.5733058452606201, 0.03302996605634689, 0.09476619213819504, 0.08051563799381256, 0.03491386026144028, 0.1496308296918869, 0.6049360632896423, 0.03562638908624649, 0.09716232866048813, 0.14679276943206787, 0.1371394842863083, 0.24626348912715912, 0.329365611076355, 0.04333481937646866, 0.005860420875251293, 0.8966443538665771, 0.011720841750502586, 0.005860420875251293, 0.07618546485900879, 0.005860420875251293, 0.004913722164928913, 0.009827444329857826, 0.9778307676315308, 0.9765896201133728, 0.006571099627763033, 0.013142199255526066, 0.0722820907831192, 0.9068117141723633, 0.9824222326278687, 0.009871638379991055, 0.008637683466076851, 0.6626337170600891, 0.26406633853912354, 0.008637683466076851, 0.04565632715821266, 0.011162270791828632, 0.01913532055914402, 0.09886582940816879, 0.1419202983379364, 0.022324541583657265, 0.7096014618873596, 0.11289548128843307, 0.8851006031036377, 0.20400801301002502, 0.41825687885284424, 0.18541283905506134, 0.0733027532696724, 0.07788417488336563, 0.040963299572467804, 0.9849153757095337, 0.011712032370269299, 0.9838107228279114, 0.007914591580629349, 0.9576655626296997, 0.015829183161258698, 0.007914591580629349, 0.015829183161258698, 0.0072654481045901775, 0.002421816112473607, 0.024218160659074783, 0.004843632224947214, 0.1646834909915924, 0.7943556904792786, 0.032398875802755356, 0.009999653324484825, 0.504782497882843, 0.27079060673713684, 0.014799486845731735, 0.16679421067237854, 0.01917906105518341, 0.9781320691108704, 0.9885444045066833, 0.009694430977106094, 0.9015820622444153, 0.009694430977106094, 0.06786101311445236, 0.4514913558959961, 0.019630057737231255, 0.11778035014867783, 0.18648555874824524, 0.03680635988712311, 0.1877124309539795, 0.6120343208312988, 0.04446403309702873, 0.10592902451753616, 0.1700095385313034, 0.0680038183927536, 0.9834502339363098, 0.03798385336995125, 0.7755036950111389, 0.022157248109579086, 0.08862899243831635, 0.060141101479530334, 0.018991926684975624, 0.9917678833007812, 0.9895761013031006, 0.1316046118736267, 0.11831486970186234, 0.42335832118988037, 0.04023119434714317, 0.20156964659690857, 0.08496124297380447, 0.021820124238729477, 0.9455386996269226, 0.01454674918204546, 0.00727337459102273, 0.01454674918204546, 0.004573734477162361, 0.027442405000329018, 0.004573734477162361, 0.13263829052448273, 0.8324196338653564, 0.01763540878891945, 0.9875828623771667, 0.021739007905125618, 0.0038362955674529076, 0.023017773404717445, 0.010230121202766895, 0.24424415826797485, 0.696927011013031, 0.9925557374954224, 0.8179852366447449, 0.1650979369878769, 0.015008903108537197, 0.981225311756134, 0.037140924483537674, 0.05526173859834671, 0.056790541857481, 0.07032494246959686, 0.5823391079902649, 0.19815987348556519, 0.9674128890037537, 0.006765125319361687, 0.02029537595808506, 0.8934218883514404, 0.013959717005491257, 0.027919434010982513, 0.05583886802196503, 0.006979858502745628, 0.02903883531689644, 0.009679611772298813, 0.051624596118927, 0.02903883531689644, 0.061304207891225815, 0.8195405006408691, 0.9270946979522705, 0.07516983896493912, 0.013263227418065071, 0.0037894933484494686, 0.04926341399550438, 0.039789680391550064, 0.21600112318992615, 0.6783193349838257, 0.9728258848190308, 0.024944253265857697, 0.046359747648239136, 0.9470633864402771, 0.9925038814544678, 0.012456458061933517, 0.006228229030966759, 0.006228229030966759, 0.9716037511825562, 0.9944402575492859, 0.09686609357595444, 0.023979879915714264, 0.06941544264554977, 0.026819603517651558, 0.7719838619232178, 0.01093819085508585, 0.9973989725112915, 0.05145920068025589, 0.09122312813997269, 0.014326709322631359, 0.24267691373825073, 0.574530303478241, 0.026021981611847878, 0.9726678133010864, 0.10995970666408539, 0.14754678308963776, 0.18303272128105164, 0.09221673756837845, 0.273381769657135, 0.19377188384532928, 0.02883884869515896, 0.019044524058699608, 0.5887478590011597, 0.01686800643801689, 0.3406248986721039, 0.005985421594232321, 0.19181780517101288, 0.0647096186876297, 0.053732097148895264, 0.1496409922838211, 0.07048726826906204, 0.4703002870082855, 0.008398370817303658, 0.008398370817303658, 0.916822075843811, 0.04339158162474632, 0.004199185408651829, 0.01819646917283535, 0.014166206121444702, 0.007083103060722351, 0.9349696040153503, 0.007083103060722351, 0.035415515303611755, 0.995536744594574, 0.012311557307839394, 0.9633793234825134, 0.006155778653919697, 0.0030778893269598484, 0.015389446169137955, 0.992534339427948, 0.009363532066345215, 0.08421996235847473, 0.06877963989973068, 0.18949492275714874, 0.13447120785713196, 0.13250607252120972, 0.39049988985061646, 0.9910913705825806, 0.02209511213004589, 0.03629910945892334, 0.7543902397155762, 0.09311511367559433, 0.05050311237573624, 0.04576844349503517, 0.9963180422782898, 0.021156318485736847, 0.666424036026001, 0.017630266025662422, 0.18864384293556213, 0.02203783206641674, 0.08374376595020294, 0.9986599683761597, 0.04156215861439705, 0.07846648246049881, 0.6542456746101379, 0.14403437077999115, 0.02723037824034691, 0.05446075648069382, 0.019218729808926582, 0.03400236740708351, 0.7761410474777222, 0.11531238257884979, 0.010348547250032425, 0.04730764403939247, 0.1867070496082306, 0.11929992586374283, 0.22790029644966125, 0.11689252406358719, 0.18617208302021027, 0.16290056705474854, 0.039269573986530304, 0.04248029738664627, 0.1264529675245285, 0.08051498234272003, 0.2511771023273468, 0.4603678584098816, 0.012090569362044334, 0.9551549553871155, 0.012090569362044334, 0.012090569362044334, 0.979169487953186, 0.9270894527435303, 0.06079275161027908, 0.007599093951284885, 0.3234667479991913, 0.5710023641586304, 0.0015186233213171363, 0.057707685977220535, 0.044040076434612274, 0.0030372466426342726, 0.5803971886634827, 0.0677766427397728, 0.01909201219677925, 0.021001214161515236, 0.2787433862686157, 0.033411022275686264, 0.03483514115214348, 0.9666751623153687, 0.06354548782110214, 0.012220286764204502, 0.7845423817634583, 0.03666086122393608, 0.02688463032245636, 0.07332172244787216, 0.12485894560813904, 0.24882730841636658, 0.1877337098121643, 0.11826867610216141, 0.21071060001850128, 0.10954101383686066, 0.012104551307857037, 0.06052275374531746, 0.9199458956718445, 0.9947172999382019, 0.08926813304424286, 0.060702331364154816, 0.7712766528129578, 0.06784377992153168, 0.014282901771366596, 0.12738542258739471, 0.8158733248710632, 0.006065972615033388, 0.051560770720243454, 0.0030918593984097242, 0.7698730230331421, 0.21952202916145325, 0.0030918593984097242, 0.0061837187968194485, 0.9909130334854126, 0.07531388103961945, 0.10583581775426865, 0.2699407935142517, 0.039638884365558624, 0.04796304926276207, 0.46139660477638245, 0.0033687311224639416, 0.057268429547548294, 0.0028072758577764034, 0.9196636080741882, 0.0033687311224639416, 0.013474924489855766, 0.012129883281886578, 0.9582607746124268, 0.03032470867037773, 0.9786607027053833, 0.1259913146495819, 0.4249430000782013, 0.039035093039274216, 0.0529988668859005, 0.2672158479690552, 0.08917773514986038, 0.010862999595701694, 0.8581770062446594, 0.02172599919140339, 0.01629449985921383, 0.005431499797850847, 0.08690399676561356, 0.9860461354255676, 0.0260440856218338, 0.02059299871325493, 0.04300302639603615, 0.04966546595096588, 0.44153809547424316, 0.41912809014320374, 0.013552411459386349, 0.9351164102554321, 0.010164308361709118, 0.003388102864846587, 0.03726913034915924, 0.9978386759757996, 0.9780248403549194, 0.013774998486042023, 0.004885334987193346, 0.8256216645240784, 0.004885334987193346, 0.08793603628873825, 0.009770669974386692, 0.06350935995578766, 0.06515597552061081, 0.07815107703208923, 0.4372308850288391, 0.20981667935848236, 0.07724864035844803, 0.13238756358623505, 0.9793744087219238, 0.9946184754371643, 0.13263718783855438, 0.11349129676818848, 0.24197635054588318, 0.14394018054008484, 0.26066088676452637, 0.10703244060277939, 0.2163005769252777, 0.09458304196596146, 0.32452839612960815, 0.07783719152212143, 0.20498161017894745, 0.08171354979276657, 0.975860059261322, 0.005270821042358875, 0.005270821042358875, 0.9540186524391174, 0.0316249281167984, 0.9942108392715454, 0.994229257106781, 0.8328289985656738, 0.027994252741336823, 0.006998563185334206, 0.13297270238399506, 0.9655805826187134, 0.007818466052412987, 0.027364632114768028, 0.0171099491417408, 0.19676442444324493, 0.7656702399253845, 0.0042774872854352, 0.0085549745708704, 0.024220136925578117, 0.5985833406448364, 0.06401035934686661, 0.05363029986619949, 0.013840077444911003, 0.24220135807991028, 0.9719939231872559, 0.9868683815002441, 0.06381504982709885, 0.8859030604362488, 0.01876913197338581, 0.007507652975618839, 0.01876913197338581, 0.007507652975618839, 0.9881502389907837, 0.041502539068460464, 0.006917089689522982, 0.8646361827850342, 0.08300507813692093, 0.006917089689522982, 0.9822061061859131, 0.0018241588259115815, 0.23896481096744537, 0.009120794013142586, 0.6858837008476257, 0.06384555995464325, 0.0018241588259115815, 0.005056329537183046, 0.9303646087646484, 0.06067595258355141, 0.32969239354133606, 0.6689134240150452, 0.159809872508049, 0.08192785084247589, 0.1775103360414505, 0.09052521735429764, 0.026297828182578087, 0.4637520909309387, 0.12125581502914429, 0.0148298479616642, 0.5740023255348206, 0.11514940857887268, 0.12387284636497498, 0.050595950335264206, 0.0068996623158454895, 0.05260992422699928, 0.0017249155789613724, 0.9098929762840271, 0.008624577894806862, 0.019836528226733208, 0.9823579788208008, 0.12843935191631317, 0.041340429335832596, 0.09183255583047867, 0.028717398643493652, 0.6598690152168274, 0.04986097663640976, 0.9973321557044983, 0.020731307566165924, 0.9743714332580566, 0.14597220718860626, 0.12397439777851105, 0.20220501720905304, 0.11275697499513626, 0.14276723563671112, 0.27227750420570374, 0.9808676838874817, 0.014639816246926785, 0.0011587007902562618, 0.11007657647132874, 0.0011587007902562618, 0.8099318146705627, 0.0023174015805125237, 0.07531554996967316, 0.9928392171859741, 0.005117727909237146, 0.10908007621765137, 0.8908206224441528, 0.9821460247039795, 0.9767343997955322, 0.9962611198425293, 0.008895188570022583, 0.06446751952171326, 0.9347790479660034, 0.9947971105575562, 0.19451920688152313, 0.04247565567493439, 0.09557022899389267, 0.11632537841796875, 0.1629038006067276, 0.38807305693626404, 0.03241059184074402, 0.030384929850697517, 0.10803530365228653, 0.1944635510444641, 0.15192465484142303, 0.482107549905777, 0.9981397986412048, 0.9770315289497375, 0.990780234336853, 0.0026742490008473396, 0.04011373221874237, 0.0026742490008473396, 0.0026742490008473396, 0.949358344078064, 0.026649512350559235, 0.9682655930519104, 1.0004940032958984, 0.6800112128257751, 0.12870875000953674, 0.017161166295409203, 0.12656359374523163, 0.015016020275652409, 0.030032040551304817, 0.05195282772183418, 0.0037109162658452988, 0.9388618469238281, 0.01188487745821476, 0.9745599031448364, 0.01188487745821476, 0.9959378242492676, 0.05987735092639923, 0.9031500220298767, 0.0049897790886461735, 0.014969337731599808, 0.014969337731599808, 0.20492258667945862, 0.09418967366218567, 0.23213885724544525, 0.18624474108219147, 0.1206054762005806, 0.161963552236557, 0.04953290894627571, 0.19480480253696442, 0.13529138267040253, 0.2620808482170105, 0.051750797778367996, 0.30680832266807556, 0.03597182780504227, 0.9592487215995789, 0.9930501580238342, 0.9930062890052795, 0.6570551991462708, 0.008478131145238876, 0.06358598172664642, 0.0727706253528595, 0.0346190370619297, 0.16320402920246124, 0.9394206404685974, 0.013815009035170078, 0.013815009035170078, 0.013815009035170078, 0.013815009035170078, 0.9945866465568542, 0.988961398601532, 0.005320487543940544, 0.9896106719970703, 0.005320487543940544, 0.02011849731206894, 0.9757471084594727, 0.983714759349823, 0.012296434491872787, 0.06576428562402725, 0.0968082994222641, 0.5657362937927246, 0.11804893612861633, 0.06658123433589935, 0.08741340041160583, 0.02298586443066597, 0.03218020871281624, 0.8826571702957153, 0.013791518285870552, 0.018388692289590836, 0.027583036571741104, 0.02595379389822483, 0.02487238682806492, 0.7158921360969543, 0.1665368527173996, 0.03352365270256996, 0.03244224190711975, 0.9911356568336487, 0.010113629512488842, 0.09009287506341934, 0.04520906135439873, 0.20100143551826477, 0.15774384140968323, 0.18343819677829742, 0.32231783866882324, 0.09617782384157181, 0.009063879027962685, 0.4612507224082947, 0.19084499776363373, 0.08056780695915222, 0.16113561391830444, 0.4997963011264801, 0.02587941847741604, 0.0242619551718235, 0.00646985461935401, 0.27335137128829956, 0.1698336899280548, 0.031526993960142136, 0.9647259712219238, 0.9817258715629578, 0.012686937116086483, 0.008457958698272705, 0.9599782824516296, 0.0042289793491363525, 0.0042289793491363525, 0.012686937116086483, 0.9946584105491638, 0.9982418417930603, 0.9937907457351685, 0.06563172489404678, 0.022651657462120056, 0.01858597621321678, 0.36939626932144165, 0.2532339096069336, 0.2706582844257355, 0.10219186544418335, 0.15171560645103455, 0.21538899838924408, 0.06092207133769989, 0.41387704014778137, 0.05581247806549072, 0.9501718282699585, 0.01195184700191021, 0.03585554286837578, 0.017030898481607437, 0.08809085935354233, 0.46335792541503906, 0.14388173818588257, 0.08515449613332748, 0.20260897278785706, 0.0010063471272587776, 0.08553951233625412, 0.6712335348129272, 0.12679974734783173, 0.06843160837888718, 0.04528562352061272, 0.9866925477981567, 0.1330176740884781, 0.37259647250175476, 0.07716494798660278, 0.19695433974266052, 0.09406774491071701, 0.1264035403728485, 0.005518755875527859, 0.0331125371158123, 0.9326697587966919, 0.01655626855790615, 0.011037511751055717, 0.08639536798000336, 0.035343561321496964, 0.29543590545654297, 0.03594772517681122, 0.43348726630210876, 0.11328064650297165, 0.9901833534240723, 0.1940285712480545, 0.18161073327064514, 0.1487552374601364, 0.13090461492538452, 0.10684506595134735, 0.2374909669160843, 0.011360417120158672, 0.9769958257675171, 0.5636518001556396, 0.07272926717996597, 0.16136805713176727, 0.09545715898275375, 0.04318300262093544, 0.0636381059885025, 0.9915980696678162, 0.012229962274432182, 0.010701216757297516, 0.10854091495275497, 0.00917247124016285, 0.00764372618868947, 0.8530398607254028, 0.9850125312805176, 0.005410763435065746, 0.005410763435065746, 0.9360620975494385, 0.021643053740262985, 0.005410763435065746, 0.021643053740262985, 0.09212269634008408, 0.4448710083961487, 0.025654422119259834, 0.11252962797880173, 0.22505925595760345, 0.10028547048568726, 0.9378381371498108, 0.05275339633226395, 0.14849179983139038, 0.11695220321416855, 0.05898755416274071, 0.17696265876293182, 0.47769689559936523, 0.02062859572470188, 0.004065255634486675, 0.0020326278172433376, 0.03455467149615288, 0.8394752740859985, 0.03861992806196213, 0.0813051089644432, 0.4214683175086975, 0.07751674950122833, 0.0566290058195591, 0.10583125054836273, 0.20702078938484192, 0.13182489573955536, 0.01840338669717312, 0.07207993417978287, 0.003067231038585305, 0.003067231038585305, 0.9002323150634766, 0.003067231038585305, 0.9887916445732117, 0.11985446512699127, 0.251277893781662, 0.1092110201716423, 0.1085168793797493, 0.1975979059934616, 0.21356306970119476, 0.010916311293840408, 0.9933843612670898, 0.011107955127954483, 0.9886080026626587, 0.99562007188797, 0.003446170361712575, 0.01895393617451191, 0.015507766045629978, 0.003446170361712575, 0.9201274514198303, 0.03618478775024414, 1.00130295753479, 0.9658583402633667, 0.025082901120185852, 0.17166109383106232, 0.02037985622882843, 0.15441660583019257, 0.6270725131034851, 0.0015676813200116158, 0.07192934304475784, 0.15565960109233856, 0.005057531874626875, 0.02865934930741787, 0.7271607518196106, 0.012362856417894363, 0.07356557250022888, 0.15624089539051056, 0.06634651869535446, 0.1686164140701294, 0.4821294844150543, 0.05328347533941269, 0.9765871167182922, 0.9794719219207764, 0.9922530651092529, 0.9921807050704956, 0.9938414692878723, 0.9957695007324219, 0.09247474372386932, 0.03336717560887337, 0.07626783102750778, 0.19638966023921967, 0.05815422162413597, 0.5434082746505737, 0.9860760569572449, 0.012642000801861286, 0.13855727016925812, 0.17292383313179016, 0.11973748356103897, 0.09600818902254105, 0.3060261011123657, 0.16637782752513885, 0.0518195815384388, 0.5418664813041687, 0.003198739606887102, 0.2558991611003876, 0.13882529735565186, 0.00767697487026453, 0.0227680541574955, 0.9410795569419861, 0.03035740554332733, 0.9844658970832825, 0.012540966272354126, 0.99298495054245, 0.006284714676439762, 0.9997525215148926, 0.07180345058441162, 0.16053199768066406, 0.1061665266752243, 0.2918297350406647, 0.07436785846948624, 0.2954199016094208, 0.03198070079088211, 0.05116911977529526, 0.12920202314853668, 0.06396140158176422, 0.044772978872060776, 0.6779907941818237, 0.9901297688484192, 0.9838689565658569, 0.008267806842923164, 0.008312680758535862, 0.9892090559005737, 0.9550564289093018, 0.028509147465229034, 0.984309732913971, 0.014929805882275105, 0.007464902941137552, 0.9629724621772766, 0.007464902941137552, 0.0024883009027689695, 0.0024883009027689695, 0.9712203741073608, 0.9717133045196533, 0.08051097393035889, 0.03699152544140816, 0.004351944196969271, 0.004351944196969271, 0.8703888654708862, 0.0021759720984846354, 0.9778783321380615, 0.016030792146921158, 0.9660969376564026, 0.01971626468002796, 0.03417487442493439, 0.9568964838981628, 0.9976550936698914, 0.9230712652206421, 0.053050071001052856, 0.010610014200210571, 0.9956661462783813, 0.9921589493751526, 0.012548916973173618, 0.028683237731456757, 0.0035854047164320946, 0.017927024513483047, 0.9214490056037903, 0.016134321689605713, 0.974500834941864, 0.09689048677682877, 0.16852760314941406, 0.14997410774230957, 0.12781298160552979, 0.2801062762737274, 0.17677360773086548, 0.8644028902053833, 0.012437451630830765, 0.07462470978498459, 0.043531082570552826, 0.9687949419021606, 0.018279150128364563, 0.9935377240180969], \"Term\": [\"able_capture\", \"able_generate\", \"abnormal\", \"acceptance\", \"acceptance\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action\", \"action_space\", \"activation_function\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"activity\", \"adam\", \"adam\", \"admm\", \"admm\", \"admm\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial_example\", \"adversarial_example\", \"adversarial_example\", \"adversary\", \"adversary\", \"adversary\", \"adversary\", \"adversary\", \"advertising\", \"advertising\", \"age\", \"age\", \"age\", \"age\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent_learn\", \"aim_learn\", \"aka\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"algorithm\", \"alternating_direction\", \"alternating_direction\", \"alternating_direction\", \"alternating_direction\", \"alternating_direction\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"ann\", \"api\", \"api\", \"app\", \"application\", \"application\", \"application\", \"application\", \"application\", \"application\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approach\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"architectural\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"artist\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"assumption\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"asymptotic\", \"attacker\", \"attention_mechanism\", \"attention_mechanism\", \"attentive\", \"attributed\", \"auction\", \"authentication\", \"automation\", \"bandit\", \"bandit\", \"bandit\", \"bart\", \"based\", \"based\", \"based\", \"based\", \"based\", \"based\", \"batch_size\", \"batch_size\", \"bidirectional\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"breakthrough\", \"bridging\", \"browsing\", \"capsule\", \"car\", \"car\", \"car\", \"card\", \"cardiac\", \"cardiac\", \"carefully_designed\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"catastrophic_forgetting\", \"catastrophic_forgetting\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"channel\", \"circuit\", \"city\", \"class\", \"class\", \"class\", \"class\", \"class\", \"class\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classification\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"client\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"cnn\", \"cnn\", \"cnn\", \"cnn\", \"cnns\", \"cnns\", \"cnns\", \"code_available\", \"code_available\", \"coded\", \"cold_start\", \"collective\", \"collective\", \"collective\", \"command\", \"community_detection\", \"community_detection\", \"community_detection\", \"company\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"concatenation\", \"concept_drift\", \"conditional_expectation\", \"confusion_matrix\", \"constellation\", \"constrains\", \"context_aware\", \"continuous_control\", \"continuous_relaxation\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex_concave\", \"convolutional_layer\", \"convolutional_layer\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"covariance\", \"cps\", \"critic\", \"cross_domain\", \"curiosity\", \"curriculum\", \"curriculum_learning\", \"curriculum_learning\", \"custom\", \"customer\", \"customer\", \"customer\", \"cyber_attack\", \"d_d\", \"data\", \"data\", \"data\", \"data\", \"data\", \"data\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"decision_tree\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep_generative\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_neural\", \"deep_neural\", \"deep_neural\", \"deep_neural\", \"deep_neural\", \"deep_neural\", \"deep_reinforcement\", \"deep_reinforcement\", \"deep_reinforcement\", \"deep_reinforcement\", \"defensive\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"demonstration\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"detection\", \"deterministic_policy\", \"detrimental\", \"device\", \"device\", \"device\", \"device\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"dictionary\", \"different\", \"different\", \"different\", \"different\", \"different\", \"different\", \"differentiability\", \"differential_equation\", \"differential_equation\", \"differentiate\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"dimension\", \"discontinuous\", \"discriminator\", \"discriminator\", \"discriminator\", \"discriminator\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distributed\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"distribution\", \"dnn\", \"dnn\", \"dnn\", \"dnn\", \"dnns\", \"dnns\", \"dnns\", \"dnns\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"domain_adaptation\", \"domain_adaptation\", \"domain_adaptation\", \"domain_adaptation\", \"domain_adaptation\", \"downstream_task\", \"dqn\", \"dqn\", \"dqn\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"edge\", \"eeg\", \"eeg\", \"eeg_signal\", \"embeddings\", \"embeddings\", \"embeddings\", \"embeddings\", \"embeddings\", \"encoders\", \"end_end\", \"end_end\", \"energy_consumption\", \"energy_consumption\", \"engine\", \"engine\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"exceptional\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"experience_replay\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"expert\", \"exploration\", \"exploration\", \"exploration\", \"exploration\", \"exploration\", \"exploration\", \"exposure\", \"extract_useful\", \"extracted_feature\", \"fairness\", \"fairness\", \"fairness\", \"fairness\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feature\", \"feed\", \"feedforward_neural\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"filter\", \"finger\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sample\", \"finite_sum\", \"fixed_length\", \"fragment\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"framework\", \"fraud\", \"fraud\", \"fully_automated\", \"fully_automated\", \"function\", \"function\", \"function\", \"function\", \"function\", \"function\", \"fusion\", \"fusion\", \"fusion\", \"fusion\", \"fusion\", \"game\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gan\", \"gan\", \"gans\", \"gans\", \"gas\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative\", \"generative_adversarial\", \"generative_adversarial\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generator\", \"generator\", \"generator\", \"generator_discriminator\", \"geyser\", \"gnn\", \"gnn\", \"gnns\", \"gnns\", \"gnns\", \"gnns\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient_boosting\", \"gradient_boosting\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph\", \"graph_laplacian\", \"graph_laplacian\", \"graph_laplacian\", \"graph_laplacian\", \"graph_topology\", \"grasp\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"hamming_distance\", \"hardware\", \"hardware\", \"hardware\", \"hardware\", \"health\", \"health\", \"health\", \"high_confidence\", \"high_confidence\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"high_dimensional\", \"hint\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hypergraph\", \"hypergraph\", \"hypergraph\", \"hypergraphs\", \"id\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image_generation\", \"image_generation\", \"imagenet\", \"imagenet\", \"imitation_learning\", \"importance_weighted\", \"inception\", \"incur\", \"indoor\", \"indoor\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"information\", \"information\", \"information\", \"information\", \"information\", \"information\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"interaction_environment\", \"intermediate_representation\", \"intrusion\", \"intrusion_detection\", \"invalid\", \"invariance\", \"invariance\", \"invariance\", \"invariance\", \"invariance\", \"inverse_reinforcement\", \"inverse_reinforcement\", \"irl\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"journey\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"knockoff\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"lagrangian\", \"language\", \"language\", \"language\", \"language\", \"language\", \"language\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent_representation\", \"latent_representation\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learned\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"learning\", \"library\", \"library\", \"library\", \"library\", \"library\", \"library\", \"linear_convergence\", \"linear_convergence\", \"linear_convergence\", \"linearized\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"lipschitz\", \"look_like\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"lower_bound\", \"lower_bound\", \"lower_bound\", \"lower_bound\", \"lower_bound\", \"lower_bound\", \"lstm\", \"lstm\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"machine_learning\", \"macro\", \"malware\", \"malware\", \"market\", \"market\", \"market\", \"market\", \"market\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"matrix\", \"maze\", \"maze\", \"mcts\", \"mdp\", \"mdp\", \"mdp\", \"mdp\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"measurement\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"mental\", \"meta\", \"meta\", \"meta\", \"meta\", \"meta\", \"meta\", \"meta_path\", \"metadata\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method\", \"method_multiplier\", \"method_multiplier\", \"method_multiplier\", \"method_multiplier\", \"method_multiplier\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing\", \"mixing_time\", \"mixing_time\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mixture\", \"mnist_cifar\", \"mobile\", \"mobile\", \"mobile\", \"mobility\", \"model\", \"model\", \"model\", \"model\", \"model\", \"model\", \"molecular\", \"molecular\", \"molecular\", \"molecule\", \"molecule\", \"molecule\", \"molecule\", \"molecule\", \"moment\", \"moment\", \"moment\", \"moment\", \"moment\", \"moment\", \"momentum\", \"momentum\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"mtl\", \"mtl\", \"multi_agent\", \"multi_agent\", \"multitask\", \"music\", \"music\", \"music\", \"music\", \"nested_dichotomy\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network\", \"network_gans\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neutral\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_smooth\", \"non_smooth\", \"non_smooth\", \"non_smooth\", \"non_smooth\", \"non_stationarity\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex_optimization\", \"nonconvex_optimization\", \"number\", \"number\", \"number\", \"number\", \"number\", \"number\", \"nystr_method\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"objective_function\", \"oja\", \"online\", \"online\", \"online\", \"online\", \"online\", \"online\", \"optimal_transport\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"parameter\", \"participant\", \"participant\", \"participant\", \"participant\", \"pascal_voc\", \"pathway\", \"pathway\", \"pathway\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pde\", \"pde\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"penalty\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"performance\", \"personalized\", \"personalized\", \"personalized\", \"pitfall\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"platform\", \"platform\", \"platform\", \"platform\", \"player\", \"player\", \"player\", \"player\", \"player\", \"poi\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy_gradient\", \"policy_gradient\", \"policy_gradient\", \"posted\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"prediction\", \"price\", \"price\", \"price\", \"price\", \"price\", \"price\", \"primal_dual\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"privacy_guarantee\", \"privacy_preserving\", \"privacy_preserving\", \"private\", \"private\", \"private\", \"private\", \"private\", \"private\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"problem\", \"prognostic\", \"programming_language\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"propose\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"proposed\", \"provider\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"pruned\", \"public_health\", \"quantification\", \"quantification\", \"quantification\", \"quantification\", \"quantization\", \"quantization\", \"quantization\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"quantum\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query\", \"query_complexity\", \"radiation\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_forest\", \"random_fourier\", \"rating\", \"rating\", \"rating\", \"rating\", \"rating\", \"realize\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommender_system\", \"recommender_system\", \"recommender_system\", \"recurrent_neural\", \"recurrent_neural\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"relative_importance\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representational\", \"resilient\", \"resilient\", \"result\", \"result\", \"result\", \"result\", \"result\", \"result\", \"revenue\", \"revenue\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward_function\", \"reward_function\", \"road\", \"road\", \"road_segment\", \"rotation_forest\", \"saddle_point\", \"saddle_point\", \"safety\", \"safety\", \"saliency\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sample\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling\", \"sampling_strategy\", \"saturation\", \"sda\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic_segmentation\", \"semantic_segmentation\", \"semidefinite_programming\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sensor\", \"sentence\", \"sentence\", \"sentence\", \"sentiment\", \"sentiment\", \"sentiment\", \"sequence_sequence\", \"service\", \"service\", \"service\", \"service\", \"service\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"shallow\", \"shallow\", \"shifting\", \"sigmoid\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signature\", \"signature\", \"signature\", \"signature\", \"signature\", \"sinkhorn\", \"sketched\", \"skill\", \"skill\", \"skill\", \"smart\", \"smart\", \"softmax\", \"softmax\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solver\", \"solver\", \"solver\", \"solver\", \"solver\", \"solver\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"source_target\", \"source_target\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"speaker\", \"speaker\", \"specifically_designed\", \"spectral_clustering\", \"spectral_clustering\", \"spectral_clustering\", \"spectral_clustering\", \"spectral_clustering\", \"spectral_clustering\", \"spn\", \"stabilize\", \"stack_overflow\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"stationary_point\", \"stationary_point\", \"stationary_point\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"strain\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strongly_convex\", \"strongly_convex\", \"strongly_convex\", \"strongly_convex\", \"strongly_convex\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"struggle\", \"study\", \"study\", \"study\", \"study\", \"study\", \"study\", \"subgraph\", \"subgraph\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subject\", \"subjected\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"subspace\", \"sum_square\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"svd\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"target_domain\", \"target_domain\", \"task\", \"task\", \"task\", \"task\", \"task\", \"task\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"thread\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time_horizon\", \"time_horizon\", \"title\", \"title\", \"token\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"tracker\", \"traffic_flow\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"training\", \"training\", \"training\", \"training\", \"training\", \"training\", \"transfer_knowledge\", \"transferable\", \"transportation\", \"tree_search\", \"tweet\", \"united_state\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"unknown\", \"urban\", \"urban\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user_item\", \"user_item\", \"user_item\", \"vae\", \"vae\", \"vaes\", \"vaes\", \"valley\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variance\", \"variance\", \"variance\", \"variance\", \"variance\", \"variance\", \"variational_auto\", \"variational_autoencoder\", \"variational_autoencoder\", \"variational_autoencoders\", \"variational_autoencoders\", \"velocity\", \"velocity\", \"verify_effectiveness\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"vertex\", \"video_game\", \"visible\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"visual\", \"vital\", \"vital\", \"voltage\", \"voltage\", \"warping\", \"warping\", \"wasserstein_distance\", \"water\", \"water\", \"water\", \"white_box\", \"winning\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"wordvec\", \"work\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worker\", \"worker\", \"worker\", \"worker\", \"workload\", \"workload\", \"wrong\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el55821401225507994803851409838\", ldavis_el55821401225507994803851409838_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el55821401225507994803851409838\", ldavis_el55821401225507994803851409838_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el55821401225507994803851409838\", ldavis_el55821401225507994803851409838_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0      0.043200  0.052050       1        1  13.913700\n",
       "1      0.106708 -0.089868       2        1  14.708337\n",
       "2     -0.104124  0.002116       3        1  18.489582\n",
       "3     -0.037187 -0.083941       4        1  11.907636\n",
       "4      0.091862  0.085582       5        1  23.139009\n",
       "5     -0.100459  0.034061       6        1  17.841734, topic_info=     Category          Freq                    Term         Total  loglift  \\\n",
       "term                                                                         \n",
       "346   Default   5543.000000                   graph   5543.000000  30.0000   \n",
       "439   Default   9507.000000                 network   9507.000000  29.0000   \n",
       "82    Default  22239.000000                   model  22239.000000  28.0000   \n",
       "101   Default  11081.000000                 problem  11081.000000  27.0000   \n",
       "0     Default  14962.000000               algorithm  14962.000000  26.0000   \n",
       "1951  Default   1781.000000                  policy   1781.000000  25.0000   \n",
       "193   Default  19338.000000                  method  19338.000000  24.0000   \n",
       "1358  Default   1466.000000                   agent   1466.000000  23.0000   \n",
       "230   Default   5656.000000                function   5656.000000  22.0000   \n",
       "342   Default   4629.000000            distribution   4629.000000  21.0000   \n",
       "544   Default   9530.000000                learning   9530.000000  20.0000   \n",
       "3227  Default   1159.000000  reinforcement_learning   1159.000000  19.0000   \n",
       "7346  Default   2359.000000           deep_learning   2359.000000  18.0000   \n",
       "2121  Default   3420.000000          neural_network   3420.000000  17.0000   \n",
       "135   Default   2131.000000              classifier   2131.000000  16.0000   \n",
       "88    Default   2791.000000            optimization   2791.000000  15.0000   \n",
       "118   Default   5865.000000                    task   5865.000000  14.0000   \n",
       "63    Default   4902.000000                 feature   4902.000000  13.0000   \n",
       "353   Default   2500.000000                  matrix   2500.000000  12.0000   \n",
       "983   Default   1883.000000                    deep   1883.000000  11.0000   \n",
       "1152  Default   2876.000000                   image   2876.000000  10.0000   \n",
       "5097  Default   1504.000000                   layer   1504.000000   9.0000   \n",
       "810   Default   3710.000000        machine_learning   3710.000000   8.0000   \n",
       "690   Default   1912.000000                   bound   1912.000000   7.0000   \n",
       "3919  Default    863.000000                  reward    863.000000   6.0000   \n",
       "3243  Default   1563.000000                    user   1563.000000   5.0000   \n",
       "1335  Default   1415.000000                  signal   1415.000000   4.0000   \n",
       "763   Default   1178.000000             environment   1178.000000   3.0000   \n",
       "644   Default   5817.000000                training   5817.000000   2.0000   \n",
       "217   Default  15976.000000                    data  15976.000000   1.0000   \n",
       "...       ...           ...                     ...           ...      ...   \n",
       "864    Topic6    358.056854             monte_carlo    527.775024   1.3357   \n",
       "539    Topic6   1119.350830                   error   2254.961182   1.0232   \n",
       "16     Topic6   1158.278076                estimate   2403.368652   0.9937   \n",
       "618    Topic6   1164.011230                   point   2522.775391   0.9501   \n",
       "180    Topic6    837.913574        high_dimensional   1671.668213   1.0330   \n",
       "623    Topic6   1607.615112                  sample   4143.549805   0.7768   \n",
       "249    Topic6    916.798950              regression   1977.349609   0.9550   \n",
       "85     Topic6   1390.812744                  number   3562.100830   0.7832   \n",
       "243    Topic6    813.528381                   noise   1730.809082   0.9687   \n",
       "1470   Topic6    569.717896                 unknown   1048.935059   1.1132   \n",
       "217    Topic6   3695.272705                    data  15976.334961   0.2596   \n",
       "82     Topic6   4406.603027                   model  22239.619141   0.1049   \n",
       "818    Topic6    714.093994                sampling   1480.997314   0.9942   \n",
       "693    Topic6    900.628052              estimation   2162.364746   0.8478   \n",
       "747    Topic6   1869.050659                  result   6864.320312   0.4227   \n",
       "0      Topic6   2711.502930               algorithm  14962.029297   0.0156   \n",
       "417    Topic6    598.822632              assumption   1233.335205   1.0011   \n",
       "134    Topic6    932.911011                    case   2747.542236   0.6435   \n",
       "33     Topic6    990.890198                   space   3074.604980   0.5913   \n",
       "779    Topic6    691.639771                   prior   1651.046631   0.8535   \n",
       "756    Topic6    949.208923                   class   3321.081543   0.4712   \n",
       "101    Topic6   1467.306763                 problem  11081.101562  -0.2982   \n",
       "447    Topic6    830.407471                 setting   2705.272217   0.5426   \n",
       "193    Topic6   1642.776733                  method  19338.228516  -0.7421   \n",
       "38     Topic6   1253.218384                approach   9942.891602  -0.3475   \n",
       "257    Topic6    918.419495                   study   3865.410156   0.2865   \n",
       "162    Topic6   1225.028198                   based  10110.185547  -0.3869   \n",
       "159    Topic6    900.310974                analysis   3756.220703   0.2952   \n",
       "36     Topic6    923.014893                    time   4321.908203   0.1798   \n",
       "518    Topic6    830.129272                     new   4283.387207   0.0827   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "346   30.0000  \n",
       "439   29.0000  \n",
       "82    28.0000  \n",
       "101   27.0000  \n",
       "0     26.0000  \n",
       "1951  25.0000  \n",
       "193   24.0000  \n",
       "1358  23.0000  \n",
       "230   22.0000  \n",
       "342   21.0000  \n",
       "544   20.0000  \n",
       "3227  19.0000  \n",
       "7346  18.0000  \n",
       "2121  17.0000  \n",
       "135   16.0000  \n",
       "88    15.0000  \n",
       "118   14.0000  \n",
       "63    13.0000  \n",
       "353   12.0000  \n",
       "983   11.0000  \n",
       "1152  10.0000  \n",
       "5097   9.0000  \n",
       "810    8.0000  \n",
       "690    7.0000  \n",
       "3919   6.0000  \n",
       "3243   5.0000  \n",
       "1335   4.0000  \n",
       "763    3.0000  \n",
       "644    2.0000  \n",
       "217    1.0000  \n",
       "...       ...  \n",
       "864   -6.6029  \n",
       "539   -5.4631  \n",
       "16    -5.4289  \n",
       "618   -5.4240  \n",
       "180   -5.7527  \n",
       "623   -5.1011  \n",
       "249   -5.6627  \n",
       "85    -5.2460  \n",
       "243   -5.7823  \n",
       "1470  -6.1385  \n",
       "217   -4.2688  \n",
       "82    -4.0928  \n",
       "818   -5.9126  \n",
       "693   -5.6805  \n",
       "747   -4.9504  \n",
       "0     -4.5784  \n",
       "417   -6.0887  \n",
       "134   -5.6453  \n",
       "33    -5.5850  \n",
       "779   -5.9446  \n",
       "756   -5.6280  \n",
       "101   -5.1924  \n",
       "447   -5.7617  \n",
       "193   -5.0795  \n",
       "38    -5.3502  \n",
       "257   -5.6610  \n",
       "162   -5.3729  \n",
       "159   -5.6809  \n",
       "36    -5.6560  \n",
       "518   -5.7620  \n",
       "\n",
       "[602 rows x 6 columns], token_table=       Topic      Freq                  Term\n",
       "term                                        \n",
       "10804      6  0.990804          able_capture\n",
       "8144       5  0.993956         able_generate\n",
       "7332       5  0.988861              abnormal\n",
       "12103      5  0.028553            acceptance\n",
       "12103      6  0.970788            acceptance\n",
       "570        1  0.233407              accuracy\n",
       "570        2  0.249835              accuracy\n",
       "570        3  0.140660              accuracy\n",
       "570        4  0.066737              accuracy\n",
       "570        5  0.205001              accuracy\n",
       "570        6  0.104383              accuracy\n",
       "1941       1  0.008390                action\n",
       "1941       2  0.194179                action\n",
       "1941       3  0.003596                action\n",
       "1941       4  0.745550                action\n",
       "1941       5  0.026370                action\n",
       "1941       6  0.023973                action\n",
       "7894       5  0.989799          action_space\n",
       "9453       4  0.996799   activation_function\n",
       "1290       1  0.621873              activity\n",
       "1290       2  0.167906              activity\n",
       "1290       3  0.012437              activity\n",
       "1290       4  0.009328              activity\n",
       "1290       5  0.167906              activity\n",
       "1290       6  0.021766              activity\n",
       "1795       3  0.975317                  adam\n",
       "1795       6  0.026967                  adam\n",
       "10032      3  0.980610                  admm\n",
       "10032      4  0.003786                  admm\n",
       "10032      6  0.011358                  admm\n",
       "...      ...       ...                   ...\n",
       "21694      1  0.966097               voltage\n",
       "21694      5  0.019716               voltage\n",
       "10109      4  0.034175               warping\n",
       "10109      6  0.956896               warping\n",
       "8123       6  0.997655  wasserstein_distance\n",
       "15845      1  0.923071                 water\n",
       "15845      5  0.053050                 water\n",
       "15845      6  0.010610                 water\n",
       "22771      2  0.995666             white_box\n",
       "6953       4  0.992159               winning\n",
       "1102       1  0.012549                  word\n",
       "1102       2  0.028683                  word\n",
       "1102       3  0.003585                  word\n",
       "1102       4  0.017927                  word\n",
       "1102       5  0.921449                  word\n",
       "1102       6  0.016134                  word\n",
       "15792      2  0.974501               wordvec\n",
       "261        1  0.096890                  work\n",
       "261        2  0.168528                  work\n",
       "261        3  0.149974                  work\n",
       "261        4  0.127813                  work\n",
       "261        5  0.280106                  work\n",
       "261        6  0.176774                  work\n",
       "7289       2  0.864403                worker\n",
       "7289       3  0.012437                worker\n",
       "7289       4  0.074625                worker\n",
       "7289       6  0.043531                worker\n",
       "12455      2  0.968795              workload\n",
       "12455      5  0.018279              workload\n",
       "8577       3  0.993538                 wrong\n",
       "\n",
       "[1636 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you set the lambda value to be lower, you can see more exclusive terms\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(full_lda, bow, dictionary, sort_topics = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, let us...\n",
    "## Filter the most and least common terms in our corpus, to get a more discriminative model: ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# let's look at the most common words throughout the corpus\n",
    "tot_counts = np.zeros(len(dictionary))\n",
    "\n",
    "# goes through the (id, count) tuples in every document and increments a total count\n",
    "for doc in bow:\n",
    "    for w in doc:\n",
    "        tot_counts[w[0]] += w[1]\n",
    "\n",
    "masterlist = [(dictionary[i], tot_counts[i]) for i in range(len(dictionary))]\n",
    "\n",
    "# the most common\n",
    "reference = sorted(masterlist, key=lambda w: w[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model', 23446.0),\n",
       " ('method', 18755.0),\n",
       " ('algorithm', 17368.0),\n",
       " ('data', 16627.0),\n",
       " ('problem', 11707.0),\n",
       " ('based', 10494.0),\n",
       " ('approach', 10204.0),\n",
       " ('learning', 10175.0),\n",
       " ('network', 9262.0),\n",
       " ('result', 6774.0),\n",
       " ('proposed', 6144.0),\n",
       " ('performance', 5449.0),\n",
       " ('function', 5355.0),\n",
       " ('task', 5323.0),\n",
       " ('feature', 5292.0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[:15]\n",
    "# we can see in just these top 15 there is a steep fall-off in descending term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25533"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ref for ref in reference if ref[1] <=10])\n",
    "# In contrast, there are around 25K words that occur 10 times or less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_ammt = (18/10)*1000 # set this to 25000 to see the whole thing\n",
    "prevalent_counts = [ref[1] for ref in reference if ref[1] <= max_ammt] \n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(prevalent_counts, bins=100);\n",
    "# plt.xlim(0, 5000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By visually inspecting this historgram of word terms, we can see that it is probably a good idea to filter the extreme terms. Words like \"model, algorithm\" on the high end, or \"solomonoff\" and \"bssd\" (which only appear twice) do not contribute to our understanding of a topic. We should instead use the central mass of our distribution as our words. Luckily, gensim makes it easy to do this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to figure out approx. how many words occur more than max_total of the time ( assuming it is a common ML term and not something special)\n",
    "max_total = (1/6) # the maximum document frequency I'd like -I'm going to set it roughly equal to 1/num_topics so I can get better overlap\n",
    "ref_cutoff = len([ref for ref in reference if ref[1] > len(corpus)*max_total])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzgAAAGfCAYAAACTCnf9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAGJJJREFUeJzt3W+MZXd93/HPt17+RECxjbcra9fuOo0VxBOMtQIjIkSxIMauuq5EkKMKVsjVVq2piNKq3eRJkjYPlkoNBaly5GLaJSIxlhPkFXZJLOMo6gMM62AMtkO9uGt5V7Z3A9hJipLI5NsH81sYr/fPzO7MzszPr5d0dc8958zc3/jHGfz2OfdMdXcAAABm8PfWegAAAAArReAAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATGPTWg8gSS655JLevn37Wg8DAABYpx566KE/7+7NZ9pvXQTO9u3bc+DAgbUeBgAAsE5V1VNL2c8lagAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA09i01gNYj7bvueeU2w7tveE8jgQAAFgOZ3AAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaSwpcKrqwqq6q6r+rKoer6p3VtXFVXVfVT0xni8a+1ZVfbqqDlbVI1V19er+CAAAAAuWegbnU0m+3N1vTvLWJI8n2ZPk/u6+Msn943WSfCDJleOxO8mtKzpiAACAUzhj4FTVG5O8O8ntSdLdf9vdzyfZmWTf2G1fkhvH8s4kn+sFX01yYVVduuIjBwAAOMFSzuBckeRYkv9RVd+oqs9U1euSbOnuZ8Y+zybZMpa3Jnl60dcfHusAAABW1VICZ1OSq5Pc2t1vS/L/8pPL0ZIk3d1JejlvXFW7q+pAVR04duzYcr4UAADgpJYSOIeTHO7uB8fru7IQPM8dv/RsPB8d248kuWzR128b616iu2/r7h3dvWPz5s1nO34AAIAfO2PgdPezSZ6uqp8dq65N8liS/Ul2jXW7ktw9lvcn+ci4m9o1SV5YdCkbAADAqtm0xP3+TZLPV9WrkzyZ5KNZiKM7q+rmJE8l+dDY994k1yc5mOSHY18AAIBVt6TA6e6Hk+w4yaZrT7JvJ7nlHMcFAACwbEv9OzgAAADrnsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYxpICp6oOVdW3qurhqjow1l1cVfdV1RPj+aKxvqrq01V1sKoeqaqrV/MHAAAAOG45Z3D+cXdf1d07xus9Se7v7iuT3D9eJ8kHklw5HruT3LpSgwUAADidc7lEbWeSfWN5X5IbF63/XC/4apILq+rSc3gfAACAJVlq4HSSP6qqh6pq91i3pbufGcvPJtkylrcmeXrR1x4e6wAAAFbVpiXu93PdfaSq/kGS+6rqzxZv7O6uql7OG49Q2p0kl19++XK+FAAA4KSWdAanu4+M56NJvpjk7UmeO37p2Xg+OnY/kuSyRV++baw78Xve1t07unvH5s2bz/4nAAAAGM4YOFX1uqp6w/HlJO9P8u0k+5PsGrvtSnL3WN6f5CPjbmrXJHlh0aVsAAAAq2Ypl6htSfLFqjq+/+9295er6utJ7qyqm5M8leRDY/97k1yf5GCSHyb56IqPGgAA4CTOGDjd/WSSt55k/feSXHuS9Z3klhUZHQAAwDKcy22iAQAA1hWBAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANPYtNYD2Gi277nnlNsO7b3hPI4EAAA4kTM4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwjSUHTlVdUFXfqKovjddXVNWDVXWwqr5QVa8e618zXh8c27evztABAABeajlncD6e5PFFrz+R5JPd/TNJfpDk5rH+5iQ/GOs/OfYDAABYdUsKnKraluSGJJ8ZryvJe5PcNXbZl+TGsbxzvM7Yfu3YHwAAYFUt9QzOf03y75P83Xj9piTPd/eL4/XhJFvH8tYkTyfJ2P7C2P8lqmp3VR2oqgPHjh07y+EDAAD8xBkDp6r+SZKj3f3QSr5xd9/W3Tu6e8fmzZtX8lsDAACvUJuWsM+7kvzTqro+yWuT/P0kn0pyYVVtGmdptiU5MvY/kuSyJIeralOSNyb53oqPHAAA4ARnPIPT3b/S3du6e3uSm5J8pbv/eZIHknxw7LYryd1jef94nbH9K93dKzpqAACAkziXv4PzH5L8clUdzMJnbG4f629P8qax/peT7Dm3IQIAACzNUi5R+7Hu/uMkfzyWn0zy9pPs89dJfmEFxgYAALAs53IGBwAAYF0ROAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwjU1rPYCZbN9zzym3Hdp7w3kcCQAAvDI5gwMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATOOMgVNVr62qr1XVN6vq0ar6jbH+iqp6sKoOVtUXqurVY/1rxuuDY/v21f0RAAAAFizlDM7fJHlvd781yVVJrquqa5J8Isknu/tnkvwgyc1j/5uT/GCs/+TYDwAAYNWdMXB6wV+Nl68aj07y3iR3jfX7ktw4lneO1xnbr62qWrERAwAAnMKSPoNTVRdU1cNJjia5L8l3kzzf3S+OXQ4n2TqWtyZ5OknG9heSvOkk33N3VR2oqgPHjh07t58CAAAgSwyc7v5Rd1+VZFuStyd587m+cXff1t07unvH5s2bz/XbAQAALO8uat39fJIHkrwzyYVVtWls2pbkyFg+kuSyJBnb35jkeysyWgAAgNNYyl3UNlfVhWP5p5K8L8njWQidD47ddiW5eyzvH68ztn+lu3slBw0AAHAym868Sy5Nsq+qLshCEN3Z3V+qqseS3FFVv5nkG0luH/vfnuR3qupgku8nuWkVxg0AAPAyZwyc7n4kydtOsv7JLHwe58T1f53kF1ZkdAAAAMuwrM/gAAAArGcCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKaxaa0H8Eqxfc89p9x2aO8N53EkAAAwL2dwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaZwxcKrqsqp6oKoeq6pHq+rjY/3FVXVfVT0xni8a66uqPl1VB6vqkaq6erV/CAAAgGRpZ3BeTPJvu/stSa5JcktVvSXJniT3d/eVSe4fr5PkA0muHI/dSW5d8VEDAACcxBkDp7uf6e4/Hct/meTxJFuT7Eyyb+y2L8mNY3lnks/1gq8mubCqLl3xkQMAAJxgWZ/BqartSd6W5MEkW7r7mbHp2SRbxvLWJE8v+rLDY92J32t3VR2oqgPHjh1b5rABAABebsmBU1WvT/L7SX6pu/9i8bbu7iS9nDfu7tu6e0d379i8efNyvhQAAOCklhQ4VfWqLMTN57v7D8bq545fejaej471R5JctujLt411AAAAq2opd1GrJLcneby7f2vRpv1Jdo3lXUnuXrT+I+NuatckeWHRpWwAAACrZtMS9nlXkg8n+VZVPTzW/WqSvUnurKqbkzyV5ENj271Jrk9yMMkPk3x0RUcMAABwCmcMnO7+30nqFJuvPcn+neSWcxwXAADAsi3rLmoAAADrmcABAACmIXAAAIBpCBwAAGAaAgcAAJiGwAEAAKYhcAAAgGks5Q99ssq277nnlNsO7b3hPI4EAAA2NmdwAACAaQgcAABgGgIHAACYhsABAACmIXAAAIBpuIvaOucOawAAsHTO4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADCNMwZOVX22qo5W1bcXrbu4qu6rqifG80VjfVXVp6vqYFU9UlVXr+bgAQAAFlvKGZz/meS6E9btSXJ/d1+Z5P7xOkk+kOTK8did5NaVGSYAAMCZnTFwuvtPknz/hNU7k+wby/uS3Lho/ed6wVeTXFhVl67UYAEAAE7nbD+Ds6W7nxnLzybZMpa3Jnl60X6HxzoAAIBVt+lcv0F3d1X1cr+uqnZn4TK2XH755ec6DE6wfc89p9x2aO8N53EkAABw/pztGZznjl96Np6PjvVHkly2aL9tY93LdPdt3b2ju3ds3rz5LIcBAADwE2d7Bmd/kl1J9o7nuxet/1hV3ZHkHUleWHQpGyvsdGdpAADgleiMgVNVv5fkPUkuqarDSX4tC2FzZ1XdnOSpJB8au9+b5PokB5P8MMlHV2HMAAAAJ3XGwOnuXzzFpmtPsm8nueVcBwUAAHA2zvYzOAAAAOuOwAEAAKYhcAAAgGkIHAAAYBoCBwAAmIbAAQAApiFwAACAaZzx7+Awn+177jnt9kN7bzhPIwEAgJXlDA4AADANgQMAAEzDJWq8zOkuYXP5GgAA65kzOAAAwDQEDgAAMA2BAwAATEPgAAAA0xA4AADANAQOAAAwDYEDAABMQ+AAAADTEDgAAMA0Nq31ANhYtu+555TbDu294TyOBAAAXs4ZHAAAYBoCBwAAmIbAAQAApuEzOKw5n+sBAGClCBxWzOlCBQAAzgeXqAEAANMQOAAAwDQEDgAAMA2BAwAATEPgAAAA03AXNTYst5cGAOBEzuAAAADTEDgAAMA0BA4AADANgQMAAEzDTQZ4xXFzAgCAeQkcWCHCCQBg7Qkc1rXTRcNqfB0AABubz+AAAADTEDgAAMA0BA4AADANn8GBRVbrsztn+33dnAAAYHkEDqxjwggAYHkEDrzCrNbtrN0mGwBYDwQOTGg1bq8tUgCAjUDgAKtuNcJJjAEAJ+MuagAAwDScwQHW1GrduW49cbYJAM6fVQmcqrouyaeSXJDkM929dzXeBzh/NlKInO+xbqSbM6yn9zvTe7q0EYCzseKBU1UXJPlvSd6X5HCSr1fV/u5+bKXfC2A9WIu/n3S2YbAaY1kL6ynU1lsYbaSxvpKZJ1g9q3EG5+1JDnb3k0lSVXck2ZlE4ACskPUWHDPwz3QO6ykcNtL/plbjP6gItbPjn+m5W43A2Zrk6UWvDyd5xyq8DwAraC3+ZWyGs02r9X6rcZZuPV2+Ocv/3lbDuYxzPf0L8Gr88z7Tz3e+L23dSF5Jfzy8untlv2HVB5Nc193/Yrz+cJJ3dPfHTthvd5Ld4+XPJvnOig7kpS5J8uer+P05/8zpnMzrnMzrfMzpnMzrnGaa13/Y3ZvPtNNqnME5kuSyRa+3jXUv0d23JbltFd7/ZarqQHfvOB/vxflhTudkXudkXudjTudkXuf0SpzX1fg7OF9PcmVVXVFVr05yU5L9q/A+AAAAL7HiZ3C6+8Wq+liSP8zCbaI/292PrvT7AAAAnGhV/g5Od9+b5N7V+N5n6bxcCsd5ZU7nZF7nZF7nY07nZF7n9Iqb1xW/yQAAAMBaWY3P4AAAAKyJqQOnqq6rqu9U1cGq2rPW42F5qupQVX2rqh6uqgNj3cVVdV9VPTGeLxrrq6o+Peb6kaq6em1Hz3FV9dmqOlpV3160btnzWFW7xv5PVNWutfhZWHCKOf31qjoyjteHq+r6Rdt+Zczpd6rq5xet9zt6Hamqy6rqgap6rKoeraqPj/WO1w3qNHPqeN3Aquq1VfW1qvrmmNffGOuvqKoHxxx9YdzsK1X1mvH64Ni+fdH3Oul8b3jdPeUjCzc4+G6Sn07y6iTfTPKWtR6Xx7Lm8FCSS05Y95+T7BnLe5J8Yixfn+R/Jakk1yR5cK3H7/HjOXt3kquTfPts5zHJxUmeHM8XjeWL1vpne6U+TjGnv57k351k37eM37+vSXLF+L18gd/R6++R5NIkV4/lNyT5P2P+HK8b9HGaOXW8buDHOOZeP5ZfleTBcQzemeSmsf63k/yrsfyvk/z2WL4pyRdON99r/fOtxGPmMzhvT3Kwu5/s7r9NckeSnWs8Js7dziT7xvK+JDcuWv+5XvDVJBdW1aVrMUBeqrv/JMn3T1i93Hn8+ST3dff3u/sHSe5Lct3qj56TOcWcnsrOJHd099909/9NcjALv5/9jl5nuvuZ7v7TsfyXSR5PsjWO1w3rNHN6Ko7XDWAcc381Xr5qPDrJe5PcNdafeKweP4bvSnJtVVVOPd8b3syBszXJ04teH87pD2rWn07yR1X1UFXtHuu2dPczY/nZJFvGsvneWJY7j+Z3Y/jYuFTps8cvY4o53ZDGJSxvy8J/GXa8TuCEOU0crxtaVV1QVQ8nOZqF/4jw3STPd/eLY5fFc/Tj+RvbX0jypkw8rzMHDhvfz3X31Uk+kOSWqnr34o29cH7VbQA3OPM4jVuT/KMkVyV5Jsl/WdvhcLaq6vVJfj/JL3X3Xyze5njdmE4yp47XDa67f9TdVyXZloWzLm9e4yGtKzMHzpEkly16vW2sY4Po7iPj+WiSL2bhAH7u+KVn4/no2N18byzLnUfzu85193Pj/3D/Lsl/z08uczCnG0hVvSoL/yL8+e7+g7Ha8bqBnWxOHa/z6O7nkzyQ5J1ZuEz0+N+4XDxHP56/sf2NSb6Xied15sD5epIrxx0lXp2FD1XtX+MxsURV9bqqesPx5STvT/LtLMzh8Tvy7Epy91jen+Qj464+1yR5YdElFaw/y53HP0zy/qq6aFxK8f6xjnXihM+8/bMsHK/JwpzeNO7ic0WSK5N8LX5Hrzvjmvzbkzze3b+1aJPjdYM61Zw6Xje2qtpcVReO5Z9K8r4sfL7qgSQfHLudeKweP4Y/mOQr42zsqeZ7w9t05l02pu5+sao+loVfqhck+Wx3P7rGw2LptiT54sLv5mxK8rvd/eWq+nqSO6vq5iRPJfnQ2P/eLNzR52CSHyb56PkfMidTVb+X5D1JLqmqw0l+LcneLGMeu/v7VfWfsvB/sknyH7t7qR9yZ4WdYk7fU1VXZeHypUNJ/mWSdPejVXVnkseSvJjklu7+0fg+fkevL+9K8uEk3xrX9ifJr8bxupGdak5/0fG6oV2aZF9VXZCFkxV3dveXquqxJHdU1W8m+UYW4jbj+Xeq6mAWbhBzU3L6+d7oaiHgAAAANr6ZL1EDAABeYQQOAAAwDYEDAABMQ+AAAADTEDgAAMA0BA4AADANgQMAAExD4AAAANP4/ydjdgKGXsWcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dictionary = copy.deepcopy(dictionary)\n",
    "filtered_dictionary.filter_extremes(no_below=50) # remove terms that occur <50 times\n",
    "high_tokens = [filtered_dictionary.token2id[reference[i][0]] for i in range(ref_cutoff)] # ids of tokens that'll be excised\n",
    "filtered_dictionary.filter_tokens(bad_ids=high_tokens) # our 97 most common term ids, approximating words that occur more than once/10 documents\n",
    "filtered_corp = [[w for w in doc if w in filtered_dictionary.token2id] for doc in corpus]\n",
    "filtered_bow = [filtered_dictionary.doc2bow(doc) for doc in filtered_corp]\n",
    "\n",
    "# now let's check our distributions again\n",
    "# let's look at the most common words throughout the corpus\n",
    "new_tot_counts = np.zeros(len(filtered_dictionary))\n",
    "\n",
    "# goes through the (id, count) tuples in every document and increments a total count\n",
    "for doc in filtered_bow:\n",
    "    for w in doc:\n",
    "        new_tot_counts[w[0]] += w[1]\n",
    "\n",
    "new_masterlist = [(filtered_dictionary[i], new_tot_counts[i]) for i in range(len(filtered_dictionary))]\n",
    "\n",
    "# the most common\n",
    "new_reference = sorted(new_masterlist, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "new_prevalent_counts = [ref[1] for ref in new_reference]\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.hist(new_prevalent_counts, bins=100);\n",
    "# plt.xlim(0, 5000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"gradient\" + 0.007*\"optimization\" + 0.006*\"sparse\" + 0.006*\"solution\" + 0.005*\"linear\" + 0.005*\"convergence\" + 0.005*\"rate\" + 0.004*\"point\" + 0.004*\"stochastic\" + 0.004*\"signal\"'),\n",
       " (1,\n",
       "  '0.009*\"deep_neural\" + 0.008*\"neural_network\" + 0.007*\"inference\" + 0.006*\"state_art\" + 0.005*\"datasets\" + 0.005*\"accuracy\" + 0.005*\"complex\" + 0.005*\"kernel\" + 0.005*\"representation\" + 0.004*\"existing\"'),\n",
       " (2,\n",
       "  '0.011*\"policy\" + 0.007*\"attack\" + 0.007*\"error\" + 0.007*\"system\" + 0.007*\"reinforcement_learning\" + 0.006*\"setting\" + 0.006*\"deep_learning\" + 0.005*\"provide\" + 0.005*\"test\" + 0.005*\"behavior\"'),\n",
       " (3,\n",
       "  '0.009*\"user\" + 0.009*\"classifier\" + 0.007*\"dataset\" + 0.006*\"image\" + 0.006*\"datasets\" + 0.006*\"multiple\" + 0.005*\"label\" + 0.005*\"accuracy\" + 0.004*\"representation\" + 0.004*\"group\"'),\n",
       " (4,\n",
       "  '0.011*\"node\" + 0.010*\"representation\" + 0.010*\"neural_network\" + 0.009*\"space\" + 0.009*\"layer\" + 0.009*\"architecture\" + 0.008*\"input\" + 0.008*\"kernel\" + 0.007*\"deep\" + 0.005*\"image\"'),\n",
       " (5,\n",
       "  '0.012*\"state\" + 0.011*\"process\" + 0.011*\"agent\" + 0.009*\"bayesian\" + 0.009*\"object\" + 0.008*\"image\" + 0.007*\"prior\" + 0.006*\"dynamic\" + 0.006*\"variable\" + 0.005*\"observation\"')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_lda = LdaModel(corpus=filtered_bow, num_topics=6, id2word=filtered_dictionary, alpha= 'asymmetric', eta=.01)\n",
    "filtered_lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My rationale for the `alpha` and `eta` parameters are as follows:**\n",
    "\n",
    "$\\alpha$ is a LDA parameter that relates to how evenly distributed topics are across documents. By default, the model assumes a document is approximately equally likely to have any topic. However, this is not the case here because all my documents relate to Machine Learning. So there are generic ML terms (and topics) that may be shared across more documents than others. Thus I set `alpha = 'asymmetric'`\n",
    "\n",
    "$\\eta$ in gensim is analogous to the parameter $\\beta$ in the formal LDA model. It signifies how many topics share a word. Low `eta` means that the model favors term exclusivity among topics. Again, because my corpus is so overlapping in nature, I want to set `eta` to be low\n",
    "\n",
    "Next up...\n",
    "\n",
    "## Visualizing the topics using pyLDAvis ##\n",
    "\n",
    "While some people (*ahem* Dave Yarrington *ahem*) question its statistcal rigor, the pyLDAvis library is a very popular way of getting a high-level intuition on the layout of your topics model. Of primary concern to me is the horizontal bar graph on the right, which shows a list of tokens in a given topic along with their relative frequencies in the corpus. \n",
    "\n",
    "By visually inspecting the tokens, you can see several topics emerge: \n",
    "    - Reinforcement Learning - _policy, reinforcement_learning, agent_\n",
    "    - Gradient Descent - _gradient, convergence, optimization, convex_\n",
    "    - Neural Networks (may combine or split up CNN, RNN, VAE, GAN into two) - _deep_neural, rnn, cnn, convolution, variational_\n",
    "    - Medical Applications of ML or \"real life\" datasets - _patient, user, recommendation, disease, drug_\n",
    "    - Baysian/ Probablistic Methods and Estimators - _bayesian, latent_variable/space, monte_carlo, estimation_\n",
    "\n",
    "Depending on whether you ran the cell or not, you may get significantly different-looking topic distribution. This is because LDA begins with a random seed every time it runs \n",
    "\n",
    "Of particular note is the relevance metric slider. When $\\lambda$ is turned low (around `0.1 - 0.2`), more discriminative words become ranked higher, even though they are not the most common. **Thus, I plan to manually inspect my topics to create a human-understandable label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el55821401227026937447976865444\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el55821401227026937447976865444_data = {\"mdsDat\": {\"x\": [-0.0900560703486865, 0.010474680184123304, -0.05180436764409854, 0.09063474833793478, -0.04549250274731227, 0.08624351221803933], \"y\": [0.026200782243467762, -0.041888773507417494, 0.04362730390948468, -0.07886011309859868, -0.05931767190903077, 0.11023847236209448], \"topics\": [1, 2, 3, 4, 5, 6], \"cluster\": [1, 1, 1, 1, 1, 1], \"Freq\": [27.727027893066406, 17.877914428710938, 13.997723579406738, 16.9823055267334, 11.68864631652832, 11.7263822555542]}, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\"], \"Freq\": [1787.0, 1871.0, 1899.0, 1977.0, 1560.0, 1771.0, 1841.0, 3650.0, 1040.0, 1954.0, 2226.0, 1728.0, 1235.0, 1571.0, 3312.0, 3046.0, 3388.0, 2402.0, 1199.0, 2731.0, 1692.0, 621.0, 1956.0, 1384.0, 2102.0, 2685.0, 904.0, 734.0, 1817.0, 1776.0, 290.7200927734375, 253.77374267578125, 211.041259765625, 212.51695251464844, 102.97525787353516, 281.4201965332031, 119.53182983398438, 73.82019805908203, 230.8630828857422, 333.3524475097656, 72.10734558105469, 113.1191177368164, 217.66818237304688, 198.5270538330078, 199.18936157226562, 128.69105529785156, 67.78359985351562, 782.4156494140625, 345.21612548828125, 87.41211700439453, 175.30215454101562, 84.05855560302734, 237.9680633544922, 112.37175750732422, 58.01992416381836, 278.4310302734375, 60.65870666503906, 769.595703125, 82.66549682617188, 120.66056060791016, 211.21844482421875, 199.5587615966797, 124.53840637207031, 848.1785888671875, 598.6216430664062, 691.4845581054688, 357.17962646484375, 287.2452697753906, 1274.4697265625, 266.48382568359375, 870.2996215820312, 868.9410400390625, 1003.8699340820312, 686.0151977539062, 695.6949462890625, 2133.033203125, 723.6942749023438, 519.5216674804688, 1679.970458984375, 620.0987548828125, 613.6439208984375, 1927.049560546875, 366.3045959472656, 986.7051391601562, 802.2957153320312, 1234.7947998046875, 1549.7923583984375, 1120.8250732421875, 1131.61376953125, 1323.9888916015625, 716.448974609375, 927.375, 792.7140502929688, 615.4390869140625, 1170.594482421875, 838.6549682617188, 993.6969604492188, 885.0907592773438, 919.8125, 1066.7996826171875, 1033.5155029296875, 1028.76220703125, 900.63232421875, 862.9324951171875, 872.2938232421875, 899.7378540039062, 862.3690795898438, 859.5336303710938, 810.8085327148438, 133.6420440673828, 86.704345703125, 87.1423568725586, 129.57618713378906, 194.77491760253906, 109.40509796142578, 84.71863555908203, 45.64897155761719, 50.201602935791016, 236.8643341064453, 133.6688690185547, 327.71038818359375, 127.12200927734375, 190.3917999267578, 90.46355438232422, 71.35459899902344, 288.16387939453125, 42.085304260253906, 75.8568344116211, 102.80130767822266, 179.1623992919922, 115.09688568115234, 48.086822509765625, 77.24070739746094, 83.77982330322266, 104.63550567626953, 82.04944610595703, 304.646728515625, 164.26327514648438, 175.29786682128906, 1547.6507568359375, 465.7828369140625, 244.72805786132812, 137.42001342773438, 174.34266662597656, 145.3452911376953, 180.76904296875, 462.3960876464844, 1282.7396240234375, 479.8775939941406, 534.1083374023438, 261.2242126464844, 903.1073608398438, 896.7925415039062, 209.2174835205078, 464.8778381347656, 1412.7470703125, 1028.96728515625, 499.8557434082031, 345.2001647949219, 933.3865356445312, 913.7066650390625, 509.17413330078125, 376.6896667480469, 886.9871826171875, 706.8046875, 611.0511474609375, 389.0469665527344, 747.311279296875, 716.1712646484375, 557.9736938476562, 656.756103515625, 573.649658203125, 617.4044189453125, 574.6220703125, 470.23126220703125, 572.4541625976562, 497.3766784667969, 444.5467529296875, 575.7854614257812, 499.6590881347656, 565.791015625, 551.7698974609375, 516.751953125, 533.1515502929688, 513.1541137695312, 152.15560913085938, 76.66410827636719, 103.84642791748047, 108.18871307373047, 69.46769714355469, 101.79927062988281, 190.0354766845703, 123.4789047241211, 65.28590393066406, 66.9495620727539, 71.5106201171875, 206.53546142578125, 358.1642150878906, 1001.4823608398438, 211.79135131835938, 84.54017639160156, 207.69754028320312, 150.50904846191406, 92.19415283203125, 273.8356018066406, 496.94708251953125, 119.85331726074219, 332.4058532714844, 52.31978988647461, 130.072509765625, 54.147804260253906, 111.48307037353516, 1544.4586181640625, 64.20564270019531, 175.0889129638672, 174.66925048828125, 904.1891479492188, 160.88677978515625, 542.79345703125, 961.6943359375, 983.5947875976562, 256.8686828613281, 156.10289001464844, 418.8056335449219, 634.5802612304688, 425.7838439941406, 552.0093383789062, 192.0116424560547, 581.7971801757812, 596.7831420898438, 365.30511474609375, 540.4723510742188, 758.1428833007812, 685.868896484375, 782.5930786132812, 617.1950073242188, 562.249267578125, 400.35638427734375, 553.1089477539062, 694.8800659179688, 472.65130615234375, 526.9779052734375, 379.5157470703125, 447.314453125, 444.31329345703125, 537.6156616210938, 583.9390258789062, 475.7935791015625, 462.3846435546875, 422.42608642578125, 418.9526672363281, 45.63440704345703, 73.58513641357422, 198.00650024414062, 207.3511962890625, 60.52519226074219, 70.66376495361328, 192.63870239257812, 103.72745513916016, 40.98704528808594, 157.94139099121094, 131.9065399169922, 209.9265594482422, 390.5195007324219, 703.0835571289062, 168.4082489013672, 170.94825744628906, 451.5613708496094, 111.13614654541016, 82.04878997802734, 280.5869445800781, 211.5037078857422, 42.96742630004883, 138.6439666748047, 342.0998229980469, 209.4631805419922, 60.30736541748047, 65.80839538574219, 55.91849136352539, 375.38824462890625, 1546.9512939453125, 550.062744140625, 275.8426208496094, 527.791748046875, 298.6415710449219, 513.192626953125, 303.64349365234375, 173.79046630859375, 1471.1375732421875, 303.8231506347656, 530.7862548828125, 305.67779541015625, 707.6088256835938, 350.731689453125, 429.218994140625, 834.6876831054688, 1089.3740234375, 518.9439697265625, 547.7119750976562, 588.7214965820312, 611.341064453125, 909.11083984375, 704.5941772460938, 600.445068359375, 1049.375244140625, 934.9423217773438, 495.96563720703125, 771.6185913085938, 629.451171875, 499.68695068359375, 460.333251953125, 718.3849487304688, 612.9697265625, 559.6743774414062, 533.0604248046875, 499.658203125, 493.29840087890625, 487.3663635253906, 493.63751220703125, 213.32040405273438, 205.08338928222656, 126.49345397949219, 114.9426498413086, 111.95951843261719, 54.07246017456055, 94.30097961425781, 266.0699157714844, 109.823974609375, 88.74836730957031, 184.35476684570312, 242.30072021484375, 201.75933837890625, 119.38796997070312, 49.983680725097656, 120.4477767944336, 72.75741577148438, 200.75894165039062, 194.820556640625, 262.81591796875, 43.71769332885742, 66.56796264648438, 50.070743560791016, 83.01351165771484, 174.53042602539062, 97.04528045654297, 34.782196044921875, 165.7352752685547, 210.6852569580078, 32.26841354370117, 1218.1256103515625, 319.35504150390625, 283.3485412597656, 158.96051025390625, 1032.3505859375, 408.45599365234375, 495.7575988769531, 412.22381591796875, 998.6024169921875, 883.7694091796875, 475.4408264160156, 1047.566650390625, 561.3366088867188, 1142.8839111328125, 451.7087097167969, 1125.099609375, 778.2568969726562, 885.3292846679688, 387.2226257324219, 591.4005126953125, 406.56866455078125, 534.9500732421875, 551.3572998046875, 597.5763549804688, 520.103271484375, 537.4569702148438, 347.6293029785156, 398.6326904296875, 375.8796691894531, 398.9131774902344, 389.97186279296875, 341.7361755371094, 389.1413879394531, 380.3825378417969, 383.6739807128906, 127.731689453125, 298.6004333496094, 590.7638549804688, 185.9591827392578, 46.75614547729492, 128.8250274658203, 187.1674346923828, 26.050168991088867, 47.40686798095703, 111.63218688964844, 68.60813903808594, 190.64801025390625, 37.28871154785156, 127.92198944091797, 320.6639404296875, 491.9367370605469, 39.753379821777344, 224.7630157470703, 66.10835266113281, 194.75315856933594, 963.9578857421875, 81.52779388427734, 44.70041275024414, 135.7762908935547, 112.60913848876953, 104.82879638671875, 343.26751708984375, 137.6381378173828, 21.22249412536621, 82.993896484375, 286.2972106933594, 246.61404418945312, 155.87112426757812, 521.4412231445312, 1310.750732421875, 1036.7117919921875, 1191.3128662109375, 340.3131408691406, 568.26513671875, 310.95977783203125, 295.9044494628906, 463.81646728515625, 847.5664672851562, 410.5000305175781, 1264.3660888671875, 321.2964172363281, 687.8606567382812, 552.102294921875, 603.3853149414062, 720.756103515625, 537.2421875, 885.8670654296875, 467.34649658203125, 424.98895263671875, 554.82568359375, 589.1232299804688, 490.7923278808594, 543.8822021484375, 527.6905517578125, 550.2550659179688, 489.64056396484375, 435.1346435546875, 442.0832214355469, 389.213623046875], \"Term\": [\"policy\", \"agent\", \"deep_neural\", \"state\", \"node\", \"user\", \"kernel\", \"neural_network\", \"attack\", \"architecture\", \"classifier\", \"layer\", \"object\", \"bayesian\", \"representation\", \"process\", \"image\", \"inference\", \"reinforcement_learning\", \"space\", \"prior\", \"student\", \"error\", \"environment\", \"deep\", \"gradient\", \"adversarial\", \"patient\", \"variable\", \"system\", \"deployment\", \"attention_mechanism\", \"multi_layer\", \"wasserstein_distance\", \"feedforward_neural\", \"admm\", \"saddle_point\", \"hypothesis_test\", \"optimal_transport\", \"step_size\", \"iteration_complexity\", \"nonconvex_optimization\", \"linear_convergence\", \"strongly_convex\", \"svd\", \"character\", \"rank_matrix\", \"generator\", \"nonconvex\", \"alternating_minimization\", \"stationary_point\", \"iterates\", \"eeg\", \"primal_dual\", \"strong_convexity\", \"descent_sgd\", \"global_convergence\", \"non_convex\", \"nonsmooth\", \"convergence_guarantee\", \"local_minimum\", \"proximal\", \"ica\", \"convex\", \"convergence_rate\", \"sgd\", \"initialization\", \"streaming\", \"convergence\", \"matrix_completion\", \"gradient_descent\", \"low_rank\", \"stochastic_gradient\", \"tensor\", \"norm\", \"gradient\", \"iteration\", \"descent\", \"sparse\", \"optimization_problem\", \"rank\", \"optimization\", \"recovery\", \"prove\", \"guarantee\", \"rate\", \"solution\", \"signal\", \"stochastic\", \"linear\", \"solving\", \"condition\", \"regularization\", \"update\", \"point\", \"estimator\", \"estimation\", \"optimal\", \"clustering\", \"provide\", \"setting\", \"case\", \"term\", \"noise\", \"vector\", \"order\", \"large\", \"existing\", \"efficient\", \"machine_translation\", \"vulnerable\", \"domain_specific\", \"mobile\", \"customer\", \"compound\", \"enabled\", \"stochastic_variational\", \"adoption\", \"gps\", \"molecular\", \"convolution\", \"industrial\", \"attacker\", \"labelled\", \"approximate_bayesian\", \"variational_inference\", \"inducing\", \"primitive\", \"production\", \"x\", \"material\", \"parametrized\", \"logic\", \"softmax\", \"company\", \"format\", \"compression\", \"library\", \"protein\", \"deep_neural\", \"variational\", \"hardware\", \"link_prediction\", \"physic\", \"open_source\", \"hyper_parameter\", \"neural\", \"inference\", \"gaussian_process\", \"memory\", \"platform\", \"complex\", \"kernel\", \"hyperparameters\", \"convolutional_neural\", \"neural_network\", \"state_art\", \"field\", \"structured\", \"datasets\", \"accuracy\", \"computation\", \"implementation\", \"representation\", \"efficient\", \"approximation\", \"map\", \"existing\", \"demonstrate\", \"regression\", \"large\", \"architecture\", \"novel\", \"deep\", \"effective\", \"experiment\", \"introduce\", \"scale\", \"present\", \"design\", \"optimization\", \"space\", \"dynamic\", \"process\", \"learn\", \"safety\", \"interpreting\", \"verification\", \"text_classification\", \"release\", \"differentially_private\", \"reward_function\", \"differential_privacy\", \"optimal_policy\", \"malicious\", \"pac\", \"quantum\", \"adversary\", \"attack\", \"teacher\", \"personal\", \"security\", \"policy_gradient\", \"mdp\", \"privacy\", \"game\", \"driving\", \"neuron\", \"passive\", \"nmf\", \"ultimately\", \"stock\", \"policy\", \"actively\", \"private\", \"detector\", \"reinforcement_learning\", \"artificial_intelligence\", \"decision\", \"system\", \"error\", \"player\", \"confidence\", \"bias\", \"behavior\", \"risk\", \"control\", \"self\", \"environment\", \"bound\", \"mechanism\", \"strategy\", \"deep_learning\", \"test\", \"setting\", \"agent\", \"noise\", \"goal\", \"design\", \"provide\", \"estimator\", \"value\", \"adversarial\", \"theory\", \"loss\", \"case\", \"neural_network\", \"estimate\", \"classifier\", \"example\", \"robot\", \"significantly_reduce\", \"rotation\", \"target_domain\", \"source_domain\", \"neuroimaging\", \"phenotype\", \"age\", \"diagnostic\", \"naive_bayes\", \"rating\", \"survival\", \"long_short\", \"disease\", \"patient\", \"music\", \"cancer\", \"forecasting\", \"recommender\", \"movie\", \"sentence\", \"modality\", \"linear_discriminant\", \"user_item\", \"clinical\", \"gene\", \"classifier_trained\", \"website\", \"lda\", \"semantic\", \"user\", \"recommendation\", \"traffic\", \"word\", \"predictor\", \"topic\", \"document\", \"forecast\", \"classifier\", \"labeled\", \"text\", \"brain\", \"group\", \"content\", \"item\", \"label\", \"dataset\", \"predict\", \"score\", \"individual\", \"time_series\", \"multiple\", \"level\", \"human\", \"image\", \"datasets\", \"interaction\", \"accuracy\", \"trained\", \"pattern\", \"ensemble\", \"representation\", \"input\", \"domain\", \"variable\", \"train\", \"type\", \"single\", \"existing\", \"momentum\", \"imagenet\", \"batch_size\", \"targeted\", \"thompson_sampling\", \"refine\", \"cascade\", \"activation_function\", \"ucb\", \"bandit_problem\", \"multi_armed\", \"arm\", \"fully_connected\", \"contextual_bandit\", \"cheap\", \"autoencoders\", \"distance_metric\", \"meta_learning\", \"particle\", \"bandit\", \"upper_confidence\", \"processor\", \"community_structure\", \"rkhs\", \"alignment\", \"contextual\", \"maximum_mean\", \"diffusion\", \"autoencoder\", \"bandwidth\", \"node\", \"regret\", \"convolutional\", \"mnist\", \"layer\", \"manifold\", \"embedding\", \"embeddings\", \"architecture\", \"kernel\", \"adversarial\", \"space\", \"metric\", \"representation\", \"reward\", \"neural_network\", \"deep\", \"input\", \"tree\", \"weight\", \"distance\", \"domain\", \"deep_learning\", \"image\", \"learn\", \"datasets\", \"ensemble\", \"sequence\", \"size\", \"experiment\", \"test\", \"cost\", \"given\", \"state_art\", \"accuracy\", \"actor_critic\", \"skill\", \"student\", \"shot\", \"posterior_probability\", \"observational_data\", \"multi_agent\", \"hidden_variable\", \"hamiltonian_monte\", \"sampler\", \"modelled\", \"scene\", \"observed_variable\", \"plan\", \"planning\", \"causal\", \"gibbs_sampler\", \"mcmc\", \"multilayer\", \"markov\", \"object\", \"analytics\", \"maximum_entropy\", \"person\", \"joint_distribution\", \"causal_inference\", \"shape\", \"perception\", \"metropolis_hastings\", \"intensity\", \"markov_chain\", \"motion\", \"posterior_distribution\", \"latent_variable\", \"state\", \"bayesian\", \"agent\", \"latent_space\", \"latent\", \"monte_carlo\", \"d\", \"generative_model\", \"prior\", \"spatial\", \"process\", \"posterior\", \"variable\", \"modeling\", \"observation\", \"dynamic\", \"environment\", \"image\", \"cluster\", \"observed\", \"clustering\", \"inference\", \"real\", \"learn\", \"estimate\", \"given\", \"point\", \"sequence\", \"present\", \"space\"], \"Total\": [1787.0, 1871.0, 1899.0, 1977.0, 1560.0, 1771.0, 1841.0, 3650.0, 1040.0, 1954.0, 2226.0, 1728.0, 1235.0, 1571.0, 3312.0, 3046.0, 3388.0, 2402.0, 1199.0, 2731.0, 1692.0, 621.0, 1956.0, 1384.0, 2102.0, 2685.0, 904.0, 734.0, 1817.0, 1776.0, 290.7668151855469, 253.81956481933594, 211.0927734375, 212.57237243652344, 103.02108001708984, 281.5509338378906, 119.5890121459961, 73.86601257324219, 231.11712646484375, 334.03924560546875, 72.291748046875, 113.4559555053711, 218.40142822265625, 199.24734497070312, 200.22897338867188, 129.41961669921875, 68.18582153320312, 787.1279296875, 347.57452392578125, 88.38883972167969, 177.55433654785156, 85.16513061523438, 241.2183380126953, 114.06902313232422, 58.93098831176758, 283.1055603027344, 61.686241149902344, 783.1387329101562, 84.1700439453125, 122.88469696044922, 215.45248413085938, 204.2464599609375, 127.03343963623047, 889.7908935546875, 629.778076171875, 734.6773681640625, 374.19293212890625, 298.918212890625, 1390.8115234375, 277.50543212890625, 963.9100952148438, 975.3150634765625, 1154.1314697265625, 773.3281860351562, 792.7966918945312, 2685.78955078125, 830.4766845703125, 589.5014038085938, 2261.85205078125, 733.265380859375, 738.4181518554688, 2888.671630859375, 408.1558532714844, 1414.7735595703125, 1095.138427734375, 1898.1171875, 2574.650634765625, 1761.16357421875, 1805.753662109375, 2233.755859375, 987.8984375, 1484.110595703125, 1214.346435546875, 857.9224243164062, 2448.76171875, 1472.78125, 2119.59130859375, 1772.240478515625, 1953.29638671875, 2873.19970703125, 2747.9638671875, 2725.932861328125, 1980.599609375, 1706.603515625, 1862.00927734375, 2262.051513671875, 2509.38671875, 2843.98193359375, 2298.47705078125, 133.737060546875, 86.91964721679688, 87.46417999267578, 130.62240600585938, 198.44357299804688, 111.49639892578125, 86.35181427001953, 46.941158294677734, 51.62622833251953, 246.6748504638672, 139.377685546875, 341.7353820800781, 133.0695037841797, 201.04931640625, 95.91610717773438, 75.75633239746094, 308.3337707519531, 45.28387451171875, 81.82264709472656, 111.0688247680664, 195.67958068847656, 125.96820068359375, 52.75267791748047, 85.29039764404297, 92.92999267578125, 117.3139419555664, 92.6671142578125, 344.5128173828125, 186.82537841796875, 200.41346740722656, 1899.576171875, 552.9989013671875, 290.8738708496094, 157.4810333251953, 204.2215576171875, 169.86767578125, 223.38233947753906, 670.8831787109375, 2402.749267578125, 739.9959106445312, 856.0100708007812, 365.536376953125, 1797.29345703125, 1841.5235595703125, 275.29248046875, 792.89111328125, 3650.58056640625, 2603.8271484375, 951.6505126953125, 608.0661010742188, 2955.586669921875, 2940.122314453125, 1208.4263916015625, 750.16015625, 3312.397216796875, 2298.47705078125, 1904.4234619140625, 840.9500732421875, 2843.98193359375, 2649.979248046875, 1722.446533203125, 2509.38671875, 1954.3936767578125, 2411.3291015625, 2102.356689453125, 1301.3731689453125, 2193.736572265625, 1531.089599609375, 1154.8118896484375, 2786.572998046875, 1713.0203857421875, 2888.671630859375, 2731.171142578125, 2167.92822265625, 3046.40966796875, 2408.945068359375, 152.20843505859375, 76.71633911132812, 103.91981506347656, 108.3061294555664, 69.93160247802734, 102.4900894165039, 191.50062561035156, 124.4795913696289, 66.27928161621094, 68.03173828125, 72.91397857666016, 211.4114990234375, 370.2447814941406, 1040.802978515625, 220.84164428710938, 88.850341796875, 220.2924041748047, 162.0499267578125, 100.05647277832031, 301.05047607421875, 547.1181030273438, 132.26498413085938, 367.61749267578125, 58.17498016357422, 145.38803100585938, 61.403385162353516, 126.57791137695312, 1787.4498291015625, 76.61785888671875, 209.54049682617188, 211.13694763183594, 1199.7373046875, 194.6877899169922, 862.6004638671875, 1776.6458740234375, 1956.5496826171875, 366.9486083984375, 195.8638916015625, 698.7279052734375, 1303.6470947265625, 761.9561767578125, 1083.7611083984375, 261.5482177734375, 1384.64990234375, 1555.115966796875, 738.4239501953125, 1395.6983642578125, 2497.47705078125, 2131.04736328125, 2747.9638671875, 1871.0986328125, 1706.603515625, 940.6661987304688, 1713.0203857421875, 2873.19970703125, 1472.78125, 1952.2490234375, 904.0283813476562, 1393.10302734375, 1465.7833251953125, 2725.932861328125, 3650.58056640625, 2250.22021484375, 2226.08544921875, 2181.919921875, 419.24468994140625, 45.69297790527344, 74.11640167236328, 200.1859588623047, 209.8990020751953, 61.32975769042969, 71.7308349609375, 196.5340118408203, 106.17815399169922, 42.15480422973633, 162.67684936523438, 136.1648406982422, 217.08340454101562, 407.3992004394531, 734.1860961914062, 176.5850830078125, 179.30609130859375, 473.7687072753906, 116.69987487792969, 86.17342376708984, 297.1836853027344, 225.16261291503906, 46.171443939208984, 149.0411834716797, 369.7785949707031, 229.13148498535156, 66.05618286132812, 72.10772705078125, 61.4508171081543, 415.93798828125, 1771.5765380859375, 627.001708984375, 315.5984802246094, 616.4855346679688, 347.499267578125, 637.4921875, 361.90533447265625, 196.4272003173828, 2226.08544921875, 371.1266174316406, 718.7717895507812, 388.94671630859375, 1123.318115234375, 482.7354431152344, 635.87109375, 1528.8890380859375, 2264.3994140625, 822.6890869140625, 935.6795043945312, 1076.302490234375, 1192.6287841796875, 2294.534912109375, 1544.9228515625, 1285.826171875, 3388.40576171875, 2955.586669921875, 1044.1549072265625, 2940.122314453125, 1929.1160888671875, 1151.1090087890625, 964.1739501953125, 3312.397216796875, 2692.104736328125, 2063.774658203125, 1817.127685546875, 1402.6318359375, 1632.8875732421875, 1638.3372802734375, 2843.98193359375, 213.37196350097656, 205.1349639892578, 126.54544830322266, 114.9963607788086, 112.08964538574219, 54.145076751708984, 94.74834442138672, 267.5906982421875, 110.46761322021484, 89.71700286865234, 186.4760284423828, 246.37718200683594, 205.15975952148438, 121.48863220214844, 50.90198516845703, 123.07498931884766, 75.42169189453125, 213.25711059570312, 207.481201171875, 280.10711669921875, 47.055545806884766, 78.62831115722656, 59.43082809448242, 98.68653106689453, 212.62010192871094, 118.45124053955078, 42.56662368774414, 205.5072021484375, 262.4654541015625, 40.2331657409668, 1560.4459228515625, 406.8150329589844, 360.5736083984375, 208.00909423828125, 1728.4281005859375, 627.2605590820312, 799.6267700195312, 656.8080444335938, 1954.3936767578125, 1841.5235595703125, 904.0283813476562, 2731.171142578125, 1191.6748046875, 3312.397216796875, 879.9093627929688, 3650.58056640625, 2102.356689453125, 2692.104736328125, 806.503173828125, 1650.4405517578125, 893.1708984375, 2063.774658203125, 2497.47705078125, 3388.40576171875, 2408.945068359375, 2955.586669921875, 964.1739501953125, 1669.7750244140625, 1438.847412109375, 2193.736572265625, 2131.04736328125, 1041.34716796875, 2548.424072265625, 2603.8271484375, 2940.122314453125, 127.77964782714844, 298.85845947265625, 621.8004150390625, 201.74838256835938, 51.97256088256836, 143.66065979003906, 214.2677001953125, 29.90972137451172, 54.88919448852539, 130.5969696044922, 81.05386352539062, 228.44715881347656, 44.83134460449219, 153.88169860839844, 389.0909118652344, 608.696044921875, 49.50305938720703, 281.80169677734375, 84.266357421875, 248.30270385742188, 1235.03662109375, 104.8672103881836, 57.885101318359375, 176.3274688720703, 146.38229370117188, 137.6001739501953, 450.8912048339844, 181.24549865722656, 27.96080780029297, 110.18577575683594, 382.4605712890625, 331.6144714355469, 209.37777709960938, 739.4373779296875, 1977.715576171875, 1571.528076171875, 1871.0986328125, 498.1641845703125, 953.0021362304688, 483.1861572265625, 456.8389892578125, 803.03173828125, 1692.059326171875, 706.93603515625, 3046.40966796875, 557.7871704101562, 1817.127685546875, 1337.93115234375, 1549.22265625, 2167.92822265625, 1384.64990234375, 3388.40576171875, 1120.3026123046875, 1088.093505859375, 1953.29638671875, 2402.749267578125, 1734.3572998046875, 2408.945068359375, 2250.22021484375, 2548.424072265625, 2448.76171875, 1669.7750244140625, 2786.572998046875, 2731.171142578125], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2826000452041626, 1.2826000452041626, 1.2825000286102295, 1.2825000286102295, 1.2822999954223633, 1.2822999954223633, 1.2822999954223633, 1.282099962234497, 1.2817000150680542, 1.2806999683380127, 1.2802000045776367, 1.2798000574111938, 1.2793999910354614, 1.279099941253662, 1.2776000499725342, 1.2770999670028687, 1.2768000364303589, 1.2768000364303589, 1.2760000228881836, 1.2717000246047974, 1.2699999809265137, 1.269700050354004, 1.2691999673843384, 1.267799973487854, 1.267199993133545, 1.2661000490188599, 1.2660000324249268, 1.2653000354766846, 1.2647000551223755, 1.2645000219345093, 1.2628999948501587, 1.2595000267028809, 1.2628999948501587, 1.2348999977111816, 1.2319999933242798, 1.2222000360488892, 1.236199975013733, 1.242900013923645, 1.1953999996185303, 1.2422000169754028, 1.1806000471115112, 1.1672999858856201, 1.1433000564575195, 1.1629999876022339, 1.1520999670028687, 1.052299976348877, 1.1450999975204468, 1.1563999652862549, 0.9854000210762024, 1.1151000261306763, 1.0976999998092651, 0.878000020980835, 1.1746000051498413, 0.9223999977111816, 0.9715999960899353, 0.8528000116348267, 0.7752000093460083, 0.8309000134468079, 0.8154000043869019, 0.7597000002861023, 0.9614999890327454, 0.8125, 0.8562999963760376, 0.9506000280380249, 0.544700026512146, 0.7196999788284302, 0.5252000093460083, 0.5885000228881836, 0.529699981212616, 0.2919999957084656, 0.30489999055862427, 0.3082999885082245, 0.49470001459121704, 0.6007999777793884, 0.5245000123977661, 0.36079999804496765, 0.21469999849796295, 0.08619999885559082, 0.24079999327659607, 1.720900058746338, 1.719099998474121, 1.717900037765503, 1.7136000394821167, 1.7029000520706177, 1.7027000188827515, 1.7024999856948853, 1.6936999559402466, 1.693600058555603, 1.680999994277954, 1.679800033569336, 1.6797000169754028, 1.6758999824523926, 1.667099952697754, 1.663100004196167, 1.6617000102996826, 1.6540000438690186, 1.6483999490737915, 1.645900011062622, 1.6442999839782715, 1.6333999633789062, 1.6312999725341797, 1.628999948501587, 1.622499942779541, 1.617900013923645, 1.607200026512146, 1.5999000072479248, 1.5986000299453735, 1.592900037765503, 1.5877000093460083, 1.516700029373169, 1.5499999523162842, 1.5489000082015991, 1.5852999687194824, 1.5634000301361084, 1.5657000541687012, 1.5098999738693237, 1.3494000434875488, 1.093999981880188, 1.2884999513626099, 1.249899983406067, 1.385599970817566, 1.033400058746338, 1.0020999908447266, 1.447100043296814, 1.1877000331878662, 0.7723000049591064, 0.7932000160217285, 1.0777000188827515, 1.155400037765503, 0.5690000057220459, 0.5529000163078308, 0.8572999835014343, 1.0326999425888062, 0.40400001406669617, 0.5424000024795532, 0.5849000215530396, 0.9508000016212463, 0.38510000705718994, 0.4131999909877777, 0.5943999886512756, 0.38109999895095825, 0.4957999885082245, 0.35920000076293945, 0.4244999885559082, 0.7037000060081482, 0.3781999945640564, 0.5971999764442444, 0.7670000195503235, 0.14480000734329224, 0.4894999861717224, 0.09130000323057175, 0.12219999730587006, 0.28760001063346863, -0.021299999207258224, 0.17520000040531158, 1.96589994430542, 1.9656000137329102, 1.9656000137329102, 1.9651999473571777, 1.9595999717712402, 1.9594999551773071, 1.9586000442504883, 1.9581999778747559, 1.951200008392334, 1.9501999616622925, 1.9467999935150146, 1.9428999423980713, 1.9330999851226807, 1.9278000593185425, 1.924399971961975, 1.9164999723434448, 1.9074000120162964, 1.8924000263214111, 1.8844000101089478, 1.871500015258789, 1.8701000213623047, 1.8676999807357788, 1.8655999898910522, 1.860200047492981, 1.8550000190734863, 1.840499997138977, 1.8393000364303589, 1.820199966430664, 1.7894999980926514, 1.7867000102996826, 1.7767000198364258, 1.683500051498413, 1.7755999565124512, 1.503100037574768, 1.3524999618530273, 1.2785999774932861, 1.6095999479293823, 1.739400029182434, 1.4543999433517456, 1.2462999820709229, 1.3842999935150146, 1.291599988937378, 1.6571999788284302, 1.0992000102996826, 1.0084999799728394, 1.2625000476837158, 1.0176000595092773, 0.7741000056266785, 0.8325999975204468, 0.7103000283241272, 0.857200026512146, 0.8560000061988831, 1.1119999885559082, 0.8357999920845032, 0.5468000173568726, 0.8296999931335449, 0.6567000150680542, 1.0982999801635742, 0.8302000164985657, 0.7727000117301941, 0.34290000796318054, 0.13339999318122864, 0.4124999940395355, 0.39469999074935913, 0.32429999113082886, 1.7723000049591064, 1.7717000246047974, 1.7657999992370605, 1.7620999813079834, 1.7608000040054321, 1.7597999572753906, 1.7580000162124634, 1.753000020980835, 1.7496000528335571, 1.7448999881744385, 1.743499994277954, 1.7411999702453613, 1.7395000457763672, 1.7307000160217285, 1.729699969291687, 1.725600004196167, 1.7252999544143677, 1.725000023841858, 1.7240999937057495, 1.7239999771118164, 1.715499997138977, 1.7103999853134155, 1.7010999917984009, 1.700700044631958, 1.695199966430664, 1.6833000183105469, 1.6819000244140625, 1.6815999746322632, 1.6786999702453613, 1.6704000234603882, 1.6374000310897827, 1.6420999765396118, 1.6383999586105347, 1.6176999807357788, 1.621500015258789, 1.5561000108718872, 1.597499966621399, 1.6505999565124512, 1.3588000535964966, 1.5729000568389893, 1.4697999954223633, 1.532099962234497, 1.3107999563217163, 1.4536000490188599, 1.3799999952316284, 1.167799949645996, 1.0413000583648682, 1.3121999502182007, 1.2374999523162842, 1.169700026512146, 1.104699969291687, 0.8471999764442444, 0.9879000186920166, 1.0115000009536743, 0.6007999777793884, 0.621999979019165, 1.028499960899353, 0.43529999256134033, 0.652999997138977, 0.9384999871253967, 1.0336999893188477, 0.24459999799728394, 0.29319998621940613, 0.46810001134872437, 0.5465999841690063, 0.7408000230789185, 0.5759999752044678, 0.5605999827384949, 0.021800000220537186, 2.1463000774383545, 2.1463000774383545, 2.1461000442504883, 2.1461000442504883, 2.145400047302246, 2.14520001411438, 2.1417999267578125, 2.140899896621704, 2.140700101852417, 2.135699987411499, 2.1350998878479004, 2.1298999786376953, 2.1298000812530518, 2.1291000843048096, 2.1282999515533447, 2.125, 2.110599994659424, 2.086199998855591, 2.0836000442504883, 2.0827999114990234, 2.072999954223633, 1.9800000190734863, 1.9752000570297241, 1.9736000299453735, 1.9491000175476074, 1.9471999406814575, 1.944599986076355, 1.93149995803833, 1.926800012588501, 1.9258999824523926, 1.898900032043457, 1.9045000076293945, 1.905500054359436, 1.8775999546051025, 1.6311999559402466, 1.7175999879837036, 1.6684999465942383, 1.6806999444961548, 1.475100040435791, 1.4124000072479248, 1.5039000511169434, 1.1883000135421753, 1.3938000202178955, 1.0823999643325806, 1.4797999858856201, 0.9695000052452087, 1.1527999639511108, 1.0343999862670898, 1.4127999544143677, 1.1202000379562378, 1.3595000505447388, 0.7964000105857849, 0.6359000205993652, 0.41130000352859497, 0.6136000156402588, 0.44190001487731934, 1.1263999938964844, 0.7141000032424927, 0.8041999936103821, 0.44190001487731934, 0.44830000400543213, 1.0322999954223633, 0.267300009727478, 0.22300000488758087, 0.11010000109672546, 2.1429998874664307, 2.1424999237060547, 2.092099905014038, 2.061800003051758, 2.037600040435791, 2.0343000888824463, 2.0081000328063965, 2.005199909210205, 1.9967999458312988, 1.9864000082015991, 1.9766000509262085, 1.962499976158142, 1.9591000080108643, 1.9586000442504883, 1.9499000310897827, 1.930400013923645, 1.9240000247955322, 1.917199969291687, 1.9005999565124512, 1.9004000425338745, 1.8954999446868896, 1.8916000127792358, 1.8848999738693237, 1.8819999694824219, 1.88100004196167, 1.8712999820709229, 1.8705999851226807, 1.8681000471115112, 1.8675999641418457, 1.8598999977111816, 1.853700041770935, 1.8472000360488892, 1.8481999635696411, 1.7940000295639038, 1.7319999933242798, 1.7273000478744507, 1.6919000148773193, 1.7623000144958496, 1.6262999773025513, 1.7026000022888184, 1.7089999914169312, 1.5944000482559204, 1.4520000219345093, 1.5997999906539917, 1.2639000415802002, 1.5916999578475952, 1.1719000339508057, 1.2582000494003296, 1.2003999948501587, 1.042099952697754, 1.1965999603271484, 0.801800012588501, 1.2690000534057617, 1.2031999826431274, 0.8847000002861023, 0.7376000285148621, 0.8809999823570251, 0.6550999879837036, 0.6930999755859375, 0.6104999780654907, 0.5336999893188477, 0.7985000014305115, 0.30230000615119934, 0.19499999284744263], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.824999809265137, -6.960899829864502, -7.145299911499023, -7.138299942016602, -7.862800121307373, -6.857500076293945, -7.713699817657471, -8.195699691772461, -7.055500030517578, -6.6880998611450195, -8.219200134277344, -7.768899917602539, -7.114299774169922, -7.206399917602539, -7.203100204467773, -7.639900207519531, -8.281000137329102, -5.83489990234375, -6.65310001373291, -8.026700019836426, -7.3308000564575195, -8.065799713134766, -7.025199890136719, -7.7754998207092285, -8.43649959564209, -6.868100166320801, -8.392000198364258, -5.851399898529053, -8.082500457763672, -7.7042999267578125, -7.144400119781494, -7.201200008392334, -7.672699928283691, -5.754199981689453, -6.102700233459473, -5.958499908447266, -6.619100093841553, -6.836999893188477, -5.3470001220703125, -6.9120001792907715, -5.728499889373779, -5.730000019073486, -5.585700035095215, -5.966400146484375, -5.952400207519531, -4.831999778747559, -5.912899971008301, -6.2444000244140625, -5.070799827575684, -6.067399978637695, -6.077899932861328, -4.933599948883057, -6.593800067901611, -5.60290002822876, -5.809800148010254, -5.378699779510498, -5.151400089263916, -5.475500106811523, -5.46589994430542, -5.308899879455566, -5.922999858856201, -5.664999961853027, -5.821800231933594, -6.074999809265137, -5.432000160217285, -5.765500068664551, -5.595900058746338, -5.711599826812744, -5.673099994659424, -5.524899959564209, -5.556600093841553, -5.561200141906738, -5.694200038909912, -5.736999988555908, -5.726200103759766, -5.695199966430664, -5.737599849700928, -5.740900039672852, -5.799300193786621, -7.163300037384033, -7.5960001945495605, -7.59089994430542, -7.194200038909912, -6.786600112915039, -7.363399982452393, -7.619100093841553, -8.237500190734863, -8.142399787902832, -6.591000080108643, -7.163099765777588, -6.266300201416016, -7.2133002281188965, -6.8094000816345215, -7.553500175476074, -7.790800094604492, -6.394899845123291, -8.31879997253418, -7.729599952697754, -7.4257001876831055, -6.870200157165527, -7.312699794769287, -8.185500144958496, -7.71150016784668, -7.630300045013428, -7.4079999923706055, -7.651100158691406, -6.339300155639648, -6.956999778747559, -6.892000198364258, -4.714000225067139, -5.91480016708374, -6.558300018310547, -7.13539981842041, -6.89739990234375, -7.079400062561035, -6.861199855804443, -5.921999931335449, -4.901700019836426, -5.884900093078613, -5.777900218963623, -6.493100166320801, -5.252600193023682, -5.2596001625061035, -6.715099811553955, -5.9166998863220215, -4.805200099945068, -5.122200012207031, -5.844200134277344, -6.214300155639648, -5.219699859619141, -5.241000175476074, -5.825699806213379, -6.126999855041504, -5.270599842071533, -5.497700214385986, -5.6433000564575195, -6.094799995422363, -5.441999912261963, -5.484600067138672, -5.7342000007629395, -5.571199893951416, -5.706500053405762, -5.632900238037109, -5.704800128936768, -5.905200004577637, -5.708499908447266, -5.849100112915039, -5.961400032043457, -5.702700138092041, -5.8445000648498535, -5.720200061798096, -5.745299816131592, -5.8109002113342285, -5.779699802398682, -5.81790018081665, -6.788899898529053, -7.474400043487549, -7.170899868011475, -7.129899978637695, -7.57289981842041, -7.190800189971924, -6.5665998458862305, -6.997700214385986, -7.635000228881836, -7.609899997711182, -7.544000148773193, -6.48330020904541, -5.932799816131592, -4.904600143432617, -6.458199977874756, -7.3765997886657715, -6.477700233459473, -6.799799919128418, -7.289899826049805, -6.201300144195557, -5.605299949645996, -7.027500152587891, -6.007400035858154, -7.856400012969971, -6.945700168609619, -7.8221001625061035, -7.099899768829346, -4.471399784088135, -7.651700019836426, -6.648499965667725, -6.650899887084961, -5.006800174713135, -6.733099937438965, -5.517099857330322, -4.945099830627441, -4.922599792480469, -6.265200138092041, -6.763299942016602, -5.776400089263916, -5.360799789428711, -5.759900093078613, -5.500199794769287, -6.55620002746582, -5.447700023651123, -5.4222002029418945, -5.913099765777588, -5.521399974822998, -5.1828999519348145, -5.283100128173828, -5.151199817657471, -5.388599872589111, -5.481900215148926, -5.821400165557861, -5.498199939727783, -5.270100116729736, -5.655399799346924, -5.546599864959717, -5.874899864196777, -5.7104997634887695, -5.717299938201904, -5.526700019836426, -5.443999767303467, -5.648799896240234, -5.6774001121521, -5.7677998542785645, -5.969299793243408, -8.186400413513184, -7.708600044250488, -6.718800067901611, -6.672699928283691, -7.9039998054504395, -7.749100208282471, -6.746300220489502, -7.365300178527832, -8.293800354003906, -6.944900035858154, -7.125, -6.660299777984619, -6.039599895477295, -5.451600074768066, -6.88070011138916, -6.865699768066406, -5.894400119781494, -7.296299934387207, -7.599800109863281, -6.370200157165527, -6.6528000831604, -8.246600151062012, -7.075200080871582, -6.171999931335449, -6.662499904632568, -7.907599925994873, -7.820300102233887, -7.9832000732421875, -6.079100131988525, -4.663000106811523, -5.697000026702881, -6.387199878692627, -5.738399982452393, -6.307799816131592, -5.76639986038208, -6.291200160980225, -6.8491997718811035, -4.7133002281188965, -6.290599822998047, -5.732699871063232, -6.2845001220703125, -5.445199966430664, -6.14709997177124, -5.945099830627441, -5.28000020980835, -5.013700008392334, -5.755300045013428, -5.701300144195557, -5.6290998458862305, -5.591400146484375, -5.1946001052856445, -5.44950008392334, -5.609399795532227, -5.05109977722168, -5.166600227355957, -5.800600051879883, -5.35860013961792, -5.56220006942749, -5.793099880218506, -5.875100135803223, -5.430099964141846, -5.588799953460693, -5.679699897766113, -5.728400230407715, -5.7932000160217285, -5.806000232696533, -5.8180999755859375, -5.805300235748291, -6.270699977874756, -6.310100078582764, -6.793300151824951, -6.889100074768066, -6.91540002822876, -7.643199920654297, -7.086999893188477, -6.049799919128418, -6.934599876403809, -7.14769983291626, -6.4166998863220215, -6.1433000564575195, -6.326399803161621, -6.851099967956543, -7.721799850463867, -6.842299938201904, -7.346399784088135, -6.331399917602539, -6.361400127410889, -6.062099933624268, -7.855800151824951, -7.435299873352051, -7.720099925994873, -7.2144999504089355, -6.471399784088135, -7.058300018310547, -8.084400177001953, -6.523099899291992, -6.283199787139893, -8.15939998626709, -4.528500080108643, -5.867199897766113, -5.986800193786621, -6.564899921417236, -4.693900108337402, -5.621099948883057, -5.4274001121521, -5.611999988555908, -4.727200031280518, -4.849299907684326, -5.469299793243408, -4.679299831390381, -5.303199768066406, -4.592199802398682, -5.520500183105469, -4.607900142669678, -4.976500034332275, -4.847599983215332, -5.674499988555908, -5.250999927520752, -5.625800132751465, -5.35129976272583, -5.321100234985352, -5.240600109100342, -5.379499912261963, -5.346700191497803, -5.782400131225586, -5.645500183105469, -5.7042999267578125, -5.644800186157227, -5.667399883270264, -5.799499988555908, -5.669600009918213, -5.692299842834473, -5.683700084686279, -6.786799907684326, -5.937600135803223, -5.255300045013428, -6.411200046539307, -7.791800022125244, -6.778299808502197, -6.404699802398682, -8.376700401306152, -7.7779998779296875, -6.921500205993652, -7.408299922943115, -6.386300086975098, -8.018099784851074, -6.785299777984619, -5.866399765014648, -5.438399791717529, -7.953999996185303, -6.221700191497803, -7.445400238037109, -6.364999771118164, -4.765699863433838, -7.235799789428711, -7.8368000984191895, -6.7256999015808105, -6.912799835205078, -6.984399795532227, -5.7982001304626465, -6.712100028991699, -8.581700325012207, -7.2179999351501465, -5.979700088500977, -6.128900051116943, -6.587699890136719, -5.380099773406982, -4.458399772644043, -4.69290018081665, -4.553899765014648, -5.8069000244140625, -5.2941999435424805, -5.89709997177124, -5.946700096130371, -5.497300148010254, -4.894400119781494, -5.6194000244140625, -4.4944000244140625, -5.8643999099731445, -5.1031999588012695, -5.322999954223633, -5.2342000007629395, -5.056399822235107, -5.350299835205078, -4.850200176239014, -5.489699840545654, -5.584700107574463, -5.3180999755859375, -5.2581000328063965, -5.440700054168701, -5.3379998207092285, -5.368199825286865, -5.326399803161621, -5.4430999755859375, -5.561100006103516, -5.545199871063232, -5.672599792480469]}, \"token.table\": {\"Topic\": [1, 2, 3, 4, 5, 6, 4, 5, 3, 5, 6, 1, 1, 2, 1, 2, 3, 5, 6, 1, 3, 5, 6, 1, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 3, 2, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 5, 1, 2, 3, 4, 1, 2, 3, 1, 2, 1, 1, 3, 4, 5, 1, 2, 5, 1, 3, 5, 5, 1, 2, 3, 5, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 5, 1, 2, 3, 4, 5, 6, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 5, 1, 2, 1, 2, 3, 4, 5, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 5, 1, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 5, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 3, 4, 1, 2, 3, 4, 5, 6, 3, 5, 6, 1, 3, 4, 1, 3, 3, 1, 2, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 3, 4, 1, 2, 3, 4, 5, 6, 1, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 3, 5, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 6, 2, 3, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 1, 4, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 1, 2, 3, 4, 5, 6, 1, 2, 2, 4, 1, 2, 4, 5, 6, 1, 2, 3, 4, 2, 3, 4, 6, 1, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 1, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 4, 5, 6, 1, 2, 3, 5, 6, 2, 4, 5, 6, 1, 2, 3, 5, 6, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 2, 3, 6, 1, 2, 3, 4, 5, 1, 2, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 4, 6, 1, 2, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 1, 3, 6, 3, 5, 1, 1, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 6, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 1, 2, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 2, 3, 6, 1, 2, 3, 4, 5, 6, 3, 5, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 4, 6, 1, 2, 3, 5, 1, 3, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 4, 6, 1, 2, 5, 6, 2, 3, 5, 2, 4, 1, 2, 3, 5, 1, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 3, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 6, 1, 2, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 5, 1, 2, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 1, 3, 6, 1, 2, 3, 4, 5, 6, 1, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 4, 4, 1, 3, 1, 2, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 1, 2, 3, 4, 5, 6, 1, 4, 6, 1, 2, 3, 4, 5, 6, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 6, 2, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 3, 4, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 1, 2, 2, 3, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 5, 6, 1, 2, 1, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 4, 6, 1, 1, 2, 3, 4, 5, 6, 4, 6, 5, 3, 4, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 3, 1, 2, 3, 4, 5, 6, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 5, 6, 1, 3, 5, 1, 2, 3, 4, 5, 6, 3, 5, 1, 2, 3, 4, 5, 6, 1, 2, 4, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 1, 2, 6, 1, 2, 3, 4, 5, 6, 3, 2, 1, 1, 3, 4, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6, 2, 4], \"Freq\": [0.14659254252910614, 0.310871422290802, 0.12754571437835693, 0.26257410645484924, 0.13060681521892548, 0.0221079234033823, 0.003737050574272871, 0.9940555095672607, 0.8353143930435181, 0.15662144124507904, 1.0017244815826416, 0.9980432391166687, 0.019369998946785927, 0.9684999585151672, 0.05088335648179054, 0.001106159994378686, 0.42034077644348145, 0.5254259705543518, 0.002212319988757372, 0.0054018315859138966, 0.9669278860092163, 0.02430824190378189, 0.0027009157929569483, 0.015264533460140228, 0.9820182919502258, 0.005088177975267172, 0.0032066721469163895, 0.0005344453966245055, 0.3297528028488159, 0.0005344453966245055, 0.029394496232271194, 0.6365244388580322, 0.07995481044054031, 0.07995481044054031, 0.00940644834190607, 0.8230642080307007, 0.00940644834190607, 0.9842871427536011, 0.011313645169138908, 0.219324991106987, 0.781941294670105, 0.026400433853268623, 0.9372153878211975, 0.026400433853268623, 0.3969705402851105, 0.3208320140838623, 0.08296474069356918, 0.012077146209776402, 0.09556698054075241, 0.0918913260102272, 0.051678430289030075, 0.2936972379684448, 0.01790836825966835, 0.1212652325630188, 0.5111559629440308, 0.004605008754879236, 0.016235269606113434, 0.9822338223457336, 0.06163714826107025, 0.10272857546806335, 0.8269650340080261, 0.005136428866535425, 0.014411949552595615, 0.023059118539094925, 0.9617574214935303, 0.054712943732738495, 0.9450417757034302, 1.0007108449935913, 0.015240101143717766, 0.0038100252859294415, 0.17907118797302246, 0.8039153218269348, 0.016250254586338997, 0.008125127293169498, 0.9750153422355652, 0.00357006280682981, 0.05712100490927696, 0.9389265179634094, 0.9920081496238708, 0.0745653510093689, 0.1242755800485611, 0.02485511638224125, 0.79536372423172, 0.9956896901130676, 0.033088814467191696, 0.15908083319664001, 0.08081306517124176, 0.04517895728349686, 0.021634994074702263, 0.6598672866821289, 0.18026350438594818, 0.05599674955010414, 0.48709502816200256, 0.12580092251300812, 0.04755888506770134, 0.10355563461780548, 0.12164964526891708, 0.028623444959521294, 0.5996611714363098, 0.12451198697090149, 0.07728330045938492, 0.048659857362508774, 0.4649170935153961, 0.04822791367769241, 0.3838942050933838, 0.012217738665640354, 0.08102289587259293, 0.010931660421192646, 0.030852554365992546, 0.06684720516204834, 0.06427615880966187, 0.7867401838302612, 0.005142092704772949, 0.04884988069534302, 0.005577055271714926, 0.022308221086859703, 0.005577055271714926, 0.9536764621734619, 0.005577055271714926, 0.011154110543429852, 0.992101788520813, 0.377485454082489, 0.09538019448518753, 0.19736362993717194, 0.13353227078914642, 0.0990486592054367, 0.09758127480745316, 0.008214280009269714, 0.0016428560484200716, 0.13142848014831543, 0.04599997028708458, 0.0049285683780908585, 0.8082851767539978, 0.007267432752996683, 0.007267432752996683, 0.0944766253232956, 0.10174405574798584, 0.029069731011986732, 0.7630804181098938, 0.9967576861381531, 0.0077268038876354694, 0.019645599648356438, 0.9822799563407898, 0.022011734545230865, 0.034589868038892746, 0.20753920078277588, 0.6608012318611145, 0.07232426851987839, 0.002246095333248377, 0.030277255922555923, 0.9083176851272583, 0.045415885746479034, 0.015138627961277962, 0.02163456752896309, 0.02163456752896309, 0.024338888004422188, 0.9248777627944946, 0.002704320941120386, 0.002704320941120386, 0.3775765597820282, 0.0419529490172863, 0.01249662321060896, 0.05802003666758537, 0.0928320586681366, 0.416851669549942, 0.4709986746311188, 0.032253168523311615, 0.0035836854949593544, 0.05887483432888985, 0.15051479637622833, 0.2841350734233856, 0.016826285049319267, 0.05047885328531265, 0.08413141965866089, 0.8413141965866089, 0.11081376671791077, 0.8950343132019043, 0.05563921481370926, 0.5024221539497375, 0.0856843963265419, 0.10515812039375305, 0.046736940741539, 0.20419593155384064, 0.9776100516319275, 0.01793779991567135, 0.04934504255652428, 0.8853081464767456, 0.002902649575844407, 0.01741589792072773, 0.02612384594976902, 0.01741589792072773, 0.29046037793159485, 0.4212089478969574, 0.07116693258285522, 0.024825673550367355, 0.17129714787006378, 0.02068806253373623, 0.6246165037155151, 0.034364014863967896, 0.13678225874900818, 0.1071348711848259, 0.030994994565844536, 0.06535901129245758, 0.03573910519480705, 0.0714782103896141, 0.7964714765548706, 0.07658379524946213, 0.010211172513663769, 0.005105586256831884, 0.03521597757935524, 0.0725034773349762, 0.029001392424106598, 0.7271063327789307, 0.05593125522136688, 0.08078959584236145, 0.1604035496711731, 0.016884583979845047, 0.8189023733139038, 0.00823122262954712, 0.9795154929161072, 0.00823122262954712, 0.13102518022060394, 0.06458988040685654, 0.5093373656272888, 0.0913485437631607, 0.0525946170091629, 0.15132486820220947, 0.9160119891166687, 0.02013213187456131, 0.0345122255384922, 0.004314028192311525, 0.01797511801123619, 0.0071900468319654465, 0.9846628904342651, 0.016275418922305107, 0.9511287212371826, 0.004763582721352577, 0.028581496328115463, 0.004763582721352577, 0.011115026660263538, 0.953032910823822, 0.016857894137501717, 0.007867016829550266, 0.007867016829550266, 0.010114735923707485, 0.004495438188314438, 0.032188646495342255, 0.9598069787025452, 0.002926240675151348, 0.005852481350302696, 0.0499204583466053, 0.16362817585468292, 0.7848605513572693, 0.26989835500717163, 0.5864613652229309, 0.14377762377262115, 0.21414567530155182, 0.23335157334804535, 0.14308388531208038, 0.06049855425953865, 0.3284207284450531, 0.021126480773091316, 0.0100784320384264, 0.9826471209526062, 0.0050392160192132, 0.0766134262084961, 0.1751164048910141, 0.008755819872021675, 0.061290740966796875, 0.030645370483398438, 0.6479306817054749, 0.0975976213812828, 0.2110935002565384, 0.02252252772450447, 0.48092222213745117, 0.14750048518180847, 0.03974564000964165, 0.11300633102655411, 0.3156733810901642, 0.03315754607319832, 0.3163500428199768, 0.1816898137331009, 0.03958604857325554, 0.04057498276233673, 0.03941569849848747, 0.6294918656349182, 0.23301634192466736, 0.04521212354302406, 0.012752138078212738, 0.11225497722625732, 0.2735025882720947, 0.007610506843775511, 0.17884691059589386, 0.37006089091300964, 0.05803011357784271, 0.07007071375846863, 0.16696849465370178, 0.3035062849521637, 0.1801818311214447, 0.22062264382839203, 0.05845899507403374, 0.18530449271202087, 0.8149186372756958, 0.2188696414232254, 0.2701908051967621, 0.14377471804618835, 0.14717096090316772, 0.10717064887285233, 0.11320843547582626, 1.000801920890808, 0.8821013569831848, 0.0695502981543541, 0.028837930411100388, 0.0033926975447684526, 0.01696348749101162, 0.0016963487723842263, 0.9819658994674683, 0.010596754029393196, 0.0035322513431310654, 0.1570325791835785, 0.29188209772109985, 0.32282161712646484, 0.054873835295438766, 0.15820010006427765, 0.015177869237959385, 0.8288459181785583, 0.07578019797801971, 0.09472525119781494, 0.009418133646249771, 0.009418133646249771, 0.9794858694076538, 0.008033445104956627, 0.9881137609481812, 0.9952181577682495, 0.1557123064994812, 0.01946403831243515, 0.807757556438446, 0.014598028734326363, 0.009818379767239094, 0.024545948952436447, 0.0024545949418097734, 0.9597465991973877, 0.0024545949418097734, 0.004909189883619547, 0.38626426458358765, 0.0526215098798275, 0.019033310934901237, 0.04926268756389618, 0.45567986369132996, 0.038066621869802475, 0.013258785009384155, 0.02651757001876831, 0.9678913354873657, 0.016578922048211098, 0.09671037644147873, 0.0027631535194814205, 0.8399986624717712, 0.005526307038962841, 0.03868414834141731, 0.11919905990362167, 0.11047717928886414, 0.07510510087013245, 0.2713474631309509, 0.2592337429523468, 0.16474667191505432, 0.9946929216384888, 0.07560580223798752, 0.9072695970535278, 0.015121160075068474, 0.20203620195388794, 0.23847652971744537, 0.1540641486644745, 0.037824131548404694, 0.03459523990750313, 0.33257558941841125, 0.986657977104187, 0.012436865828931332, 0.15061014890670776, 0.36115697026252747, 0.0714629739522934, 0.19978897273540497, 0.1513785570859909, 0.0653156265616417, 0.3528423309326172, 0.30759498476982117, 0.0739620178937912, 0.05612412095069885, 0.12181979417800903, 0.08788427710533142, 0.010004667565226555, 0.297638863325119, 0.0025011668913066387, 0.06252916902303696, 0.6202893853187561, 0.007503500673919916, 0.003045029705390334, 0.3684485852718353, 0.6272761225700378, 0.9843452572822571, 0.011580532416701317, 0.01659451611340046, 0.048746392130851746, 0.09023267775774002, 0.47709232568740845, 0.3609307110309601, 0.006222943309694529, 0.049832090735435486, 0.03683241456747055, 0.42032286524772644, 0.007944246754050255, 0.0967753678560257, 0.3878236711025238, 0.3189287781715393, 0.036288369446992874, 0.5029261708259583, 0.0669545978307724, 0.04702154919505119, 0.02811070904135704, 0.279083788394928, 0.06843774765729904, 0.21153485774993896, 0.1706499606370926, 0.03599647805094719, 0.23464369773864746, 0.46895831823349, 0.10096286237239838, 0.14719818532466888, 0.1321009397506714, 0.023117663338780403, 0.1278548389673233, 0.5696704983711243, 0.03394937142729759, 0.32116106152534485, 0.04345519840717316, 0.015616711229085922, 0.016974685713648796, 0.2222813069820404, 0.1164112389087677, 0.19340765476226807, 0.20303219556808472, 0.1301605999469757, 0.13474372029304504, 0.3023929297924042, 0.2626599073410034, 0.07981766760349274, 0.17370012402534485, 0.11111181974411011, 0.0706755518913269, 0.1805139183998108, 0.26074233651161194, 0.12945948541164398, 0.1604568213224411, 0.18188145756721497, 0.08706606179475784, 0.9997953772544861, 0.0945725366473198, 0.5254029631614685, 0.10928381979465485, 0.1355539709329605, 0.024168536067008972, 0.11138542741537094, 0.015272834338247776, 0.02036377787590027, 0.07127322256565094, 0.8858243823051453, 0.005090944468975067, 0.004221469163894653, 0.0021107345819473267, 0.029550284147262573, 0.9540520310401917, 0.00633220374584198, 0.00633220374584198, 0.010791314765810966, 0.8848878145217896, 0.09712183475494385, 0.009748500771820545, 0.004874250385910273, 0.9845985174179077, 0.038382936269044876, 0.0018277589697390795, 0.9083961844444275, 0.005483276676386595, 0.04569397494196892, 0.012162229046225548, 0.6486522555351257, 0.009459511376917362, 0.027027176693081856, 0.009459511376917362, 0.293244868516922, 0.013092919252812862, 0.06546459347009659, 0.004364306572824717, 0.9121400117874146, 0.004364306572824717, 0.14071673154830933, 0.12203751504421234, 0.007471684832125902, 0.12203751504421234, 0.02988673932850361, 0.5778102874755859, 0.9934852719306946, 0.005081766285002232, 0.06060231477022171, 0.08080308884382248, 0.06060231477022171, 0.8080308437347412, 0.2589835822582245, 0.04434112831950188, 0.14440296590328217, 0.1836429089307785, 0.1526433527469635, 0.21581965684890747, 0.9888752698898315, 0.016211070120334625, 0.15946145355701447, 0.061658427119255066, 0.4252305328845978, 0.1467045396566391, 0.10418148338794708, 0.10205533355474472, 0.004053919576108456, 0.9607789516448975, 0.004053919576108456, 0.008107839152216911, 0.024323517456650734, 0.7941798567771912, 0.09457182139158249, 0.061806779354810715, 0.014520869590342045, 0.030531059950590134, 0.0044679599814116955, 0.902573823928833, 0.039422765374183655, 0.007262087892740965, 0.0010374411940574646, 0.04875973239541054, 0.17982439696788788, 0.00801197811961174, 0.01157285738736391, 0.6302756071090698, 0.060534942895174026, 0.11127746850252151, 0.7323275208473206, 0.027393797412514687, 0.16162340342998505, 0.015523151494562626, 0.05022196099162102, 0.01187064591795206, 0.10931113362312317, 0.018218522891402245, 0.8562705516815186, 0.8422894477844238, 0.15814414620399475, 0.0334339439868927, 0.0334339439868927, 0.0334339439868927, 0.869282603263855, 0.013998781330883503, 0.11199025064706802, 0.24031242728233337, 0.46662604808807373, 0.0489957369863987, 0.11821193248033524, 0.11191573739051819, 0.8102699518203735, 0.03581303730607033, 0.022383147850632668, 0.017906518653035164, 0.0871800035238266, 0.7591925263404846, 0.01453000120818615, 0.03632500395178795, 0.0036325003020465374, 0.1017100065946579, 1.0018138885498047, 0.983992874622345, 0.015743885189294815, 0.12336184829473495, 0.11627887189388275, 0.01298545766621828, 0.3095851242542267, 0.17648418247699738, 0.261479914188385, 0.9993420839309692, 0.1946251094341278, 0.5025593638420105, 0.12930572032928467, 0.042657557874917984, 0.09464645385742188, 0.03732536360621452, 0.09569800645112991, 0.051100876182317734, 0.11056371033191681, 0.5472439527511597, 0.0696830153465271, 0.1254294216632843, 0.06624874472618103, 0.9274824857711792, 0.03757435083389282, 0.954388439655304, 0.007514869794249535, 0.09572368115186691, 0.5339716672897339, 0.08448655158281326, 0.020393306389451027, 0.019977115094661713, 0.2451358586549759, 0.9540532827377319, 0.026724182069301605, 0.0026724182534962893, 0.008017254993319511, 0.010689673013985157, 0.11849464476108551, 0.15192574262619019, 0.11069405823945999, 0.22770288586616516, 0.32873907685279846, 0.06203324720263481, 0.10890697687864304, 0.009075582027435303, 0.12705814838409424, 0.7532732486724854, 0.089067243039608, 0.14269913733005524, 0.04788561537861824, 0.4750252962112427, 0.005746273789554834, 0.23847036063671112, 1.0036975145339966, 0.21749216318130493, 0.32460543513298035, 0.0999288335442543, 0.0999288335442543, 0.10384761542081833, 0.1534854620695114, 0.228033646941185, 0.03302556276321411, 0.04560672864317894, 0.6746650338172913, 0.01572645828127861, 0.0015726457349956036, 0.9863191843032837, 0.01174189429730177, 0.871788501739502, 0.055389873683452606, 0.0072247665375471115, 0.013245404697954655, 0.03732796013355255, 0.014449533075094223, 0.9959642887115479, 0.0546514168381691, 0.006831427104771137, 0.02732570841908455, 0.10247141122817993, 0.03415713831782341, 0.7719512581825256, 0.008145429193973541, 0.4870966672897339, 0.00488725770264864, 0.014661773107945919, 0.48003730177879333, 0.005973314866423607, 0.04382266849279404, 0.06802324950695038, 0.16351743042469025, 0.5461481809616089, 0.13604649901390076, 0.04251452907919884, 0.03772297501564026, 0.0026944982819259167, 0.024250483140349388, 0.8191274404525757, 0.09700193256139755, 0.016166988760232925, 0.05212888866662979, 0.938319981098175, 0.343510240316391, 0.26181694865226746, 0.09404688328504562, 0.15940149128437042, 0.08368578553199768, 0.05778304114937782, 0.0745014101266861, 0.2098631113767624, 0.008394524455070496, 0.08184661716222763, 0.030430153012275696, 0.5960112810134888, 0.014051591977477074, 0.2990981638431549, 0.002007370349019766, 0.002007370349019766, 0.6825059056282043, 0.05679994076490402, 0.20285692811012268, 0.0013523795641958714, 0.029752351343631744, 0.005409518256783485, 0.7045897245407104, 0.18282507359981537, 0.0983552634716034, 0.006364163942635059, 0.027192337438464165, 0.5970743298530579, 0.08851973712444305, 0.032546352595090866, 0.04881953075528145, 0.9112979173660278, 0.09008092433214188, 0.2129562944173813, 0.0610225610435009, 0.19427591562271118, 0.21586212515830994, 0.2258249968290329, 0.1456383466720581, 0.06925912201404572, 0.14369650185108185, 0.4563334584236145, 0.12168892472982407, 0.0640808716416359, 0.010705183260142803, 0.8778250813484192, 0.11240442842245102, 0.5927236676216125, 0.10654700547456741, 0.05327350273728371, 0.09311670809984207, 0.09311670809984207, 0.06133168190717697, 0.9981619715690613, 0.004578724503517151, 0.04331681877374649, 0.9313115477561951, 0.06984968483448029, 0.8699460029602051, 0.006349971052259207, 0.006349971052259207, 0.04444979876279831, 0.9793342351913452, 0.009282789193093777, 0.004641394596546888, 0.004641394596546888, 0.9027979969978333, 0.07034789770841599, 0.02344929799437523, 0.011724648997187614, 0.03224566951394081, 0.9673701524734497, 0.3745437562465668, 0.08596086502075195, 0.3029097020626068, 0.07163405418395996, 0.16168828308582306, 0.0034111454151570797, 0.890994131565094, 0.031784601509571075, 0.0030759291257709265, 0.027683362364768982, 0.01332902628928423, 0.03383522108197212, 1.001966118812561, 0.014699021354317665, 0.9848344326019287, 0.13869833946228027, 0.06695782393217087, 0.020725039765238762, 0.006376935169100761, 0.650447428226471, 0.114784836769104, 0.07372613996267319, 0.46257203817367554, 0.0404304638504982, 0.07134787738323212, 0.09037397056818008, 0.26041972637176514, 0.1087382435798645, 0.028191396966576576, 0.05235544964671135, 0.004027342423796654, 0.024164054542779922, 0.7853317856788635, 0.12288848310709, 0.0653662160038948, 0.05490761995315552, 0.0026146485470235348, 0.0052292970940470695, 0.7477895021438599, 0.039692558348178864, 0.9129288196563721, 0.023815535008907318, 0.023815535008907318, 0.9585397839546204, 0.007207066286355257, 0.0036035331431776285, 0.007207066286355257, 0.02162119746208191, 0.017275601625442505, 0.10365361720323563, 0.017275601625442505, 0.08637801557779312, 0.7774021029472351, 0.09397033601999283, 0.023492584004998207, 0.8222404718399048, 0.07047775387763977, 0.021291568875312805, 0.14904098212718964, 0.024840163066983223, 0.00709718931466341, 0.798433780670166, 0.9194807410240173, 0.009994355961680412, 0.059966135770082474, 0.09208802133798599, 0.15167438983917236, 0.49429598450660706, 0.07042025029659271, 0.07177449017763138, 0.11917272955179214, 0.16705411672592163, 0.623824417591095, 0.07009263336658478, 0.007009263150393963, 0.10981179028749466, 0.022196000441908836, 0.004689175635576248, 0.009378351271152496, 0.04689175263047218, 0.942524254322052, 0.11496424674987793, 0.20727130770683289, 0.15272623300552368, 0.03272704780101776, 0.47076600790023804, 0.02181803248822689, 0.07152868807315826, 0.14305737614631653, 0.7510512471199036, 0.12499453127384186, 0.04807482287287712, 0.04807482287287712, 0.014422446489334106, 0.7643896341323853, 0.007655654568225145, 0.9952350854873657, 0.00444123474881053, 0.00444123474881053, 0.9415417313575745, 0.01776493899524212, 0.03552987799048424, 0.12407215684652328, 0.24216492474079132, 0.04110824316740036, 0.1487371027469635, 0.031391751021146774, 0.4125772714614868, 0.12337474524974823, 0.012337475083768368, 0.012337475083768368, 0.8512857556343079, 0.014349499717354774, 0.9614164233207703, 0.021524248644709587, 0.9982567429542542, 0.07450544834136963, 0.14487169682979584, 0.10968857258558273, 0.008278382942080498, 0.02069595642387867, 0.6436442732810974, 0.04523324966430664, 0.04523324966430664, 0.15680859982967377, 0.009046649560332298, 0.7448408603668213, 0.011604505591094494, 0.011604505591094494, 0.023209011182188988, 0.9515694975852966, 0.004667059052735567, 0.12134353071451187, 0.8727400302886963, 0.010725239291787148, 0.9867219924926758, 0.999560534954071, 0.18987411260604858, 0.023734264075756073, 0.783230721950531, 0.14643490314483643, 0.15253636240959167, 0.03355799987912178, 0.39615872502326965, 0.12159326672554016, 0.14948563277721405, 0.016988977789878845, 0.011325985193252563, 0.9513827562332153, 0.016988977789878845, 0.9726056456565857, 0.02372208796441555, 0.03875488415360451, 0.6886444687843323, 0.12520809471607208, 0.0327925942838192, 0.013415152207016945, 0.09986835718154907, 0.09149229526519775, 0.3870617151260376, 0.15997456014156342, 0.0257493294775486, 0.3081701695919037, 0.027666831389069557, 0.9946231842041016, 0.01630529761314392, 0.08704700320959091, 0.002720218850299716, 0.9031126499176025, 0.002720218850299716, 0.002720218850299716, 0.013756291009485722, 0.03439072519540787, 0.8941588997840881, 0.02063443511724472, 0.02063443511724472, 0.006878145504742861, 0.11535163968801498, 0.02563369832932949, 0.009612636640667915, 0.021147800609469414, 0.7805461287498474, 0.047422342002391815, 0.5056827664375305, 0.047462694346904755, 0.32930904626846313, 0.07441681623458862, 0.010547265410423279, 0.03281371295452118, 0.9832229614257812, 0.007661477662622929, 0.0038307388313114643, 0.0025538259651511908, 0.0012769129825755954, 0.0025538259651511908, 0.9925928711891174, 0.00287708081305027, 0.00287708081305027, 0.9959812164306641, 0.9860990643501282, 0.011880711652338505, 0.8779047727584839, 0.05549972876906395, 0.03910208120942116, 0.012613574042916298, 0.010090859606862068, 0.005045429803431034, 0.2583637237548828, 0.25587549805641174, 0.10326255112886429, 0.19491325318813324, 0.12192445993423462, 0.06552402675151825, 0.03724585846066475, 0.03886524587869644, 0.004858155734837055, 0.09230495244264603, 0.045342784374952316, 0.7805436849594116, 0.2814314663410187, 0.05680268257856369, 0.09488629549741745, 0.11876924335956573, 0.05873913690447807, 0.3892274498939514, 0.00696084788069129, 0.08353017270565033, 0.00696084788069129, 0.8979493975639343, 0.23343582451343536, 0.052385203540325165, 0.1553175300359726, 0.09466098248958588, 0.07444213330745697, 0.39059144258499146, 0.11152911186218262, 0.022305821999907494, 0.04461164399981499, 0.8253154158592224, 0.011773870326578617, 0.8536056280136108, 0.12362563610076904, 0.011773870326578617, 0.499367892742157, 0.03272693604230881, 0.20031142234802246, 0.07278922200202942, 0.15968488156795502, 0.03498396649956703, 0.9806986451148987, 0.015087670646607876, 0.9994932413101196, 0.6670886278152466, 0.19593781232833862, 0.04984990134835243, 0.016616635024547577, 0.0630047395825386, 0.007615957409143448, 0.8455328941345215, 0.08728081732988358, 0.0068188137374818325, 0.019092679023742676, 0.025911491364240646, 0.016365153715014458, 0.39786893129348755, 0.09769892692565918, 0.09327816218137741, 0.18832463026046753, 0.14190658926963806, 0.08090001344680786, 0.013714791275560856, 0.987464964389801, 0.037912767380476, 0.9099063873291016, 0.018956383690238, 0.018956383690238, 0.014459141530096531, 0.014459141530096531, 0.028918283060193062, 0.9398441910743713, 0.03437903895974159, 0.8938550353050232, 0.05156856030225754, 0.012258472852408886, 0.009534367360174656, 0.009534367360174656, 0.9575229287147522, 0.008172314614057541, 0.0027241050265729427, 0.17027057707309723, 0.2076258659362793, 0.06254837661981583, 0.4343637228012085, 0.025193097069859505, 0.10077238827943802, 0.033104270696640015, 0.2096603810787201, 0.761398196220398, 0.02835633046925068, 0.12476785480976105, 0.07939772307872772, 0.7712922096252441, 0.022509761154651642, 0.9566648602485657, 0.022509761154651642, 0.013941005803644657, 0.9898114204406738, 0.05386307090520859, 0.8520158529281616, 0.05386307090520859, 0.03917314112186432, 0.1559639722108841, 0.012996996752917767, 0.8318077921867371, 0.007710280362516642, 0.010280373506247997, 0.07453271001577377, 0.005140186753123999, 0.07967289537191391, 0.824999988079071, 0.027357058599591255, 0.714019238948822, 0.06839264929294586, 0.1477281153202057, 0.024621352553367615, 0.016414234414696693, 0.002725177211686969, 0.7003705501556396, 0.2970443069934845, 0.47820088267326355, 0.08739110827445984, 0.060030341148376465, 0.05717175453901291, 0.11761046200990677, 0.20010113716125488, 0.006154018919914961, 0.0011189124779775739, 0.8638004660606384, 0.0016783687751740217, 0.07664550840854645, 0.050351064652204514, 0.012341875582933426, 0.9318115711212158, 0.05553843826055527, 0.026891976594924927, 0.299397349357605, 0.07888313382863998, 0.007171194069087505, 0.01075679063796997, 0.5754883289337158, 0.028656335547566414, 0.13850562274456024, 0.06686478108167648, 0.009552111849188805, 0.014328167773783207, 0.7450647354125977, 0.01924092136323452, 0.01924092136323452, 0.05772276595234871, 0.9043233394622803, 0.030388150364160538, 0.12763023376464844, 0.14950969815254211, 0.6308580040931702, 0.01944841630756855, 0.04254341125488281, 0.034532446414232254, 0.04892096668481827, 0.04028785601258278, 0.8604334592819214, 0.011510815471410751, 0.005755407735705376, 0.2303905189037323, 0.2067055106163025, 0.13206185400485992, 0.16220641136169434, 0.11017116904258728, 0.15861776471138, 0.9818616509437561, 0.00876662228256464, 0.07332932949066162, 0.9288381934165955, 0.09633231908082962, 0.20093858242034912, 0.12056314945220947, 0.043733689934015274, 0.037232737988233566, 0.501164436340332, 0.0066434042528271675, 0.9101463556289673, 0.07639914751052856, 0.0033217021264135838, 0.0047723473981022835, 0.8351607322692871, 0.10021929442882538, 0.0572681650519371, 0.17069274187088013, 0.17496004700660706, 0.12834781408309937, 0.0758269652724266, 0.0351233147084713, 0.41491463780403137, 0.10174452513456345, 0.05087226256728172, 0.8521103858947754, 0.07202741503715515, 0.9273529052734375, 0.06985558569431305, 0.8731948137283325, 0.0399174764752388, 0.00498968455940485, 0.0099793691188097, 0.6976381540298462, 0.031100383028388023, 0.14914047718048096, 0.01767067238688469, 0.07987143844366074, 0.024738941341638565, 0.37136298418045044, 0.14060978591442108, 0.24189059436321259, 0.11798692494630814, 0.06960880756378174, 0.05847139656543732, 0.9792091250419617, 0.014688136987388134, 0.004896045662462711, 0.004896045662462711, 0.009460222907364368, 0.979133129119873, 0.009460222907364368, 0.8315072059631348, 0.04198163375258446, 0.004062738735228777, 0.09344299137592316, 0.012188215740025043, 0.01760520040988922, 0.9972747564315796, 0.6506447792053223, 0.04741540923714638, 0.1706954687833786, 0.08429405838251114, 0.03635181114077568, 0.010536757297813892, 0.012294312007725239, 0.006147156003862619, 0.9712506532669067, 0.006147156003862619, 0.25830894708633423, 0.10609117150306702, 0.08072154223918915, 0.2225608229637146, 0.04958609119057655, 0.28310197591781616, 0.03668251633644104, 0.04625186696648598, 0.019138703122735023, 0.8771905899047852, 0.009569351561367512, 0.009569351561367512, 0.008568989112973213, 0.951157808303833, 0.008568989112973213, 0.03427595645189285, 0.896716296672821, 0.017150310799479485, 0.017150310799479485, 0.019600355997681618, 0.0049000889994204044, 0.0441008023917675, 0.9973205924034119, 0.2688036859035492, 0.32395780086517334, 0.05341239646077156, 0.2693842649459839, 0.041801005601882935, 0.04296214506030083, 0.06145298853516579, 0.14748717844486237, 0.7841401696205139, 0.0049162390641868114, 0.6530261635780334, 0.2058720588684082, 0.037056971341371536, 0.03376301750540733, 0.06011464074254036, 0.009881858713924885, 0.010835705325007439, 0.019170863553881645, 0.7534982562065125, 0.0025005473289638758, 0.08418509364128113, 0.1291949450969696, 0.9866783618927002, 0.11260727047920227, 0.2677818834781647, 0.015396703965961933, 0.21676144003868103, 0.3450673222541809, 0.04226546362042427, 0.009091845713555813, 0.003409442026168108, 0.41026952862739563, 0.0011364807141944766, 0.5136892795562744, 0.06364291906356812, 0.00522191496565938, 0.9921638369560242, 0.09580603241920471, 0.044621989130973816, 0.5590872764587402, 0.26904433965682983, 0.013124113902449608, 0.01837375946342945, 0.01013309508562088, 0.13173022866249084, 0.02026619017124176, 0.8410468697547913, 0.9994163513183594, 0.9984294772148132, 1.0034366846084595, 0.9986305832862854, 0.01531429123133421, 0.10720003396272659, 0.022971436381340027, 0.007657145615667105, 0.8576002717018127, 0.3065434396266937, 0.38534414768218994, 0.04416303709149361, 0.05801810696721077, 0.12469563633203506, 0.08139853924512863, 0.052528560161590576, 0.10505712032318115, 0.008754760026931763, 0.8360795378684998, 0.09191181510686874, 0.03419974446296692, 0.15176136791706085, 0.5856706500053406, 0.030993519350886345, 0.10580545663833618, 0.004539421293884516, 0.04539421200752258, 0.9441996216773987, 0.009078842587769032, 0.19881610572338104, 0.007646773476153612, 0.734090268611908, 0.03058709390461445, 0.026763707399368286, 0.021637840196490288, 0.012021022848784924, 0.0024042045697569847, 0.9015766978263855, 0.040871478617191315, 0.019233636558055878, 0.003364922245964408, 0.050473835319280624, 0.9455431699752808, 0.003364922245964408, 0.25392642617225647, 0.140138640999794, 0.035932984203100204, 0.07126708328723907, 0.2389543503522873, 0.26051414012908936, 0.3762786090373993, 0.07569240778684616, 0.284938246011734, 0.09825456887483597, 0.11135517805814743, 0.053858060389757156, 0.9405488967895508, 0.02858397550880909, 0.027222834527492523, 0.002722283359616995, 0.08871319890022278, 0.04879225790500641, 0.011089149862527847, 0.022178299725055695, 0.06653489917516708, 0.760715663433075, 0.06443669646978378, 0.014870007522404194, 0.9219405055046082, 0.6365110278129578, 0.06132309511303902, 0.11412908881902695, 0.04144987091422081, 0.006245870608836412, 0.14024819433689117, 1.0067192316055298, 0.18860585987567902, 0.13000985980033875, 0.11108823865652084, 0.2972525954246521, 0.15442486107349396, 0.11841273307800293, 0.3989304304122925, 0.17722518742084503, 0.06046506389975548, 0.05977006256580353, 0.2613202631473541, 0.04170004278421402, 1.0004736185073853, 0.9039062261581421, 0.09684709459543228, 0.6020234227180481, 0.15186525881290436, 0.0617559514939785, 0.07884564995765686, 0.041947439312934875, 0.06408636271953583, 0.7247708439826965, 0.11843322962522507, 0.024293994531035423, 0.039477743208408356, 0.04352674260735512, 0.04858798906207085, 0.0047641959972679615, 0.0047641959972679615, 0.9861885905265808, 0.0047641959972679615, 0.1087445616722107, 0.20211109519004822, 0.08750824630260468, 0.07579166442155838, 0.3837181627750397, 0.14242973923683167, 0.7427541613578796, 0.08798100054264069, 0.013263466767966747, 0.09593907743692398, 0.015916161239147186, 0.044211555272340775, 0.03677843511104584, 0.11033529788255692, 0.004243665374815464, 0.24330347776412964, 0.02404743805527687, 0.581382155418396, 0.04399014636874199, 0.03590000420808792, 0.20023101568222046, 0.012135213240981102, 0.04500141367316246, 0.6628860235214233, 0.18319188058376312, 0.39518752694129944, 0.06029586121439934, 0.18549618124961853, 0.14593902230262756, 0.029571855440735817, 0.9856137633323669, 0.005632078740745783, 0.9968888759613037, 0.6268850564956665, 0.05039447173476219, 0.10521922260522842, 0.0038764977362006903, 0.11574114859104156, 0.09857379645109177, 0.8699182271957397, 0.08664523810148239, 0.034658096730709076, 0.0008664524066261947, 0.007798071950674057, 0.021303266286849976, 0.9799502491950989, 0.007900272496044636, 0.8769302368164062, 0.039501361548900604, 0.07110245525836945, 0.1468798667192459, 0.08167953789234161, 0.3869030773639679, 0.198466956615448, 0.1368490606546402, 0.049437616020441055, 0.9601288437843323, 0.010036190040409565, 0.02007238008081913, 0.003345396602526307, 0.006690793205052614, 0.9842020869255066, 0.016969000920653343, 0.9987586140632629, 0.2729966342449188, 0.5673725008964539, 0.0065782321617007256, 0.023023812100291252, 0.034535717219114304, 0.09538436681032181, 0.0016082330839708447, 0.0016082330839708447, 0.03698936104774475, 0.00804116576910019, 0.0032164661679416895, 0.950465738773346, 0.9694132208824158, 0.02203211933374405, 0.9938621520996094, 0.08386589586734772, 0.14296603202819824, 0.5414697527885437, 0.09343449026346207, 0.04221437871456146, 0.09681163728237152, 0.9890803694725037, 0.009990710765123367, 1.000031590461731, 0.959963858127594, 0.03169691935181618, 0.00905626267194748, 0.887074887752533, 0.10086274147033691, 0.002586224116384983, 0.005172448232769966, 0.002586224116384983, 0.4549127519130707, 0.16409172117710114, 0.08835708349943161, 0.15803295373916626, 0.08684238791465759, 0.04796527326107025, 0.09478907287120819, 0.051148559898138046, 0.3219074308872223, 0.21679480373859406, 0.1830085963010788, 0.1323293000459671, 0.012521359138190746, 0.19477669894695282, 0.0027825243305414915, 0.738760232925415, 0.023651456460356712, 0.027825243771076202, 0.9971734881401062, 0.4773516356945038, 0.061014868319034576, 0.3208664357662201, 0.018663372844457626, 0.043787140399217606, 0.07752477377653122, 0.9992002248764038, 0.1148722916841507, 0.10984138399362564, 0.07881748676300049, 0.5123136639595032, 0.035216324031353, 0.14841164648532867, 0.045490752905607224, 0.10666797310113907, 0.00784323364496231, 0.8047157526016235, 0.0015686466358602047, 0.032941579818725586, 0.04752874746918678, 0.02851724810898304, 0.8745289444923401, 0.02218008227646351, 0.025348665192723274, 0.050619129091501236, 0.3001500368118286, 0.10409003496170044, 0.3564727306365967, 0.13403373956680298, 0.054896801710128784, 0.06220465525984764, 0.2130509465932846, 0.17054443061351776, 0.3260560631752014, 0.18557721376419067, 0.04198814183473587, 0.04215730354189873, 0.04959682747721672, 0.028518177568912506, 0.2938612103462219, 0.47984930872917175, 0.1041533425450325, 0.24129034578800201, 0.11574587225914001, 0.11390864104032516, 0.3019191324710846, 0.10043556988239288, 0.125544473528862, 0.9957669377326965, 0.009052426554262638, 0.06514298915863037, 0.87943035364151, 0.04885724186897278, 0.7168480157852173, 0.020980918779969215, 0.05012108013033867, 0.011656065471470356, 0.12238869071006775, 0.07693003118038177, 0.06375443935394287, 0.9350651502609253, 0.01241831760853529, 0.031610261648893356, 0.05814030393958092, 0.8732334971427917, 0.01919194497168064, 0.005080220755189657, 0.026838218793272972, 0.04025733098387718, 0.9326281547546387, 0.24894365668296814, 0.06812655180692673, 0.26994508504867554, 0.12805743515491486, 0.1552056074142456, 0.1301063597202301, 0.23113401234149933, 0.05007903277873993, 0.037972014397382736, 0.29332005977630615, 0.009355423972010612, 0.378619521856308, 0.050633013248443604, 0.8426780104637146, 0.0036166436038911343, 0.0036166436038911343, 0.0036166436038911343, 0.09584105759859085, 0.01621619239449501, 0.9340527057647705, 0.04864858090877533, 0.4683113098144531, 0.2078399956226349, 0.033834416419267654, 0.13802294433116913, 0.10257735848426819, 0.04887193813920021, 1.0007716417312622, 1.0009244680404663, 1.0020116567611694, 0.0416044183075428, 0.027736278250813484, 0.9152972102165222, 0.013868139125406742, 0.24781262874603271, 0.14117442071437836, 0.09088482707738876, 0.052107300609350204, 0.3580861985683441, 0.10966768860816956, 0.02919776551425457, 0.04055245220661163, 0.006488392129540443, 0.8564677834510803, 0.04379664734005928, 0.022709373384714127, 0.9147607684135437, 0.08176632225513458], \"Term\": [\"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"accuracy\", \"activation_function\", \"activation_function\", \"actively\", \"actively\", \"actor_critic\", \"admm\", \"adoption\", \"adoption\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversarial\", \"adversary\", \"adversary\", \"adversary\", \"adversary\", \"age\", \"age\", \"age\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"agent\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alignment\", \"alternating_minimization\", \"alternating_minimization\", \"analytics\", \"analytics\", \"approximate_bayesian\", \"approximate_bayesian\", \"approximate_bayesian\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"approximation\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"architecture\", \"arm\", \"arm\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"artificial_intelligence\", \"attack\", \"attack\", \"attack\", \"attacker\", \"attacker\", \"attention_mechanism\", \"autoencoder\", \"autoencoder\", \"autoencoder\", \"autoencoder\", \"autoencoders\", \"autoencoders\", \"autoencoders\", \"bandit\", \"bandit\", \"bandit\", \"bandit_problem\", \"bandwidth\", \"bandwidth\", \"bandwidth\", \"bandwidth\", \"batch_size\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"bayesian\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"behavior\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"bias\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"bound\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"brain\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cancer\", \"cascade\", \"case\", \"case\", \"case\", \"case\", \"case\", \"case\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal\", \"causal_inference\", \"causal_inference\", \"causal_inference\", \"causal_inference\", \"causal_inference\", \"causal_inference\", \"character\", \"character\", \"cheap\", \"cheap\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier\", \"classifier_trained\", \"classifier_trained\", \"classifier_trained\", \"classifier_trained\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"clinical\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"cluster\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"clustering\", \"community_structure\", \"community_structure\", \"community_structure\", \"community_structure\", \"company\", \"company\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"compound\", \"compound\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"compression\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"computation\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"condition\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"confidence\", \"content\", \"content\", \"content\", \"content\", \"content\", \"content\", \"contextual\", \"contextual\", \"contextual\", \"contextual_bandit\", \"contextual_bandit\", \"contextual_bandit\", \"control\", \"control\", \"control\", \"control\", \"control\", \"control\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence\", \"convergence_guarantee\", \"convergence_guarantee\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convergence_rate\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convex\", \"convolution\", \"convolution\", \"convolution\", \"convolution\", \"convolutional\", \"convolutional\", \"convolutional\", \"convolutional_neural\", \"convolutional_neural\", \"convolutional_neural\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"cost\", \"customer\", \"customer\", \"customer\", \"d\", \"d\", \"d\", \"d\", \"d\", \"d\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"datasets\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"decision\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_learning\", \"deep_neural\", \"deep_neural\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"demonstrate\", \"deployment\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent\", \"descent_sgd\", \"descent_sgd\", \"descent_sgd\", \"design\", \"design\", \"design\", \"design\", \"design\", \"design\", \"detector\", \"detector\", \"detector\", \"diagnostic\", \"diagnostic\", \"diagnostic\", \"differential_privacy\", \"differential_privacy\", \"differentially_private\", \"diffusion\", \"diffusion\", \"diffusion\", \"diffusion\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"disease\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance\", \"distance_metric\", \"distance_metric\", \"distance_metric\", \"document\", \"document\", \"document\", \"document\", \"document\", \"document\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain\", \"domain_specific\", \"driving\", \"driving\", \"driving\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"dynamic\", \"eeg\", \"eeg\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"effective\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"efficient\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"embeddings\", \"embeddings\", \"embeddings\", \"enabled\", \"enabled\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"ensemble\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"environment\", \"error\", \"error\", \"error\", \"error\", \"error\", \"error\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimate\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimation\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"estimator\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"existing\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"experiment\", \"feedforward_neural\", \"field\", \"field\", \"field\", \"field\", \"field\", \"field\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecast\", \"forecasting\", \"forecasting\", \"forecasting\", \"forecasting\", \"forecasting\", \"forecasting\", \"format\", \"format\", \"format\", \"fully_connected\", \"fully_connected\", \"fully_connected\", \"game\", \"game\", \"game\", \"game\", \"game\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gaussian_process\", \"gene\", \"gene\", \"gene\", \"gene\", \"gene\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generative_model\", \"generator\", \"generator\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"gibbs_sampler\", \"given\", \"given\", \"given\", \"given\", \"given\", \"given\", \"global_convergence\", \"global_convergence\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"goal\", \"gps\", \"gps\", \"gps\", \"gps\", \"gps\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"gradient_descent\", \"group\", \"group\", \"group\", \"group\", \"group\", \"group\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"guarantee\", \"hamiltonian_monte\", \"hamiltonian_monte\", \"hamiltonian_monte\", \"hardware\", \"hardware\", \"hidden_variable\", \"hidden_variable\", \"hidden_variable\", \"hidden_variable\", \"human\", \"human\", \"human\", \"human\", \"human\", \"human\", \"hyper_parameter\", \"hyper_parameter\", \"hyper_parameter\", \"hyper_parameter\", \"hyper_parameter\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hyperparameters\", \"hypothesis_test\", \"ica\", \"ica\", \"image\", \"image\", \"image\", \"image\", \"image\", \"image\", \"imagenet\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"implementation\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"individual\", \"inducing\", \"inducing\", \"industrial\", \"industrial\", \"industrial\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"inference\", \"initialization\", \"initialization\", \"initialization\", \"initialization\", \"initialization\", \"input\", \"input\", \"input\", \"input\", \"input\", \"input\", \"intensity\", \"intensity\", \"intensity\", \"intensity\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interaction\", \"interpreting\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"introduce\", \"item\", \"item\", \"item\", \"item\", \"item\", \"item\", \"iterates\", \"iterates\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration\", \"iteration_complexity\", \"joint_distribution\", \"joint_distribution\", \"joint_distribution\", \"joint_distribution\", \"joint_distribution\", \"joint_distribution\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"kernel\", \"label\", \"label\", \"label\", \"label\", \"label\", \"label\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labeled\", \"labelled\", \"labelled\", \"large\", \"large\", \"large\", \"large\", \"large\", \"large\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent\", \"latent_space\", \"latent_space\", \"latent_space\", \"latent_space\", \"latent_space\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"latent_variable\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"layer\", \"lda\", \"lda\", \"lda\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"learn\", \"level\", \"level\", \"level\", \"level\", \"level\", \"level\", \"library\", \"library\", \"library\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear\", \"linear_convergence\", \"linear_convergence\", \"linear_discriminant\", \"linear_discriminant\", \"link_prediction\", \"link_prediction\", \"link_prediction\", \"link_prediction\", \"link_prediction\", \"local_minimum\", \"local_minimum\", \"local_minimum\", \"local_minimum\", \"logic\", \"logic\", \"logic\", \"logic\", \"long_short\", \"long_short\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"loss\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"low_rank\", \"machine_translation\", \"malicious\", \"malicious\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"manifold\", \"map\", \"map\", \"map\", \"map\", \"map\", \"map\", \"markov\", \"markov\", \"markov\", \"markov\", \"markov\", \"markov\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"markov_chain\", \"material\", \"material\", \"material\", \"material\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"matrix_completion\", \"maximum_entropy\", \"maximum_entropy\", \"maximum_entropy\", \"maximum_entropy\", \"maximum_entropy\", \"maximum_mean\", \"maximum_mean\", \"maximum_mean\", \"maximum_mean\", \"mcmc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mcmc\", \"mdp\", \"mdp\", \"mdp\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"mechanism\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"memory\", \"meta_learning\", \"meta_learning\", \"meta_learning\", \"meta_learning\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metric\", \"metropolis_hastings\", \"metropolis_hastings\", \"metropolis_hastings\", \"mnist\", \"mnist\", \"mnist\", \"mnist\", \"mnist\", \"mobile\", \"mobile\", \"modality\", \"modality\", \"modality\", \"modality\", \"modality\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modeling\", \"modelled\", \"modelled\", \"modelled\", \"modelled\", \"molecular\", \"molecular\", \"molecular\", \"momentum\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"monte_carlo\", \"motion\", \"motion\", \"motion\", \"motion\", \"motion\", \"movie\", \"movie\", \"movie\", \"movie\", \"multi_agent\", \"multi_agent\", \"multi_agent\", \"multi_armed\", \"multi_armed\", \"multi_layer\", \"multilayer\", \"multilayer\", \"multilayer\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"multiple\", \"music\", \"music\", \"music\", \"music\", \"naive_bayes\", \"naive_bayes\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neural_network\", \"neuroimaging\", \"neuroimaging\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"neuron\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"node\", \"node\", \"node\", \"node\", \"node\", \"node\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"noise\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"non_convex\", \"nonconvex\", \"nonconvex\", \"nonconvex\", \"nonconvex_optimization\", \"nonsmooth\", \"nonsmooth\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"norm\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"novel\", \"object\", \"object\", \"object\", \"object\", \"object\", \"object\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observation\", \"observational_data\", \"observational_data\", \"observational_data\", \"observational_data\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed\", \"observed_variable\", \"observed_variable\", \"observed_variable\", \"observed_variable\", \"open_source\", \"open_source\", \"open_source\", \"open_source\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal\", \"optimal_policy\", \"optimal_policy\", \"optimal_transport\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"optimization_problem\", \"order\", \"order\", \"order\", \"order\", \"order\", \"order\", \"pac\", \"pac\", \"parametrized\", \"parametrized\", \"parametrized\", \"parametrized\", \"particle\", \"particle\", \"particle\", \"particle\", \"passive\", \"passive\", \"passive\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"patient\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"pattern\", \"perception\", \"perception\", \"perception\", \"person\", \"person\", \"person\", \"person\", \"personal\", \"personal\", \"personal\", \"phenotype\", \"phenotype\", \"physic\", \"physic\", \"physic\", \"physic\", \"plan\", \"plan\", \"plan\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"planning\", \"platform\", \"platform\", \"platform\", \"platform\", \"platform\", \"platform\", \"player\", \"player\", \"player\", \"point\", \"point\", \"point\", \"point\", \"point\", \"point\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy\", \"policy_gradient\", \"policy_gradient\", \"policy_gradient\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_distribution\", \"posterior_probability\", \"posterior_probability\", \"posterior_probability\", \"posterior_probability\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predict\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"predictor\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"primal_dual\", \"primal_dual\", \"primitive\", \"primitive\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"prior\", \"privacy\", \"privacy\", \"privacy\", \"privacy\", \"private\", \"private\", \"private\", \"private\", \"process\", \"process\", \"process\", \"process\", \"process\", \"process\", \"processor\", \"processor\", \"processor\", \"production\", \"production\", \"protein\", \"protein\", \"protein\", \"protein\", \"protein\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"prove\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"provide\", \"proximal\", \"proximal\", \"proximal\", \"proximal\", \"quantum\", \"quantum\", \"quantum\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank\", \"rank_matrix\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rate\", \"rating\", \"rating\", \"rating\", \"rating\", \"real\", \"real\", \"real\", \"real\", \"real\", \"real\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommendation\", \"recommender\", \"recommender\", \"recommender\", \"recommender\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"recovery\", \"refine\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regression\", \"regret\", \"regret\", \"regret\", \"regret\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"regularization\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"reinforcement_learning\", \"release\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"representation\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward\", \"reward_function\", \"reward_function\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"risk\", \"rkhs\", \"rkhs\", \"rkhs\", \"rkhs\", \"robot\", \"rotation\", \"saddle_point\", \"safety\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"sampler\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scale\", \"scene\", \"scene\", \"scene\", \"scene\", \"score\", \"score\", \"score\", \"score\", \"score\", \"score\", \"security\", \"security\", \"security\", \"security\", \"self\", \"self\", \"self\", \"self\", \"self\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"semantic\", \"sentence\", \"sentence\", \"sentence\", \"sentence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"sequence\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"setting\", \"sgd\", \"sgd\", \"sgd\", \"sgd\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shape\", \"shot\", \"shot\", \"shot\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"signal\", \"significantly_reduce\", \"single\", \"single\", \"single\", \"single\", \"single\", \"single\", \"size\", \"size\", \"size\", \"size\", \"size\", \"size\", \"skill\", \"softmax\", \"softmax\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solution\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"solving\", \"source_domain\", \"source_domain\", \"source_domain\", \"source_domain\", \"space\", \"space\", \"space\", \"space\", \"space\", \"space\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"sparse\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"spatial\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"state_art\", \"stationary_point\", \"stationary_point\", \"step_size\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_gradient\", \"stochastic_variational\", \"stochastic_variational\", \"stock\", \"stock\", \"stock\", \"stock\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"strategy\", \"streaming\", \"streaming\", \"streaming\", \"streaming\", \"streaming\", \"strong_convexity\", \"strong_convexity\", \"strongly_convex\", \"structured\", \"structured\", \"structured\", \"structured\", \"structured\", \"structured\", \"student\", \"student\", \"student\", \"student\", \"student\", \"student\", \"survival\", \"survival\", \"svd\", \"system\", \"system\", \"system\", \"system\", \"system\", \"system\", \"target_domain\", \"target_domain\", \"targeted\", \"teacher\", \"teacher\", \"teacher\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"tensor\", \"term\", \"term\", \"term\", \"term\", \"term\", \"term\", \"test\", \"test\", \"test\", \"test\", \"test\", \"test\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text\", \"text_classification\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"theory\", \"thompson_sampling\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"time_series\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"topic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"traffic\", \"train\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"trained\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"tree\", \"type\", \"type\", \"type\", \"type\", \"type\", \"type\", \"ucb\", \"ucb\", \"ultimately\", \"ultimately\", \"ultimately\", \"update\", \"update\", \"update\", \"update\", \"update\", \"update\", \"upper_confidence\", \"upper_confidence\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user\", \"user_item\", \"user_item\", \"user_item\", \"value\", \"value\", \"value\", \"value\", \"value\", \"value\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variable\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational\", \"variational_inference\", \"variational_inference\", \"variational_inference\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"vector\", \"verification\", \"vulnerable\", \"wasserstein_distance\", \"website\", \"website\", \"website\", \"website\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"x\", \"x\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el55821401227026937447976865444\", ldavis_el55821401227026937447976865444_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el55821401227026937447976865444\", ldavis_el55821401227026937447976865444_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el55821401227026937447976865444\", ldavis_el55821401227026937447976865444_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "0     -0.090056  0.026201       1        1  27.727028\n",
       "1      0.010475 -0.041889       2        1  17.877914\n",
       "2     -0.051804  0.043627       3        1  13.997724\n",
       "3      0.090635 -0.078860       4        1  16.982306\n",
       "4     -0.045493 -0.059318       5        1  11.688646\n",
       "5      0.086244  0.110238       6        1  11.726382, topic_info=     Category         Freq                    Term        Total  loglift  \\\n",
       "term                                                                       \n",
       "1256  Default  1787.000000                  policy  1787.000000  30.0000   \n",
       "919   Default  1871.000000                   agent  1871.000000  29.0000   \n",
       "2777  Default  1899.000000             deep_neural  1899.000000  28.0000   \n",
       "1259  Default  1977.000000                   state  1977.000000  27.0000   \n",
       "807   Default  1560.000000                    node  1560.000000  26.0000   \n",
       "1838  Default  1771.000000                    user  1771.000000  25.0000   \n",
       "125   Default  1841.000000                  kernel  1841.000000  24.0000   \n",
       "1349  Default  3650.000000          neural_network  3650.000000  23.0000   \n",
       "2392  Default  1040.000000                  attack  1040.000000  22.0000   \n",
       "1942  Default  1954.000000            architecture  1954.000000  21.0000   \n",
       "90    Default  2226.000000              classifier  2226.000000  20.0000   \n",
       "2422  Default  1728.000000                   layer  1728.000000  19.0000   \n",
       "55    Default  1235.000000                  object  1235.000000  18.0000   \n",
       "525   Default  1571.000000                bayesian  1571.000000  17.0000   \n",
       "317   Default  3312.000000          representation  3312.000000  16.0000   \n",
       "460   Default  3046.000000                 process  3046.000000  15.0000   \n",
       "791   Default  3388.000000                   image  3388.000000  14.0000   \n",
       "308   Default  2402.000000               inference  2402.000000  13.0000   \n",
       "1832  Default  1199.000000  reinforcement_learning  1199.000000  12.0000   \n",
       "25    Default  2731.000000                   space  2731.000000  11.0000   \n",
       "540   Default  1692.000000                   prior  1692.000000  10.0000   \n",
       "1605  Default   621.000000                 student   621.000000   9.0000   \n",
       "385   Default  1956.000000                   error  1956.000000   8.0000   \n",
       "531   Default  1384.000000             environment  1384.000000   7.0000   \n",
       "684   Default  2102.000000                    deep  2102.000000   6.0000   \n",
       "2025  Default  2685.000000                gradient  2685.000000   5.0000   \n",
       "1900  Default   904.000000             adversarial   904.000000   4.0000   \n",
       "1193  Default   734.000000                 patient   734.000000   3.0000   \n",
       "196   Default  1817.000000                variable  1817.000000   2.0000   \n",
       "75    Default  1776.000000                  system  1776.000000   1.0000   \n",
       "...       ...          ...                     ...          ...      ...   \n",
       "1259   Topic6  1310.750732                   state  1977.715576   1.7320   \n",
       "525    Topic6  1036.711792                bayesian  1571.528076   1.7273   \n",
       "919    Topic6  1191.312866                   agent  1871.098633   1.6919   \n",
       "2527   Topic6   340.313141            latent_space   498.164185   1.7623   \n",
       "309    Topic6   568.265137                  latent   953.002136   1.6263   \n",
       "601    Topic6   310.959778             monte_carlo   483.186157   1.7026   \n",
       "2018   Topic6   295.904449                       d   456.838989   1.7090   \n",
       "1741   Topic6   463.816467        generative_model   803.031738   1.5944   \n",
       "540    Topic6   847.566467                   prior  1692.059326   1.4520   \n",
       "2271   Topic6   410.500031                 spatial   706.936035   1.5998   \n",
       "460    Topic6  1264.366089                 process  3046.409668   1.2639   \n",
       "853    Topic6   321.296417               posterior   557.787170   1.5917   \n",
       "196    Topic6   687.860657                variable  1817.127686   1.1719   \n",
       "1149   Topic6   552.102295                modeling  1337.931152   1.2582   \n",
       "267    Topic6   603.385315             observation  1549.222656   1.2004   \n",
       "686    Topic6   720.756104                 dynamic  2167.928223   1.0421   \n",
       "531    Topic6   537.242188             environment  1384.649902   1.1966   \n",
       "791    Topic6   885.867065                   image  3388.405762   0.8018   \n",
       "477    Topic6   467.346497                 cluster  1120.302612   1.2690   \n",
       "795    Topic6   424.988953                observed  1088.093506   1.2032   \n",
       "201    Topic6   554.825684              clustering  1953.296387   0.8847   \n",
       "308    Topic6   589.123230               inference  2402.749268   0.7376   \n",
       "584    Topic6   490.792328                    real  1734.357300   0.8810   \n",
       "1402   Topic6   543.882202                   learn  2408.945068   0.6551   \n",
       "12     Topic6   527.690552                estimate  2250.220215   0.6931   \n",
       "183    Topic6   550.255066                   given  2548.424072   0.6105   \n",
       "444    Topic6   489.640564                   point  2448.761719   0.5337   \n",
       "24     Topic6   435.134644                sequence  1669.775024   0.7985   \n",
       "252    Topic6   442.083221                 present  2786.572998   0.3023   \n",
       "25     Topic6   389.213623                   space  2731.171143   0.1950   \n",
       "\n",
       "      logprob  \n",
       "term           \n",
       "1256  30.0000  \n",
       "919   29.0000  \n",
       "2777  28.0000  \n",
       "1259  27.0000  \n",
       "807   26.0000  \n",
       "1838  25.0000  \n",
       "125   24.0000  \n",
       "1349  23.0000  \n",
       "2392  22.0000  \n",
       "1942  21.0000  \n",
       "90    20.0000  \n",
       "2422  19.0000  \n",
       "55    18.0000  \n",
       "525   17.0000  \n",
       "317   16.0000  \n",
       "460   15.0000  \n",
       "791   14.0000  \n",
       "308   13.0000  \n",
       "1832  12.0000  \n",
       "25    11.0000  \n",
       "540   10.0000  \n",
       "1605   9.0000  \n",
       "385    8.0000  \n",
       "531    7.0000  \n",
       "684    6.0000  \n",
       "2025   5.0000  \n",
       "1900   4.0000  \n",
       "1193   3.0000  \n",
       "196    2.0000  \n",
       "75     1.0000  \n",
       "...       ...  \n",
       "1259  -4.4584  \n",
       "525   -4.6929  \n",
       "919   -4.5539  \n",
       "2527  -5.8069  \n",
       "309   -5.2942  \n",
       "601   -5.8971  \n",
       "2018  -5.9467  \n",
       "1741  -5.4973  \n",
       "540   -4.8944  \n",
       "2271  -5.6194  \n",
       "460   -4.4944  \n",
       "853   -5.8644  \n",
       "196   -5.1032  \n",
       "1149  -5.3230  \n",
       "267   -5.2342  \n",
       "686   -5.0564  \n",
       "531   -5.3503  \n",
       "791   -4.8502  \n",
       "477   -5.4897  \n",
       "795   -5.5847  \n",
       "201   -5.3181  \n",
       "308   -5.2581  \n",
       "584   -5.4407  \n",
       "1402  -5.3380  \n",
       "12    -5.3682  \n",
       "183   -5.3264  \n",
       "444   -5.4431  \n",
       "24    -5.5611  \n",
       "252   -5.5452  \n",
       "25    -5.6726  \n",
       "\n",
       "[448 rows x 6 columns], token_table=      Topic      Freq                   Term\n",
       "term                                        \n",
       "404       1  0.146593               accuracy\n",
       "404       2  0.310871               accuracy\n",
       "404       3  0.127546               accuracy\n",
       "404       4  0.262574               accuracy\n",
       "404       5  0.130607               accuracy\n",
       "404       6  0.022108               accuracy\n",
       "2959      4  0.003737    activation_function\n",
       "2959      5  0.994056    activation_function\n",
       "1938      3  0.835314               actively\n",
       "1938      5  0.156621               actively\n",
       "3013      6  1.001724           actor_critic\n",
       "2985      1  0.998043                   admm\n",
       "2716      1  0.019370               adoption\n",
       "2716      2  0.968500               adoption\n",
       "1900      1  0.050883            adversarial\n",
       "1900      2  0.001106            adversarial\n",
       "1900      3  0.420341            adversarial\n",
       "1900      5  0.525426            adversarial\n",
       "1900      6  0.002212            adversarial\n",
       "1901      1  0.005402              adversary\n",
       "1901      3  0.966928              adversary\n",
       "1901      5  0.024308              adversary\n",
       "1901      6  0.002701              adversary\n",
       "1524      1  0.015265                    age\n",
       "1524      4  0.982018                    age\n",
       "1524      6  0.005088                    age\n",
       "919       1  0.003207                  agent\n",
       "919       2  0.000534                  agent\n",
       "919       3  0.329753                  agent\n",
       "919       4  0.000534                  agent\n",
       "...     ...       ...                    ...\n",
       "321       1  0.016216  variational_inference\n",
       "321       2  0.934053  variational_inference\n",
       "321       6  0.048649  variational_inference\n",
       "231       1  0.468311                 vector\n",
       "231       2  0.207840                 vector\n",
       "231       3  0.033834                 vector\n",
       "231       4  0.138023                 vector\n",
       "231       5  0.102577                 vector\n",
       "231       6  0.048872                 vector\n",
       "2972      3  1.000772           verification\n",
       "3015      2  1.000924             vulnerable\n",
       "2851      1  1.002012   wasserstein_distance\n",
       "1933      1  0.041604                website\n",
       "1933      3  0.027736                website\n",
       "1933      4  0.915297                website\n",
       "1933      5  0.013868                website\n",
       "1047      1  0.247813                 weight\n",
       "1047      2  0.141174                 weight\n",
       "1047      3  0.090885                 weight\n",
       "1047      4  0.052107                 weight\n",
       "1047      5  0.358086                 weight\n",
       "1047      6  0.109668                 weight\n",
       "761       1  0.029198                   word\n",
       "761       2  0.040552                   word\n",
       "761       3  0.006488                   word\n",
       "761       4  0.856468                   word\n",
       "761       5  0.043797                   word\n",
       "761       6  0.022709                   word\n",
       "2729      2  0.914761                      x\n",
       "2729      4  0.081766                      x\n",
       "\n",
       "[1606 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you set the lambda value to be lower, you can see more exclusive terms\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(filtered_lda, filtered_bow, filtered_dictionary, sort_topics = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, the bulk of the topic modeling is completed. I have chosen my number of topics, and have gotten a general human-readable tag for each topic. What's next? Well, there are several options:**\n",
    "\n",
    "1. Set up a small program that allows one to compare a new document against the existing models and corpus. \n",
    "2. Set up Dynamic Topic Modeling to track the Evolution of a Topic over time\n",
    "3. Set up a Grid-Search process for fine-tuning the number of topics and other parameters\n",
    "4. Include more abstracts, this time from different fields than ML, to see how well the process generalizes while mainting intra-topic distinctions\n",
    "\n",
    "**Let's begin with a technique to help us choose the best number of topics:\n",
    "\n",
    "### Coherence analys: ###\n",
    "\n",
    "*note:* I this is heavily \"inspired\" by [this notebook](https://nbviewer.jupyter.org/github/dsquareindia/gensim/blob/280375fe14adea67ce6384ba7eabf362b05e6029/docs/notebooks/topic_coherence_tutorial.ipynb#topic=2&lambda=1&term=)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_vals = compute_coherence_values(dictionary=dictionary,\n",
    "                                                      corpus = filterd_bow ,\n",
    "                                                      texts=corpus, limit = 21, \n",
    "                                                      start=5, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "limit=21; start=5; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_vals)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "#In this case itseems to suggest 11 topics...hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that this function is not run with the exact correct parameters... (still need to update that)**\n",
    "\n",
    "But if run correctly, it can be used to help us inform our choice of $t$ topics. \n",
    "\n",
    "Ok, so out of our list: \n",
    "\n",
    "1. Set up a small program that allows one to compare a new document against the existing models and corpus. \n",
    "2. Set up Dynamic Topic Modeling to track the Evolution of a Topic over time\n",
    "3. Set up a Grid-Search process for fine-tuning the number of topics and other parameters\n",
    "4. Include more abstracts, this time from different fields than ML, to see how well the process generalizes while mainting intra-topic distinctions\n",
    "\n",
    "I just showed (part of) #3. Now lets look at #1: \n",
    "\n",
    "## Querying a new document against the existing model and corpus ##\n",
    "\n",
    "Ideally, I will integrate this feature with a user interface, such that the user will be able to see how the words are distributed among topics, and also get a list of the most closely related documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_lda.get_document_topics(dictionary.doc2bow(['general', 'topic', 'network']), per_word_topics = True)\n",
    "proc_ab = prep_text(myab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_bow = dictionary.doc2bow(bigrams[proc_ab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gradient'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[3706]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"This\", -1], [\"paper\", -1], [\"presents\", 0], [\"a\", -1], [\"novel\", 0], [\"optimization\", 0], [\"method\", -1], [\"for\", -1], [\"maximizing\", 0], [\"generalization\", 0], [\"over\", -1], [\"tasks\", -1], [\"in\", -1], [\"meta-learning.\\\\\", -1], [\"The\", -1], [\"goal\", 0], [\"of\", -1], [\"meta-learning\", -1], [\"is\", -1], [\"to\", -1], [\"learn\", 0], [\"a\", -1], [\"model\", -1], [\"for\", -1], [\"an\", -1], [\"agent\", 5], [\"adapting\", 0], [\"rapidly\", 0], [\"when\", -1], [\"presented\", 0], [\"with\", -1], [\"previously\", 0], [\"unseen\", 0], [\"tasks.\\\\\", -1], [\"Tasks\", -1], [\"are\", -1], [\"sampled\", 1], [\"from\", -1], [\"a\", -1], [\"specific\", 0], [\"distribution\", -1], [\"which\", -1], [\"is\", -1], [\"assumed\", 0], [\"to\", -1], [\"be\", -1], [\"similar\", 0], [\"for\", -1], [\"both\", -1], [\"seen\", 0], [\"and\", -1], [\"unseen\", 0], [\"tasks.\", -1], [\"We\", -1], [\"focus\", 0], [\"on\", -1], [\"\\\\\", -1], [\"a\", -1], [\"family\", 0], [\"of\", -1], [\"meta-learning\", -1], [\"methods\", -1], [\"learning\", -1], [\"initial\", 0], [\"parameters\", -1], [\"of\", -1], [\"a\", -1], [\"base\", 0], [\"model\", -1], [\"which\", -1], [\"can\", -1], [\"be\", -1], [\"fine-tuned\", -1], [\"quickly\", 0], [\"on\", -1], [\"a\", -1], [\"new\", -1], [\"task,\", -1], [\"\\\\\", -1], [\"by\", -1], [\"few\", -1], [\"gradient\", 1], [\"steps\", 0], [\"(MAML).\", -1], [\"Our\", -1], [\"approach\", -1], [\"is\", -1], [\"based\", -1], [\"on\", -1], [\"pushing\", -1], [\"the\", -1], [\"parameters\", -1], [\"of\", -1], [\"the\", -1], [\"model\", -1], [\"to\", -1], [\"a\", -1], [\"direction\", 0], [\"in\", -1], [\"which\", -1], [\"tasks\", -1], [\"have\\\\\", -1], [\"more\", -1], [\"agreement\", 0], [\"upon.\", -1], [\"If\", -1], [\"the\", -1], [\"gradients\", 1], [\"of\", -1], [\"a\", -1], [\"task\", -1], [\"agree\", -1], [\"with\", -1], [\"the\", -1], [\"parameters\", -1], [\"update\", 0], [\"vector,\", 0], [\"then\", -1], [\"their\", -1], [\"inner\", -1], [\"product\", 0], [\"will\", -1], [\"be\", -1], [\"a\", -1], [\"large\\\\\", 0], [\"positive\", 0], [\"value.\", 0], [\"As\", -1], [\"a\", -1], [\"result,\", -1], [\"given\", 0], [\"a\", -1], [\"batch\", 0], [\"of\", -1], [\"tasks\", -1], [\"to\", -1], [\"be\", -1], [\"optimized\", 0], [\"for,\", -1], [\"we\", -1], [\"associate\", -1], [\"a\", -1], [\"positive\", 0], [\"(negative)\", 0], [\"weight\", 0], [\"to\", -1], [\"the\", -1], [\"loss\\\\\", 0], [\"function\", -1], [\"of\", -1], [\"a\", -1], [\"task,\", -1], [\"if\", -1], [\"the\", -1], [\"inner\", -1], [\"product\", 0], [\"between\", -1], [\"its\", -1], [\"gradients\", 1], [\"and\", -1], [\"the\", -1], [\"average\", 0], [\"of\", -1], [\"the\", -1], [\"gradients\", 1], [\"of\", -1], [\"all\", -1], [\"tasks\", -1], [\"in\", -1], [\"the\", -1], [\"batch\", 0], [\"is\", -1], [\"a\\\\\", -1], [\"positive\", 0], [\"(negative)\", 0], [\"value.\", 0], [\"Therefore,\", -1], [\"the\", -1], [\"degree\", 0], [\"of\", -1], [\"the\", -1], [\"contribution\", 0], [\"of\", -1], [\"a\", -1], [\"task\", -1], [\"to\", -1], [\"the\", -1], [\"parameter\", -1], [\"updates\", 0], [\"is\", -1], [\"controlled\", 0], [\"by\", -1], [\"introducing\\\\\", 0], [\"a\", -1], [\"set\", -1], [\"of\", -1], [\"weights\", 0], [\"on\", -1], [\"the\", -1], [\"loss\", 0], [\"function\", -1], [\"of\", -1], [\"the\", -1], [\"tasks.\", -1], [\"Our\", -1], [\"method\", -1], [\"can\", -1], [\"be\", -1], [\"easily\", 0], [\"integrated\", 0], [\"with\", -1], [\"the\", -1], [\"current\", 0], [\"meta-learning\", -1], [\"algorithms\", -1], [\"for\\\\\", -1], [\"neural\", 0], [\"networks.\", -1], [\"Our\", -1], [\"experiments\", 0], [\"demonstrate\", 0], [\"that\", -1], [\"it\", -1], [\"yields\", 0], [\"models\", -1], [\"with\", -1], [\"better\", 0], [\"generalization\", 0], [\"compared\", 0], [\"to\", -1], [\"MAML\", -1], [\"and\", -1], [\"Reptile.\", -1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def colorize(ab, model, dictionary):\n",
    "    \"\"\"\n",
    "    takes an abstract, an LDA model, and dictionary, and returns json with each word mapped to it's topic (or -1 if n/a)\n",
    "    \n",
    "    Still needs refinement to take advantage of bigrams\n",
    "    \"\"\"\n",
    "    proc_ab = prep_text(ab)\n",
    "#     bigram_ab = bigrams[proc_ab]\n",
    "#     ab_bow = dictionary.doc2bow(bigram_ab)\n",
    "    ab_bow = dictionary.doc2bow(proc_ab) # not using bigrams for now b/c it complicates things\n",
    "#     print(len(ab_bow))\n",
    "    doc_topics, word_topics, phi_values = model.get_document_topics(ab_bow, per_word_topics = True)\n",
    "#     print(model.get_document_topics(ab_bow, per_word_topics = True))\n",
    "    topic_colors = {0: 'red', 1: 'yellow', 2:'green', 3:'blue', 4:'aqua', 5: \"magenta\", 6:\"purple\"} # Have to figure out how to not hard-code this for the future\n",
    "    raw_ab = ab.split()\n",
    "    raw_proc = [\"\".join(prep_text(w)) for w in raw_ab] #this is a hack b/c the preprocessing is off\n",
    "#     print(\"bigrams:\", bigram_ab)\n",
    "#     print(\"raw_ab: \", raw_ab)\n",
    "#     print(\"raw_proc: \", raw_proc)\n",
    "#     print(\"word_topics: \", word_topics)\n",
    "    result = [];\n",
    "    for i in range(len(raw_proc)):\n",
    "#         print(raw_proc[i])\n",
    "        if not raw_proc[i] or raw_proc[i] not in dictionary.token2id:\n",
    "#             print(\"this isn't valid\")\n",
    "            result.append((raw_ab[i], -1))\n",
    "        else:\n",
    "#             print(\"searching for \", raw_proc[i])\n",
    "            #word_topics.filter(lambda w: w[0] == dictionary.token2id[raw_proc[i]])[0] \n",
    "            sample = [w for w in word_topics if w[0] == dictionary.token2id[raw_proc[i]]]\n",
    "#             print(sample)\n",
    "            match = sample[0] \n",
    "#             print(match)\n",
    "            result.append((raw_ab[i], match[1][0]))\n",
    "#             word_topics.remove(match)\n",
    "    return json.dumps(result)\n",
    "\n",
    "myab = r\"This paper presents a novel optimization method for maximizing generalization over tasks in meta-learning.\\\n",
    "        The goal of meta-learning is to learn a model for an agent adapting rapidly when presented with previously unseen tasks.\\\n",
    "        Tasks are sampled from a specific distribution which is assumed to be similar for both seen and unseen tasks. We focus on \\\n",
    "        a family of meta-learning methods learning initial parameters of a base model which can be fine-tuned quickly on a new task, \\\n",
    "        by few gradient steps (MAML). Our approach is based on pushing the parameters of the model to a direction in which tasks have\\\n",
    "        more agreement upon. If the gradients of a task agree with the parameters update vector, then their inner product will be a large\\\n",
    "        positive value. As a result, given a batch of tasks to be optimized for, we associate a positive (negative) weight to the loss\\\n",
    "        function of a task, if the inner product between its gradients and the average of the gradients of all tasks in the batch is a\\\n",
    "        positive (negative) value. Therefore, the degree of the contribution of a task to the parameter updates is controlled by introducing\\\n",
    "        a set of weights on the loss function of the tasks. Our method can be easily integrated with the current meta-learning algorithms for\\\n",
    "        neural networks. Our experiments demonstrate that it yields models with better generalization compared to MAML and Reptile.\"\n",
    "\n",
    "print(colorize(myab, filtered_lda, dictionary))\n",
    "\n",
    "# This particular choice of abstract may not have been ideal. but it gets the point across that the topics are sampled from topics 1 and 2, both of\n",
    "# which have relation to the idea of gradient descent and optimization (the topic of this abstract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, colorize, can be called by the front end, and the Flask server can return a json with the entire text nicely labled with each word'd topic distribution. Next up is the document similarity. To do this effectivley, we need to utilize the index matrix provided by Gensim: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#The index of corp_matrix so that query lookup is efficient\n",
    "\n",
    "index = similarities.MatrixSimilarity(corp_matrix)\n",
    "# index.save('./the_data_strikes_back/first_sim.index')\n",
    "# index = similarities.MatrixSimilarity.load('./the_data_strikes_back/first_sim.index')\n",
    "\n",
    "def get_similar_docs(querydoc, index, tokenizer, phraser, dictionary, top_n_docs=10): \n",
    "    \"\"\"\n",
    "    does a (I think?) cosine similarity with all documents in my corpus\n",
    "    \"\"\"\n",
    "    # curr_text is a list of strings (including bigrams)\n",
    "    curr_text = phraser[tokenizer(querydoc)]\n",
    "    curr_bow = dictionary.doc2bow(curr_text)\n",
    "    #fit the model to the bow - convert to lda space\n",
    "    sims = index[curr_bow]\n",
    "    return sorted(enumerate(sims), key= lambda i: -i[1])[:top_n_docs]\n",
    "\n",
    "def display_similars(tup_array, corp_matrix, corpus= abstracts):\n",
    "    result = [{\"topics\":model[corp_matrix[sim[0]]], \"percentage\": float(sim[1]), \"text\":corpus[sim[0]]} for sim in tup_array]\n",
    "#     print(type(result[1][\"topics\"]), type(result[1][\"percentage\"]), type(result[1][\"text\"]))\n",
    "#     sim = tup_array[0]\n",
    "#     print(sim[0], type(sim[0]))\n",
    "#     print(corp_matrix[sim[0]], type(corp_matrix[sim[0]]))\n",
    "#     print(model[corp_matrix[sim[0]]], type(model[corp_matrix[sim[0]]]))\n",
    "#     return json.dumps(result) # for some reason it won't json-serialize at the moment...hmmm\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's select a new abstract at random, and then query it against our word colorizer and our similar documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'topics': [(1, 0.07292722), (4, 0.91831183)],\n",
       "  'percentage': 0.16466586291790009,\n",
       "  'text': 'Support vector machines (SVMs) naturally embody sparseness due to their use of hinge loss functions. However, SVMs can not directly estimate conditional class probabilities. In this paper we propose and study a family of coherence functions, which are convex and differentiable, as surrogates of the hinge function. The coherence function is derived by using the maximum-entropy principle and is characterized by a temperature parameter. It bridges the hinge function and the logit function in logistic regression. The limit of the coherence function at zero temperature corresponds to the hinge function, and the limit of the minimizer of its expected error is the minimizer of the expected error of the hinge loss. We refer to the use of the coherence function in large-margin classification as C-learning, and we present efficient coordinate descent algorithms for the training of regularized ${\\\\cal C}$-learning models.'},\n",
       " {'topics': [(2, 0.2752507), (3, 0.016539626), (4, 0.7039611)],\n",
       "  'percentage': 0.14559419453144073,\n",
       "  'text': \"Stein's method for measuring convergence to a continuous target distribution relies on an operator characterizing the target and Stein factor bounds on the solutions of an associated differential equation. While such operators and bounds are readily available for a diversity of univariate targets, few multivariate targets have been analyzed. We introduce a new class of characterizing operators based on Ito diffusions and develop explicit multivariate Stein factor bounds for any target with a fast-coupling Ito diffusion. As example applications, we develop computable and convergence-determining diffusion Stein discrepancies for log-concave, heavy-tailed, and multimodal targets and use these quality measures to select the hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare random and deterministic quadrature rules, and quantify bias-variance tradeoffs in approximate MCMC. Our results establish a near-linear relationship between diffusion Stein discrepancies and Wasserstein distances, improving upon past work even for strongly log-concave targets. The exposed relationship between Stein factors and Markov process coupling may be of independent interest.\"},\n",
       " {'topics': [(2, 0.30414206), (4, 0.68635696)],\n",
       "  'percentage': 0.14113496243953705,\n",
       "  'text': 'Bayesian Optimisation (BO) is a technique used in optimising a $D$-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on $D$ even though the function depends on all $D$ dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.'},\n",
       " {'topics': [(0, 0.46043965), (3, 0.28854957), (4, 0.24198627)],\n",
       "  'percentage': 0.13725489377975464,\n",
       "  'text': 'We introduce the use of rectified linear units (ReLU) as the classification function in a deep neural network (DNN). Conventionally, ReLU is used as an activation function in DNNs, with Softmax function as their classification function. However, there have been several studies on using a classification function other than Softmax, and this study is an addition to those. We accomplish this by taking the activation of the penultimate layer $h_{n - 1}$ in a neural network, then multiply it by weight parameters $\\\\theta$ to get the raw scores $o_{i}$. Afterwards, we threshold the raw scores $o_{i}$ by $0$, i.e. $f(o) = \\\\max(0, o_{i})$, where $f(o)$ is the ReLU function. We provide class predictions $\\\\hat{y}$ through argmax function, i.e. argmax $f(x)$.'},\n",
       " {'topics': [(2, 0.41510826), (3, 0.07854465), (4, 0.5009047)],\n",
       "  'percentage': 0.13720646500587463,\n",
       "  'text': \"A latent force model is a Gaussian process with a covariance function inspired by a differential operator. Such covariance function is obtained by performing convolution integrals between Green's functions associated to the differential operators, and covariance functions associated to latent functions. In the classical formulation of latent force models, the covariance functions are obtained analytically by solving a double integral, leading to expressions that involve numerical solutions of different types of error functions. In consequence, the covariance matrix calculation is considerably expensive, because it requires the evaluation of one or more of these error functions. In this paper, we use random Fourier features to approximate the solution of these double integrals obtaining simpler analytical expressions for such covariance functions. We show experimental results using ordinary differential operators and provide an extension to build general kernel functions for convolved multiple output Gaussian processes.\"},\n",
       " {'topics': [(1, 0.072280586), (2, 0.07588256), (4, 0.8455522)],\n",
       "  'percentage': 0.12983012199401855,\n",
       "  'text': 'In this paper, we introduce the first principled adaptive-sampling procedure for learning a convex function in the $L_\\\\infty$ norm, a problem that arises often in the behavioral and social sciences. We present a function-specific measure of complexity and use it to prove that, for each convex function $f_{\\\\star}$, our algorithm nearly attains the information-theoretically optimal, function-specific error rate. We also corroborate our theoretical contributions with numerical experiments, finding that our method substantially outperforms passive, uniform sampling for favorable synthetic and data-derived functions in low-noise settings with large sampling budgets. Our results also suggest an idealized \"oracle strategy\", which we use to gauge the potential advance of any adaptive-sampling strategy over passive sampling, for any given convex function.'},\n",
       " {'topics': [(0, 0.13516293), (2, 0.7405239), (4, 0.11878457)],\n",
       "  'percentage': 0.12973344326019287,\n",
       "  'text': 'In this contribution we describe an approach to evolve composite covariance functions for Gaussian processes using genetic programming. A critical aspect of Gaussian processes and similar kernel-based models such as SVM is, that the covariance function should be adapted to the modeled data. Frequently, the squared exponential covariance function is used as a default. However, this can lead to a misspecified model, which does not fit the data well. In the proposed approach we use a grammar for the composition of covariance functions and genetic programming to search over the space of sentences that can be derived from the grammar. We tested the proposed approach on synthetic data from two-dimensional test functions, and on the Mauna Loa CO2 time series. The results show, that our approach is feasible, finding covariance functions that perform much better than a default covariance function. For the CO2 data set a composite covariance function is found, that matches the performance of a hand-tuned covariance function.'},\n",
       " {'topics': [(2, 0.3066402), (4, 0.68359596)],\n",
       "  'percentage': 0.12871918082237244,\n",
       "  'text': 'We propose an improved LASSO estimation technique based on Stein-rule. We shrink classical LASSO estimator using preliminary test, shrinkage, and positive-rule shrinkage principle. Simulation results have been carried out for various configurations of correlation coefficients ($r$), size of the parameter vector ($\\\\beta$), error variance ($\\\\sigma^2$) and number of non-zero coefficients ($k$) in the model parameter vector. Several real data examples have been used to demonstrate the practical usefulness of the proposed estimators. Our study shows that the risk ordering given by LSE $>$ LASSO $>$ Stein-type LASSO $>$ Stein-type positive rule LASSO, remains the same uniformly in the divergence parameter $\\\\Delta^2$ as in the traditional case.'},\n",
       " {'topics': [(1, 0.100783534), (3, 0.42506042), (4, 0.47069862)],\n",
       "  'percentage': 0.12846019864082336,\n",
       "  'text': 'Genome-wide association studies (GWASs) aim to detect genetic risk factors for complex human diseases by identifying disease-associated single-nucleotide polymorphisms (SNPs). SNP-wise approach, the standard method for analyzing GWAS, tests each SNP individually. Then the P-values are adjusted for multiple testing. Multiple testing adjustment (purely based on p-values) is over-conservative and causes lack of power in many GWASs, due to insufficiently modelling the relationship among SNPs. To address this problem, we propose a novel method, which borrows information across SNPs by grouping SNPs into three clusters. We pre-specify the patterns of clusters by minor allele frequencies of SNPs between cases and controls, and enforce the patterns with prior distributions. Therefore, compared with the traditional approach, it better controls false discovery rate (FDR) and shows higher sensitivity, which is confirmed by our simulation studies. We re-analyzed real data studies on identifying SNPs associated with severe bortezomib-induced peripheral neuropathy (BiPN) in patients with multiple myeloma. The original analysis in the literature failed to identify SNPs after FDR adjustment. Our proposed method not only detected the reported SNPs after FDR adjustment but also discovered a novel SNP rs4351714 that has been reported to be related to multiple myeloma in another study.'},\n",
       " {'topics': [(1, 0.19373415), (2, 0.7988715)],\n",
       "  'percentage': 0.12729819118976593,\n",
       "  'text': 'Due to the intractable partition function, the exact likelihood function for a Markov random field (MRF), in many situations, can only be approximated. Major approximation approaches include pseudolikelihood and Laplace approximation. In this paper, we propose a novel way of approximating the likelihood function through first approximating the marginal likelihood functions of individual parameters and then reconstructing the joint likelihood function from these marginal likelihood functions. For approximating the marginal likelihood functions, we derive a particular likelihood function from a modified scenario of coin tossing which is useful for capturing how one parameter interacts with the remaining parameters in the likelihood function. For reconstructing the joint likelihood function, we use an appropriate copula to link up these marginal likelihood functions. Numerical investigation suggests the superior performance of our approach. Especially as the size of the MRF increases, both the numerical performance and the computational cost of our approach remain consistently satisfactory, whereas Laplace approximation deteriorates and pseudolikelihood becomes computationally unbearable.'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = get_similar_docs(myab, index, prep_text, bigrams, dictionary)\n",
    "\n",
    "display_similars(q, corp_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are the ten most closely related docuements to our query abstract. \n",
    "\n",
    "### Further areas of investigation ###\n",
    "\n",
    "1. How exactly does the similarites finder work? Would it be better if I used a TF-IDF matrix?\n",
    "    - Are those really the most simlar documents? Wieht a percentage score of at most 16%? Worth investigating more criticall\n",
    "    - When tested with a paper from the corpus, the function predictably returns itself as it's perfect match. \n",
    "2. Would Grid-Searching over `eta` and `alpha` and other LDA parameters result in better performing models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
